


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._functorch.aot_autograd &mdash; functorch 2.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>2.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torch._functorch.aot_autograd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._functorch.aot_autograd</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">functorch</span> <span class="kn">import</span> <span class="n">make_fx</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx.traceback</span> <span class="k">as</span> <span class="nn">fx_traceback</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">dynamo_timed</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">CrossRefFakeMode</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">,</span> <span class="n">FakeTensorMode</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">immutable_collections</span><span class="p">,</span> <span class="n">Interpreter</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="n">is_sym_node</span><span class="p">,</span> <span class="n">py_sym_types</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing.reductions</span> <span class="kn">import</span> <span class="n">StorageWeakRef</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">stateless</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="n">default_partition</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">TracingContext</span><span class="p">,</span> <span class="n">DuplicateInputs</span>

<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">MutationType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;MutationType&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;metadata_only&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;data_and_metadata&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">OutputType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;OutputType&quot;</span><span class="p">,</span> <span class="p">(</span>
        <span class="c1"># output is not an alias</span>
        <span class="s2">&quot;non_alias&quot;</span><span class="p">,</span>
        <span class="c1"># output aliases an input</span>
        <span class="s2">&quot;alias_of_input&quot;</span><span class="p">,</span>
        <span class="c1"># output **is** an input tensor</span>
        <span class="s2">&quot;is_input&quot;</span><span class="p">,</span>
        <span class="c1"># output has a ._base tensor, which is a graph intermediate.</span>
        <span class="c1"># We need to return its ._base as a graph output,</span>
        <span class="c1"># so its requires_grad info is populated correctly.</span>
        <span class="c1"># Instructs the runtime code to regenerate the current output</span>
        <span class="c1"># from a base tensor, graph_intermediates[base_idx]</span>
        <span class="s2">&quot;alias_of_intermediate_save_as_output&quot;</span><span class="p">,</span>
        <span class="c1"># Same as above; but we don&#39;t need to explicitly add its ._base</span>
        <span class="c1"># as a graph output, because it already **is** a graph output.</span>
        <span class="s2">&quot;alias_of_intermediate&quot;</span><span class="p">,</span>
        <span class="c1"># Same as above; but the output&#39;s ._base is **already** a user output.</span>
        <span class="c1"># Instructs the runtime code to regenerate the current output from</span>
        <span class="c1"># a base tensor, user_outputs[base_idx]</span>
        <span class="s2">&quot;alias_of_intermediate_base_is_user_output&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">(</span>
        <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)}</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="c1"># This global counter increments every time we compile a graph with</span>
<span class="c1"># AOTAutograd.  You can use this to correlate runtime error messages</span>
<span class="c1"># with compile time (e.g., if you get an error at runtime saying</span>
<span class="c1"># compiled graph 3 failed, you can set a breakpoint at compile time</span>
<span class="c1"># for this graph number to investigate further at compile time.)</span>
<span class="c1">#</span>
<span class="c1"># NB: this is different from get_aot_compilation_context, which tracks</span>
<span class="c1"># each underlying graph that is compiled.  In contrast, AOT_COUNTER</span>
<span class="c1"># corresponds to top-level invocations of aot_module/aot_function;</span>
<span class="c1"># one counter is allocated per entire compiled block (but this block</span>
<span class="c1"># may involve compiling multiple subgraphs; e.g., for forwards/backwards)</span>
<span class="n">AOT_COUNTER</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="n">KNOWN_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
    <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">py_sym_types</span><span class="p">)</span>
<span class="p">)</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">preserve_rng_state</span><span class="p">():</span>
    <span class="n">rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">cuda_rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">cuda_rng_state</span><span class="p">)</span>


<span class="c1"># Set up hooks so that during backward the fx&#39;s stack_trace is properly set</span>
<span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">setup_stacktrace_preservation_hooks</span><span class="p">(</span><span class="n">roots</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">q</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">fn</span><span class="p">,</span> <span class="n">_idx</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">next_functions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">or</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">node</span>

    <span class="k">def</span> <span class="nf">get_callback</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">callback</span><span class="p">():</span>
            <span class="k">global</span> <span class="n">callback_set</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">)</span>
            <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">callback</span>

    <span class="k">def</span> <span class="nf">get_prehook</span><span class="p">(</span><span class="n">stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">prehook</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
            <span class="k">global</span> <span class="n">callback_set</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">callback_set</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">variable</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span>
                    <span class="n">get_callback</span><span class="p">(</span><span class="n">fx_traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prehook</span>

    <span class="k">def</span> <span class="nf">get_posthook</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">posthook</span><span class="p">(</span><span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">posthook</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="n">forward_node_stack</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;traceback_&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_prehook</span><span class="p">(</span><span class="n">get_prehook</span><span class="p">(</span><span class="n">forward_node_stack</span><span class="p">))</span>

        <span class="n">special_stack</span> <span class="o">=</span> <span class="n">forward_node_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">special_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;Gradient addition node due to multiple use of tensor around:&quot;</span>
        <span class="p">)</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">get_posthook</span><span class="p">(</span><span class="n">special_stack</span><span class="p">))</span>


<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd contains a pretty non-trivial amount of logic to handle edge cases around aliasing and mutation</span>
<span class="c1"># that are external to the graph (they show up as side effects in some way when you run the graph).</span>
<span class="c1">#</span>
<span class="c1"># Take a look at `test_aotdispatch.py TestAOTAutograd.test_input_mutation*` tests for some examples functions</span>
<span class="c1"># and what they&#39;re compiled graphs looks like.</span>
<span class="c1"># Below is a very long comment detailing several edge cases, and showing how AOT Autograd handles them.</span>
<span class="c1">#</span>
<span class="c1"># Note [AOT Autograd: input data mutations]</span>
<span class="c1">#</span>
<span class="c1"># If we compile a function that mutates inputs, then those input mutations are real side effects</span>
<span class="c1"># that a user expects to see after running the compiled graph.</span>
<span class="c1"># However, the graph that we want to send to a backend needs to be *entirely* functional.</span>
<span class="c1"># The way we reconcile this difference is that we remove the mutations completely from the graph that we compile</span>
<span class="c1"># but we update the graph to return (updated_inputs, user_outputs).</span>
<span class="c1"># In the epilogue that runs after the compiled graph is executed, we copy the updated inputs back to the originals.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># After AOT Autograd compiles, we end up with a:</span>
<span class="c1"># (a) compiled graph</span>
<span class="c1"># (b) autograd.Function.forward() method, that executes the compiled graph</span>
<span class="c1"># (c) wrapper function, that calls the autograd.Function.forward() and performs the epilogue</span>
<span class="c1">#</span>
<span class="c1"># The output of (a, b, c) are all written below.</span>
<span class="c1">#</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated gets a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_x_updated, grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is that updated inputs (due to data mutations) *do* participate</span>
<span class="c1"># in the compiled backward graph! Since the compiled forward graph gets N extra outputs</span>
<span class="c1"># (due to updated inputs showing up as graph outputs),</span>
<span class="c1"># The compiled backward gets an additional N inputs.</span>
<span class="c1"># That way, during the x.copy_(x_updated) bit in the epilogue, gradients will flow from the updated input</span>
<span class="c1"># back to the original input.</span>


<span class="c1"># Note [AOT Autograd: input metadata mutations]</span>
<span class="c1">#</span>
<span class="c1"># For the same reason as input mutations, we also don&#39;t put input metadata mutations in the graph.</span>
<span class="c1"># Instead, we return the updated version of the input (a view), and mutate the input&#39;s metadata outside of the graph</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.t_()</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.t()</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated does *not* get a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.as_strided_(x_updated)</span>
<span class="c1">#     return out</span>


<span class="c1"># Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd needs special handling for outputs that alias graph inputs or intermediates!</span>
<span class="c1"># Why?</span>
<span class="c1"># (1) autograd.Function.forward() has a limitation, where views that returned in the forward cannot later be mutated.</span>
<span class="c1"># (2) views don&#39;t need to be compiled in the graph anyway - it&#39;s cheap to generate them outside of the compiled graph,</span>
<span class="c1">#     in an epilogue.</span>
<span class="c1"># For outputs that alias inputs, we do the following:</span>
<span class="c1"># (a) *still* return the aliased output as a graph output</span>
<span class="c1"># (b) In the AOT Autograd wrapper/epilogue, we don&#39;t return that aliased output. Instead, we use it to regenerate the output.</span>
<span class="c1">#</span>
<span class="c1"># For outputs that alias *intermediates*, we do the following:</span>
<span class="c1"># (a) Return the output in the compiled forward, **and** return it&#39;s ._base (a graph intermediates) as an output in the forward</span>
<span class="c1"># (b) Use (output, graph_intermediate) to regenerate the alias, and return that to the user (instead of the compiled fw output).</span>
<span class="c1"># You might wonder why we return the aliased output directly in the graph (and making the graph compute it),</span>
<span class="c1"># only to not return it and instead generate a fresh alias off of the intermediate,</span>
<span class="c1"># instead of (say) just storing metadata about the size/stride of the output somewhere to generate the alias. There are two reasons:</span>
<span class="c1"># (1) Getting the actual alias tensor allows us to use view-replay to generate the alias, instead of an as_strided() call</span>
<span class="c1"># (2) Inductor (and other backends) are free to change the memory format of graph outputs, if it results in better performance.</span>
<span class="c1">#     This can result in problems if a user later tries to .view() that output expecting it to have one set of strides,</span>
<span class="c1">#     when it has a different set of strides.</span>
<span class="c1">#     By including the view op directly in the graph, inductor takes that into account when deciding what memory format</span>
<span class="c1">#     the graph intermediate should be.</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is how our traced backward() graph handles aliases.</span>
<span class="c1"># (this applies to outputs aliasing inputs, outputs aliasing intermediates,</span>
<span class="c1">#  *and* updated inputs returned in the compiled forward due to metadata-only mutations).</span>
<span class="c1"># Any outputs that alias (either inputs or intermediates) do NOT participate in the compiled backward graph</span>
<span class="c1"># It would be wasteful to include them in the compiled backward(), because we regenerate them eagerly</span>
<span class="c1"># at the end of the forward.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     return out1, out2</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     # the compiled graph also returns the intermediate</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># # intermediate gets a gradient in the compiled backward.</span>
<span class="c1"># # both output aliases (out1 and out2) do not.</span>
<span class="c1"># def compiled_backward_graph(grad_intermediate):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     out1, out2, intermediate = compiled_forward_graph(x)</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     out1, out2, intermediate = autograd.Function.apply(x)</span>
<span class="c1">#     # regenerate out1 from the input</span>
<span class="c1">#     out1_regenerated = out1._view_func(x)</span>
<span class="c1">#     # regenerate out1 from the intermediate</span>
<span class="c1">#     out2_regenerated = out2._view_func(intermediate)</span>
<span class="c1">#     return out1_regenerated, out2_regenerated</span>


<span class="c1"># Note [AOT Autograd: mutations to inputs that alias other inputs]</span>
<span class="c1">#</span>
<span class="c1"># Another edge case that is (only partially) handled today is when an input is mutated, but itself aliases another input.</span>
<span class="c1"># AOT Autograd needs to **ensure** that functionalization knows that the two inputs are aliased to each other.</span>
<span class="c1"># That way, when the aliased input is accessed later in the graph, functionalization knows to &quot;update&quot; the alias</span>
<span class="c1"># given the mutation that occurred.</span>
<span class="c1">#</span>
<span class="c1"># This is handled by updating the calling convention: we create a &quot;synthetic base&quot; that becomes a new input</span>
<span class="c1"># in the compiled function, and we regenerate the original (aliased) inputs directly off of the base</span>
<span class="c1"># inside of the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># See merge_view_inputs() for more detailed info.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x, x_view):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x * x_view</span>
<span class="c1">#     return out</span>
<span class="c1"># f(x, x.view(-1))</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(base)</span>
<span class="c1">#     x = generate_x(base)</span>
<span class="c1">#     x_view = generate_x_view(base)</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     x_view_updated = x_updated.view(-1)</span>
<span class="c1">#     out = x_updated * x_view_udpated</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The calling convention change from (aliases) -&gt; (base) happens</span>
<span class="c1"># # *outside* of the autograd.Function.forward().</span>
<span class="c1"># # That means the forward() only has 1 input (base),</span>
<span class="c1"># # and the backward() only has 1 output (grad_base)</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_base = ...</span>
<span class="c1">#     return grad_base</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(base):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(base)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The compiled wrapper is where we create synthetic bases.</span>
<span class="c1"># # The info on which inputs are mutated is also tracked *before* synthetic base creation.</span>
<span class="c1"># def compiled_wrapper(x, x_view):</span>
<span class="c1">#     base = merge_view_inputs(x, x_view)</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(base)</span>
<span class="c1">#     # x and x_view are aliased in eager mode, so this mutation to x will automatically affect x_view.</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="c1"># This class stores info about every user output.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">OutputAliasInfo</span><span class="p">:</span>
    <span class="c1"># Tells us if this output is:</span>
    <span class="c1"># (1) a regular (non-aliased) output</span>
    <span class="c1"># (2) an alias of a forward input</span>
    <span class="c1"># (3) **is** a forward input (special case of &quot;alias_of_input&quot;)</span>
    <span class="c1"># (4) an alias of an intermediate (aka an alias of an output of the inner traced forward)</span>
    <span class="c1"># (5) an alias of an intermediate, that explicitly requires returning the intermediate</span>
    <span class="c1">#     as a graph output</span>
    <span class="c1"># (6) an alias of an intermediate, where that intermediate is also a user output</span>
    <span class="n">output_type</span><span class="p">:</span> <span class="n">OutputType</span>
    <span class="c1"># If (1) above, then</span>
    <span class="c1"># - base_idx is None</span>
    <span class="c1"># If (2) or (3) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is user_fwd_input[base_idx]</span>
    <span class="c1">#   (This is an index into the inputs *before* we make synthetic bases)</span>
    <span class="c1"># If (4) or (5) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is output_graph_intermediates[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="c1"># If (6) above, then:</span>
    <span class="c1"># - Tells us that the base of this alias is output_user_fwds[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="n">base_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>


<span class="c1"># This class tells us info about user inputs.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">InputAliasInfo</span><span class="p">:</span>
    <span class="n">mutates_data</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">mutates_metadata</span><span class="p">:</span> <span class="nb">bool</span>


<span class="c1"># This class encapsulates all aliasing + mutation info we need about the forward graph</span>
<span class="c1"># See a more detailed overview of the edge case handling at</span>
<span class="c1"># https://docs.google.com/document/d/19UoIh_SVrMy_b2Sx5ZaeOJttm6P0Qmyss2rdBuyfoic/edit</span>
<span class="nd">@dataclass</span><span class="p">()</span>
<span class="k">class</span> <span class="nc">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="c1"># length = # user inputs</span>
    <span class="c1"># This gives us info about every input, and what sort of mutation happened to it (if any)</span>
    <span class="n">input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]</span>

    <span class="c1"># length = # user outputs</span>
    <span class="c1"># This gives us info about every output (mostly around whether it aliases other tensors)</span>
    <span class="n">output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span>

    <span class="c1"># length = # mutated inps + # user outputs</span>
    <span class="c1"># For every output *and* mutated input returned from the forward,</span>
    <span class="c1"># tells us whether or not the output should require gradients or not</span>
    <span class="n">requires_grad_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>

    <span class="c1"># length = the number of intermediate bases appended as outputs to the end of the forward graph.</span>
    <span class="c1"># Note: this is not necessarily the same thing as:</span>
    <span class="c1">#   len([x for x in output_info if x.output_type == OutputType.alias_of_intermediate])</span>
    <span class="c1"># Because outputs might share a ._base, or an output&#39;s ._base might itself be</span>
    <span class="c1"># another user output (in both cases, we won&#39;t redundantly append bases to the end of the graph)</span>
    <span class="n">num_intermediate_bases</span><span class="p">:</span> <span class="nb">int</span>

    <span class="c1"># For inference only: instructs us to keep data-only input mutations directly in the graph</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># pre-compute the indices of the inputs that are mutated.</span>
        <span class="c1"># When keep_input_mutations is set, we don&#39;t need to worry about our epilogue</span>
        <span class="c1"># handling data-only mutations, because we keep them directly in the graph.</span>
        <span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">aliased_out_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
        <span class="p">]</span>

        <span class="c1"># This is pre-computed in post_init for perf.</span>
        <span class="c1"># It contains the index of every element</span>
        <span class="c1"># of input_info that corresponds to a mutation (data or metadata or both)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="n">mutated_inp_indices</span>
        <span class="c1"># This is pre-computed for perf.</span>
        <span class="c1"># It contains the index of every element</span>
        <span class="c1"># of output_info that corresponds to an alias (either of an input or intermediate)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aliased_out_indices</span> <span class="o">=</span> <span class="n">aliased_out_indices</span>


<span class="c1"># This class exists because:</span>
<span class="c1"># - the autograd.Function.forward() in aot autograd returns outputs that might alias inputs</span>
<span class="c1"># - we only care about the metadata on those aliases, so we can regenerate them.</span>
<span class="c1">#   We do not want them to participate in the autograd.Function.</span>
<span class="c1"># We do that by wrapping them in an opaque class, so the autograd.Function</span>
<span class="c1"># does not know to treat them as tensors.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorAlias</span><span class="p">:</span>
    <span class="n">alias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>


<span class="k">def</span> <span class="nf">has_same_metadata</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">t1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">target_meta_tensor</span><span class="p">,</span> <span class="n">target_requires_grad</span><span class="p">):</span>
    <span class="c1"># Try to do view-replay if possible.</span>
    <span class="c1"># fall back to .as_strided() if we can&#39;t.</span>
    <span class="k">if</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># The base that we want to replay our view off of might have a different shape than the view&#39;s original base.</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_base</span>
        <span class="n">abt</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
        <span class="c1"># Don&#39;t unnecessarily call as_strided if nothing changed; as_strided&#39;s</span>
        <span class="c1"># backward is poorly implemented and slow</span>
        <span class="k">if</span> <span class="n">abt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">b</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="ow">or</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="ow">or</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">reshaped_base_tensor</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
                <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reshaped_base_tensor</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_view_func</span><span class="p">(</span><span class="n">reshaped_base_tensor</span><span class="p">)</span>
        <span class="c1"># This shape mismatch can happen due to a bug in inplace/view handling in autograd.</span>
        <span class="c1"># Try putting a breakpoint here and running</span>
        <span class="c1"># `test/functorch/test_aotdispatch TestAOTAutograd.test_output_all_alias_types`</span>
        <span class="c1"># Also, https://github.com/pytorch/pytorch/issues/49825</span>
        <span class="c1">#</span>
        <span class="c1"># As a stopgap, we&#39;ll fall back to as_strided.</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_requires_grad</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_requires_grad</span><span class="p">:</span>
                <span class="n">out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
    <span class="n">storage_offset</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">)</span>
    <span class="c1"># For outputs aliasing inputs, we need to check if the requires-gradness has changed.</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">aliased_out</span>

<span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mirror_autograd_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">t</span>

<span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>


<span class="c1"># This is a version of functionalization that is specifically designed</span>
<span class="c1"># for the AOTAutograd use case.</span>
<span class="c1">#</span>
<span class="c1"># Unlike functorch&#39;s variant, this doesn&#39;t use the functorch level system,</span>
<span class="c1"># instead it directly uses PyTorch&#39;s conventional dispatcher to hit the</span>
<span class="c1"># functionalization key.  In particular, this means that FunctionalTensorWrapper</span>
<span class="c1"># can have autograd data stored directly on it.</span>
<span class="c1">#</span>
<span class="c1"># In typical AOTAutograd usage, the dispatch key order will look like:</span>
<span class="c1">#</span>
<span class="c1">#   Autograd - Functionalization ~~~~&gt; Proxy Mode - Fake Tensor</span>
<span class="c1">#       outer tensor                        inner tensor</span>
<span class="c1">#</span>
<span class="c1"># Returns:</span>
<span class="c1"># - ViewAndMutationMeta, telling us metadata about the inputs and outputs</span>
<span class="c1"># - The list of outputs from the forward, but **only** the outputs that we need</span>
<span class="c1">#   to pass in as tangents into the backward.</span>
<span class="c1">#   Specifically, aliased outputs from the forward get regenerated, and don&#39;t participate</span>
<span class="c1">#   in the compiled backward function.</span>
<span class="k">def</span> <span class="nf">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
    <span class="n">f</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ViewAndMutationMeta</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]:</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mirror_autograd_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
            <span class="k">return</span> <span class="n">r</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="c1"># This function is meant to be run with the forward, which expects a flat list of tensor/symint/other args.</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">KNOWN_TYPES</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">)</span>

        <span class="n">input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">input_requires_grad_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_requires_grad_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">flat_f_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># precondition: The passed in function already handles unflattening inputs + flattening outputs</span>
            <span class="n">flat_f_outs</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">flat_f_args</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="c1"># Inspect the state of the input tensor functional wrapper to detect input mutation info</span>
        <span class="c1"># If inp[i] has a metadata-only mutation, then maybe_inputs_with_mutated_metadata[i] contains the updated version</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">f_arg</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_f_args</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">arg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_arg</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="o">==</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">new_arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()):</span>
                    <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">has_same_metadata</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">new_arg</span><span class="p">)</span>
                <span class="c1"># Only track requires_grad info on *mutated* inputs,</span>
                <span class="c1"># because they show up in the autograd.Function.forward as outputs</span>
                <span class="n">input_requires_grad_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">f_arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">f_arg</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">InputAliasInfo</span><span class="p">(</span>
                <span class="n">mutates_data</span><span class="o">=</span><span class="n">mutates_data</span><span class="p">,</span>
                <span class="n">mutates_metadata</span><span class="o">=</span><span class="n">mutates_metadata</span>
            <span class="p">))</span>

        <span class="c1"># If a function involves creating a tensor, and returning a view of it, such that its _base is the intermediiate,</span>
        <span class="c1"># We need to make sure our graph returns the _base as a graph output, and we manually recreate the view</span>
        <span class="c1"># to return to the user. Why? The backend compiler is free to (incorrectly) not set requires_grad</span>
        <span class="c1"># on the base tensor, but we are obligated to properly set requires-gradness on the real output.</span>

        <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">inp_storage_refs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()):</span> <span class="n">idx</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1"># We need inp tensor id&#39;s to be able to tell if an outputs **are** inputs.</span>
        <span class="n">inp_tensor_ids</span> <span class="o">=</span> <span class="p">{</span>
            <span class="nb">id</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span> <span class="k">for</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="n">flat_f_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="c1"># We need output tensor id&#39;s to tell if any output._base` attributes **are** other outputs.</span>
        <span class="c1"># (This is also a dict because we need to know that output&#39;s index, so we can regenerate</span>
        <span class="c1"># the alias from it).</span>
        <span class="n">out_tensor_ids</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">)}</span>
        <span class="c1"># maps the id of an intermediate base to its index in the output of the compiled forward</span>
        <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">intermediate_bases</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">flat_f_outs</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="ow">in</span> <span class="n">inp_storage_refs</span>
            <span class="p">):</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="n">inp_storage_refs</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())]</span>
                <span class="n">is_input_tensor</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">in</span> <span class="n">inp_tensor_ids</span>
                <span class="k">if</span> <span class="n">is_input_tensor</span><span class="p">:</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>

            <span class="c1"># We only need to handle the intermediate base case when both</span>
            <span class="c1"># the intermediate base and the output require gradients.</span>
            <span class="c1"># See Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">):</span>
                <span class="c1"># First, check if o&#39;s ._base is an existing output</span>
                <span class="n">maybe_existing_out_idx</span> <span class="o">=</span> <span class="n">out_tensor_ids</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">maybe_existing_out_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Special case where the output is an alias of a graph intermediate, but that intermediate</span>
                    <span class="c1"># is itself also a user output.</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                    <span class="n">base_idx</span> <span class="o">=</span> <span class="n">maybe_existing_out_idx</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Next, check if o&#39;s ._base is an intermediate base that we already returned</span>
                    <span class="n">maybe_existing_base_output_idx</span> <span class="o">=</span> <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">),</span> <span class="kc">None</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_existing_base_output_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span>
                        <span class="n">base_idx</span> <span class="o">=</span> <span class="n">maybe_existing_base_output_idx</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Otherwise, take o._base and explicitly return it as an output in the compiled graph</span>
                        <span class="n">new_out_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">)</span>
                        <span class="n">base_idx</span> <span class="o">=</span> <span class="n">new_out_idx</span>
                        <span class="c1"># Indicate to the logic later on (when we trace the joint)</span>
                        <span class="c1"># that this particular output should get it&#39;s ._base appended to the forward graph outputs</span>
                        <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span>
                        <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_out_idx</span>
                        <span class="n">intermediate_bases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">out_info</span> <span class="o">=</span> <span class="n">OutputAliasInfo</span><span class="p">(</span>
                <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">,</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="n">base_idx</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_info</span><span class="p">)</span>
            <span class="n">output_requires_grad_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">)</span>

        <span class="c1"># Our autograd.Function.forward returns both mutated inputs and outputs,</span>
        <span class="c1"># so we need grad info on all of them.</span>
        <span class="n">requires_grad_info</span> <span class="o">=</span> <span class="n">input_requires_grad_info</span> <span class="o">+</span> <span class="n">output_requires_grad_info</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">requires_grad_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_info</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># This analysis function returns *only* the outputs that are meant to be tangents to the backwards.</span>
        <span class="c1"># Anything that aliases (inputs returned in the fw due to metadata mutations, or outputs that alias inputs/intermediates)</span>
        <span class="c1"># are *regenerated* later, and not used directly in the autograd graph</span>
        <span class="n">f_input_tangents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">inp</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">,</span> <span class="n">input_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="p">]</span>
        <span class="n">f_output_tangents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">o</span>
            <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">,</span> <span class="n">output_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
        <span class="p">]</span>
        <span class="c1"># intermediate bases are also included in the backward graph</span>
        <span class="n">f_tangents</span> <span class="o">=</span> <span class="n">f_input_tangents</span> <span class="o">+</span> <span class="n">f_output_tangents</span> <span class="o">+</span> <span class="n">intermediate_bases</span>

        <span class="n">metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
            <span class="n">input_info</span><span class="o">=</span><span class="n">input_info</span><span class="p">,</span>
            <span class="n">requires_grad_info</span><span class="o">=</span><span class="n">requires_grad_info</span><span class="p">,</span>
            <span class="n">output_info</span><span class="o">=</span><span class="n">output_info</span><span class="p">,</span>
            <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">),</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_tangents</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner</span>


<span class="k">def</span> <span class="nf">unpack_synthetic_bases</span><span class="p">(</span>
    <span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">synthetic_base_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
    <span class="c1"># This is only not None if our graph mutates a graph input that aliases another graph input.</span>
    <span class="k">if</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">primals</span>

    <span class="n">f_args_inner</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">outer_idx_or_tuple</span> <span class="ow">in</span> <span class="n">synthetic_base_info</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_idx_or_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primals</span><span class="p">[</span><span class="n">outer_idx_or_tuple</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outer_base_idx</span><span class="p">,</span> <span class="n">view_tensor</span> <span class="o">=</span> <span class="n">outer_idx_or_tuple</span>
            <span class="n">outer_base</span> <span class="o">=</span> <span class="n">primals</span><span class="p">[</span><span class="n">outer_base_idx</span><span class="p">]</span>
            <span class="n">view_arg</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span>
                <span class="n">outer_base</span><span class="p">,</span> <span class="n">view_tensor</span><span class="p">,</span> <span class="n">view_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">)</span>
            <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">view_arg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f_args_inner</span>

<span class="c1"># This class contains all the metadata we care about for the current function we&#39;re compiling.</span>
<span class="c1"># This data is needed both at trace time and at runtime.</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CompiledRuntimeMetadata</span><span class="p">:</span>
    <span class="c1"># This type / object should be cleaned up</span>
    <span class="c1"># See Note [Synthetic Base Info Metadata]</span>
    <span class="n">synthetic_base_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_non_aliased</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span>

<span class="c1"># This function takes in a tensor t, and returns one of t, t.view(), or t.clone().</span>
<span class="c1"># When tracing the joint forward + backward, for any inputs in the graph that are mutated,</span>
<span class="c1"># we need to clone them first (and similarly for metadata-only mutations, we need to view them first).</span>
<span class="c1"># The idea is that when we trace the backward, we need to pass in the *original* primals</span>
<span class="c1"># to autograd.grad(), before they were mutated.</span>
<span class="c1"># Note: when we have synthetic base inputs, we need to clone them *before* creating views off of them.</span>
<span class="c1"># This means that &quot;idx&quot; here represents the index of the (potentially) synthetic base.</span>
<span class="c1"># What we need to do is:</span>
<span class="c1"># (1) map the current (post-synthetic-base calling convention) input argument index</span>
<span class="c1">#     to int index pre-synthetic-base-calling-convention.</span>
<span class="c1"># (2) There could be multiple, if this index corresponds to a synthetic base</span>
<span class="c1">#     that has multiple input aliases.</span>
<span class="c1"># (3) If any of those corresponding inputs get metadata mutations, then we clone the base.</span>
<span class="k">def</span> <span class="nf">maybe_to_fresh_input</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">meta</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span>

    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outer_aliased_indices_of_current_base_arg</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">outer_aliased_indices_of_current_base_arg</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># For every argument index in the outer calling convention (before synthetic bases)</span>
            <span class="c1"># find its index in the inner calling convention.</span>
            <span class="c1"># if it matches the index of our current arg (idx), track the outer argument&#39;s index (i)</span>
            <span class="n">i</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">outer_idx_or_tuple</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">synthetic_base_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_idx_or_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">outer_idx_or_tuple</span> <span class="o">==</span> <span class="n">idx</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_idx_or_tuple</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">outer_idx_or_tuple</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">idx</span>
            <span class="p">)</span>
        <span class="p">]</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outer_aliased_indices_of_current_base_arg</span>
    <span class="p">):</span>
        <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
        <span class="c1"># sees the tensor before the mutation</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outer_aliased_indices_of_current_base_arg</span>
    <span class="p">):</span>
        <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
        <span class="c1"># sees the tensor before the metadata mutation</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span>

<span class="c1"># This function takes in a forward fn, runs it, and (optionally) runs autograd to compute the joint.</span>
<span class="c1"># When maybe_tangents is None, we only run the forward. Otherwise we run the &quot;joint&quot; forward + backward.</span>
<span class="c1"># Preconditions:</span>
<span class="c1"># - fn corresponds to the flattened user fw function, with duplicate inputs removed</span>
<span class="c1"># - functionalization is turned on (and inputs are wrapped in functional tensors)</span>
<span class="c1"># - Synthetic bases have been *removed* (we&#39;ve taken views on them corresponding to the user argument views).</span>
<span class="c1"># - primals_after_cloning are what we run our forward function on. It is identical to primals_before_cloning,</span>
<span class="c1">#   except that every input we know will be mutated in the forward has been cloned.</span>
<span class="c1">#   We run our forward on primals_after_cloning (potentially mutating some inputs), and then compute our gradients</span>
<span class="c1">#   w.r.t. primals_before_cloning (so we properly capture the mutation in our gradient computation).</span>
<span class="c1"># Importantly, due functionalization + some autograd.Function constraints, this function can return EXTRA outputs</span>
<span class="c1"># compared to what the original user forward returns.</span>
<span class="c1">#</span>
<span class="c1"># If we are only running the forward (and not computing the joint):</span>
<span class="c1"># - Our function will return (updated_inputs, fw_outs)</span>
<span class="c1">#</span>
<span class="c1"># If we are running the forward + backward (computing the joint):</span>
<span class="c1"># - Our function will return (updated_inputs, fw_outs, intermediate_bases), (gradients)</span>
<span class="c1">#</span>
<span class="c1"># Finally, if keep_input_mutations is set, then we will explicitly *not* return updated inputs, for any inputs</span>
<span class="c1"># that experienced data-only mutations.</span>
<span class="c1"># Instead, we are relying on the logic in create_forward_or_joint_functionalized to manually perform the input mutations,</span>
<span class="c1"># keeping them directly in the traced graph.</span>
<span class="k">def</span> <span class="nf">forward_or_joint</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">primals_before_cloning</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">primals_after_cloning</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">maybe_tangents</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="n">outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals_after_cloning</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>

    <span class="c1"># The compiled fw will return mutated input tensors, *including* metadata-only mutation.</span>
    <span class="c1"># However, if keep_input_mutations is set, the compiled fw only needs to return metadata-mutated inputs.</span>
    <span class="c1"># (because data-only input mutations are handled directly in the compiled graph)</span>
    <span class="k">if</span> <span class="n">keep_input_mutations</span><span class="p">:</span>
        <span class="n">mutated_inputs_to_return</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_after_cloning</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mutated_inputs_to_return</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_after_cloning</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span>
        <span class="p">]</span>

    <span class="c1"># Case 1: We are just tracing the forward; not the joint forward + backward.</span>
    <span class="k">if</span> <span class="n">maybe_tangents</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">*</span><span class="n">mutated_inputs_to_return</span><span class="p">,</span> <span class="o">*</span><span class="n">outs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tangents</span> <span class="o">=</span> <span class="n">maybe_tangents</span>

    <span class="c1"># Case 2: We are tracing the joint forward backward.</span>
    <span class="c1"># This also requires us to:</span>
    <span class="c1"># - update the graph to return intermediate bases</span>
    <span class="c1"># - Figure out what grad_outputs to pass into the backward</span>
    <span class="c1"># - (this includes intermediate bases in the forward, and forward inputs that had data mutations)</span>
    <span class="c1"># - actually call autograd.grad to trace the backward.</span>
    <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">:</span>
            <span class="n">intermediate_bases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">)</span>

    <span class="c1"># Pass any (non-aliased) outputs in as tangents, since they&#39;ll be returned as outputs in the fw</span>
    <span class="c1"># For outputs that are aliases of intermediates, we will have returned the output&#39;s _base as an output in the graph instead,</span>
    <span class="c1"># which we *should* send to grad()</span>
    <span class="n">outputs_for_grad</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
    <span class="p">]</span>
    <span class="c1"># Pass any (non-aliased) mutated inputs in as tangents, since they&#39;ll be returned as outputs in the fw</span>
    <span class="c1"># Important: the traced joint fw/bw will return updated inputs with data mutations,</span>
    <span class="c1"># but *not* with metadata mutations.</span>
    <span class="c1"># Instead, we shunt the updated metadata around externally</span>
    <span class="c1"># and update the input&#39;s metadata outside of the autograd.Function</span>
    <span class="n">mutated_inputs_for_grad</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_after_cloning</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
    <span class="p">]</span>
    <span class="c1"># The tensors that we include in the backward graph are:</span>
    <span class="c1"># - inputs that recieve *data* mutations (not metadata-only; those are recomputed later)</span>
    <span class="c1"># - outputs that are not aliased (aliased outputs are recomputed later)</span>
    <span class="c1"># - intermediate ._base tensors of aliased outputs (we use those later to recompute the aliased outputs)</span>
    <span class="n">fw_outs_to_grad</span> <span class="o">=</span> <span class="n">mutated_inputs_for_grad</span> <span class="o">+</span> <span class="n">outputs_for_grad</span> <span class="o">+</span> <span class="n">intermediate_bases</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_to_grad</span><span class="p">)</span>

    <span class="c1"># the compiled forward should return (mutated_inputs, user_outs, intermediate_bases)</span>
    <span class="n">fw_outs_to_return</span> <span class="o">=</span> <span class="o">*</span><span class="n">mutated_inputs_to_return</span><span class="p">,</span> <span class="o">*</span><span class="n">outs</span><span class="p">,</span> <span class="o">*</span><span class="n">intermediate_bases</span>

    <span class="c1"># Take care to grab and sync the updated inputs from primals_after_cloning (the inputs we actually mutate!)</span>
    <span class="c1"># and not primals_before_cloning (the preserved inputs, pre-mutation, that we pass to grad())</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_after_cloning</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>

    <span class="c1"># Get the inputs that need gradients</span>
    <span class="n">grad_primals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">inputs_needs_grads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Note that we&#39;re not using primals_before_cloning here,</span>
    <span class="c1"># being carefully not to pass any mutated inputs into autograd.grad()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primals_before_cloning</span><span class="p">:</span>
        <span class="n">is_grad_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="n">inputs_needs_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_grad_tensor</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_grad_tensor</span><span class="p">:</span>
            <span class="n">grad_primals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># Get the outputs that need gradients</span>
    <span class="n">needed_outs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">needed_tangents</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fw_outs_to_grad</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># A bit sketchy, but fixes e.g. test_aot_autograd_exhaustive_matmul_cpu_float32</span>
            <span class="c1"># The issue is that we are sensitive to decomps that don&#39;t accurately maintain</span>
            <span class="c1"># their output&#39;s _base.shape compared to eager mode, and this helps mitigate a bit.</span>
            <span class="n">needed_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">out</span> <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">tangent</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tangent</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">needed_tangents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">setup_stacktrace_preservation_hooks</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">grad_fn</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">needed_outs</span><span class="p">])</span>

    <span class="n">backward_out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Call the backwards pass</span>
    <span class="k">if</span> <span class="n">grad_primals</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">():</span>
            <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                <span class="n">needed_outs</span><span class="p">,</span>
                <span class="n">grad_primals</span><span class="p">,</span>
                <span class="n">grad_outputs</span><span class="o">=</span><span class="n">needed_tangents</span><span class="p">,</span>
                <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="n">backward_out_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backward_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fw_outs_to_return</span><span class="p">,</span> <span class="p">[</span>
        <span class="nb">next</span><span class="p">(</span><span class="n">backward_out_iter</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs_needs_grads</span>
    <span class="p">]</span>

<span class="c1"># This function expands synthetic base arguments into the original aliased inputs that the user passed in.</span>
<span class="c1"># Preconditions:</span>
<span class="c1"># - fn corresponds to the flattened user fw function, with duplicate inputs removed</span>
<span class="c1"># - functionalization is turned on (and inputs are wrapped in functional tensors)</span>
<span class="c1"># - both primals args **include** synthetic bases.</span>
<span class="c1">#   &quot;primals_after_cloning&quot; just corresponds to &quot;primals_before_cloning&quot;, but with some inputs (optionally) cloned.</span>
<span class="c1">#   &quot;primals_before_cloning&quot; is unused, and is only needed so we can pass the correct leaf tensors into autograd.</span>
<span class="k">def</span> <span class="nf">flat_fn_with_synthetic_bases_expanded</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">primals_before_cloning</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">primals_after_cloning</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">maybe_tangents</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">):</span>
    <span class="c1"># This is where we handle the calling convention around synthetic bases.</span>
    <span class="c1"># We need to make sure that we convert any synthetic base arguments into views</span>
    <span class="c1"># *after* we clone inputs for autograd (see below), to preserve the view relationship.</span>
    <span class="n">primals</span> <span class="o">=</span> <span class="n">unpack_synthetic_bases</span><span class="p">(</span><span class="n">primals_after_cloning</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">synthetic_base_info</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
    <span class="n">outs</span> <span class="o">=</span> <span class="n">forward_or_joint</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">primals_before_cloning</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">maybe_tangents</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">keep_input_mutations</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outs</span>

<span class="c1"># This function adds extra clone() calls on any inputs in the forward that get mutated.</span>
<span class="c1"># It *only* does this if we plan on performing autograd on fn.</span>
<span class="c1"># The idea here is that when computing grdients w.r.t. inputs, we need to compute our gradients</span>
<span class="c1"># w.r.t. the inputs *before* they were mutated!</span>
<span class="c1"># Preconditions:</span>
<span class="c1"># - fn corresponds to the flattened user fw function, with duplicate inputs removed</span>
<span class="c1"># - primals **includes** synthetic bases. Importantly, if a synthetic base is mutated,</span>
<span class="c1">#   we need to clone it *before* taking views off of it (if we clone the views they won&#39;t be views anymore)</span>
<span class="c1"># - functionalization is turned on (and inputs are wrapped in functional tensors)</span>
<span class="k">def</span> <span class="nf">flat_fn_no_input_mutations</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">maybe_tangents</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">):</span>
    <span class="c1"># When tracing the joint fwd + bwd, making sure to clone any inputs that are mutated first.</span>
    <span class="c1"># We need to ensure that the inputs we pass to autograd.grad() are the *original*</span>
    <span class="c1"># inputs, and not their mutated values.</span>
    <span class="k">if</span> <span class="n">maybe_tangents</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">primals_after_cloning</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">maybe_to_fresh_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">primals_after_cloning</span> <span class="o">=</span> <span class="n">primals</span>
    <span class="n">outs</span> <span class="o">=</span> <span class="n">flat_fn_with_synthetic_bases_expanded</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">primals_after_cloning</span><span class="p">,</span> <span class="n">maybe_tangents</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">keep_input_mutations</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outs</span>

<span class="c1"># This creates the final function that we want to trace using make_fx(),</span>
<span class="c1"># in both aot_dispatch_autograd and aot_dispatch_base.</span>
<span class="c1"># Preconditions:</span>
<span class="c1"># - fn corresponds to the user&#39;s fw function</span>
<span class="c1"># - fn arguments have been flattened, duplicate arguments have been handled</span>
<span class="c1"># - In the returned function, the &quot;primals&quot; arguments *includes* synthetic bases.</span>
<span class="c1"># This function does the work of functionalizing the input function,</span>
<span class="c1"># and performing copy_() calls at the end of the function if `keep_input_mutations` is set.</span>
<span class="c1"># The function returned has signature that is either:</span>
<span class="c1"># (1) &quot;traced_fn(primals: List[Any])&quot; if trace_joint is False</span>
<span class="c1"># (2) &quot;traced_fn(primals: List[Any], tangents: List[Any])&quot; if trace_joint is True</span>
<span class="k">def</span> <span class="nf">create_forward_or_joint_functionalized</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">,</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">):</span>

    <span class="k">def</span> <span class="nf">functionalized_f_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">maybe_tangents</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Convention: this function is used to trace both the joint, and just the forward (for inference).</span>
        <span class="c1"># When trace_joint is set, tangents should be passed in.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">maybe_tangents</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">trace_joint</span>
        <span class="c1"># Wrap inputs into functional wrappers</span>
        <span class="n">f_primals</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">primals</span><span class="p">)</span>
        <span class="n">f_tangents</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">maybe_tangents</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">maybe_tangents</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Run the joint</span>
            <span class="n">f_outs</span> <span class="o">=</span> <span class="n">flat_fn_no_input_mutations</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">f_primals</span><span class="p">,</span> <span class="n">f_tangents</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">keep_input_mutations</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">keep_input_mutations</span><span class="p">:</span>
            <span class="c1"># Note: This is a bit annoying. There&#39;s a layering issue here, where:</span>
            <span class="c1"># (1) functionalization needs to operate on **synthetic base** inputs, before unpacking them into the &quot;real&quot; inputs.</span>
            <span class="c1"># (2) For keep_input_mutations, we support tracing a call to copy_() directly on mutated inputs.</span>
            <span class="c1">#     However, we **only** want to support this for inputs that have data-only (and no metadata) mutations,</span>
            <span class="c1">#     because inductor (and backends in generally) would prefer not to see these (e.g. as_strided_(), resize_()).</span>
            <span class="c1">#     This makes it pretty difficult for this logic to operate on synthetic bases.</span>
            <span class="c1"># (3) In addition, there are cases where it&#39;s significantly cheaper to perform the copy on the individual</span>
            <span class="c1">#     (unpacked) input aliases, instead of the synthetic base.</span>
            <span class="c1"># The result is that ideally this function shouldn&#39;t have to worry about synthetic bases</span>
            <span class="c1"># (unpacking them happens underneath this function),</span>
            <span class="c1"># but we actually do need to unpack the synthetic bases when performing the copy_&#39;s to keep input mutations around.</span>
            <span class="c1"># Example case where this could be important:</span>
            <span class="c1">#</span>
            <span class="c1">#     def f(x, y):</span>
            <span class="c1">#         x.mul_(2)</span>
            <span class="c1">#         y.mul_(3)</span>
            <span class="c1">#         return x, y</span>
            <span class="c1">#    a = torch.ones(1&#39;000&#39;000)</span>
            <span class="c1">#    x, y = out(a[0:9], a[1:10])</span>
            <span class="c1">#</span>
            <span class="c1"># It would be much better to add copy_() calls into the graph for the two tiny slices, instead of materializing</span>
            <span class="c1"># a giant &quot;updated synthetic base&quot; and copying into a&#39;s entire storage.</span>
            <span class="n">primals_unpacked</span> <span class="o">=</span> <span class="n">unpack_synthetic_bases</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">synthetic_base_info</span><span class="p">)</span>
            <span class="n">f_primals_unpacked</span> <span class="o">=</span> <span class="n">unpack_synthetic_bases</span><span class="p">(</span><span class="n">f_primals</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">synthetic_base_info</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">f_primals_unpacked</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inpt_old</span><span class="p">,</span> <span class="n">inpt_f</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">primals_unpacked</span><span class="p">,</span> <span class="n">f_primals_unpacked</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">)</span>
                <span class="n">inpt_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                    <span class="c1"># We found an input that had a (data-only) mutation.</span>
                    <span class="c1"># Since keep_input_mutations is set, we need to faithfully apply a copy_()</span>
                    <span class="c1"># so the compiler will see the input mutation in the graph.</span>
                    <span class="k">assert</span> <span class="n">inpt_new</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">inpt_old</span>
                    <span class="k">assert</span> <span class="n">has_same_metadata</span><span class="p">(</span><span class="n">inpt_new</span><span class="p">,</span> <span class="n">inpt_old</span><span class="p">)</span>
                    <span class="n">inpt_old</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inpt_new</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_outs</span><span class="p">)</span>

    <span class="c1"># the joint needs have args named &quot;primals&quot; and &quot;tangents&quot;,</span>
    <span class="c1"># which are hardcoded into the partitioning logic.</span>
    <span class="k">def</span> <span class="nf">traced_joint</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">traced_forward</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">traced_joint</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">traced_forward</span>


<span class="k">def</span> <span class="nf">normalize_as_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>


<span class="n">aot_autograd_decompositions</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># This is a list since looking forward, we can have this arbitrarily nested.</span>
<span class="n">graph_being_compiled</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># TODO: It would be nice to reset the numbering every time aot_id goes</span>
<span class="c1"># up, but this is annoying to do right now (because we don&#39;t know if</span>
<span class="c1"># an aot_id will come back from the dead), so right now this also happens</span>
<span class="c1"># to be a globally unique number too (at the cost of wobbling if you change</span>
<span class="c1"># how the graphs compile)</span>
<span class="n">nth_graph</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>


<span class="k">def</span> <span class="nf">set_model_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">model_name</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">get_aot_compilation_context</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">),</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">nth_graph</span>


<span class="k">def</span> <span class="nf">get_aot_graph_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the name of the graph being compiled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">graph_being_compiled</span><span class="p">,</span> <span class="n">nth_graph</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">__</span><span class="si">{</span><span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">nth_graph</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="n">get_graph_being_compiled</span> <span class="o">=</span> <span class="n">get_aot_graph_name</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">graph_being_compiled</span>
    <span class="c1"># TODO: Don&#39;t shove the aot_id in here; set it in the context</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="k">yield</span>
    <span class="k">global</span> <span class="n">nth_graph</span>
    <span class="n">nth_graph</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">make_boxed_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">make_boxed_compiler</span><span class="p">(</span><span class="n">compiler</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiler</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">):</span>
        <span class="n">out_f</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fx_g</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">call_func_with_args</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">steal_args</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">steal_args</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
        <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Please remove soon</span>
            <span class="c1"># https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Your compiler for AOTAutograd is returning a a function that doesn&#39;t take boxed arguments. &quot;</span>
                <span class="s2">&quot;Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. &quot;</span>
                <span class="s2">&quot;See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.&quot;</span>
            <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">guard</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">AOTConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for AOTDispatcher</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">aot_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>

<span class="k">def</span> <span class="nf">aot_dispatch_base</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
        <span class="n">_fw_metadata</span><span class="p">,</span> <span class="n">_out</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
            <span class="n">flat_fn</span><span class="p">,</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="p">)(</span>
            <span class="o">*</span><span class="n">flat_args</span>
        <span class="p">)</span>

    <span class="n">_input_info</span> <span class="o">=</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">input_info</span>

    <span class="n">flat_args_with_views_handled</span><span class="p">,</span> <span class="n">_synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">metadata_</span> <span class="o">=</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">(</span>
        <span class="n">synthetic_base_info</span><span class="o">=</span><span class="n">_synthetic_base_info</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="o">=</span><span class="n">_fw_metadata</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># aot_dispatch_base requires functionalization, but doesn&#39;t need to handle as many cases as the autograd case.</span>
    <span class="c1"># The cases that aot_dispatch_base doesn&#39;t need to handle include:</span>
    <span class="c1"># - outputs that are aliases of graph intermediates</span>
    <span class="c1"># - outputs that are aliases of graph inputs</span>
    <span class="c1"># While cases that it does need to handle include:</span>
    <span class="c1"># - input mutations (including when inputs are aliases of each other)</span>
    <span class="c1"># - input metadata mutations</span>
    <span class="n">trace_fn</span> <span class="o">=</span> <span class="n">create_forward_or_joint_functionalized</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="n">metadata_</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
        <span class="n">fw_module</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">trace_fn</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span><span class="o">*</span><span class="n">flat_args_with_views_handled</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">:</span>
        <span class="c1"># As long as we opted to remove input mutations, then</span>
        <span class="c1"># there should be *NO* mutating ops in the graph at this point.</span>
        <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
        <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
        <span class="n">fw_module</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_graphs</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;====== Forward (only) graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>

    <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;inference&quot;</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span><span class="n">fw_module</span><span class="p">,</span> <span class="n">flat_args_with_views_handled</span><span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_runtime_wrapper</span><span class="p">(</span>
        <span class="n">compiled_fw</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">metadata_</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="k">def</span> <span class="nf">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">,</span> \
                <span class="sa">f</span><span class="s1">&#39;aot_autograd expected to have an entirely functional graph, but found </span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">format_node</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_autocast_manager</span><span class="p">():</span>
    <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">guard</span>


<span class="k">def</span> <span class="nf">are_differentiable_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">or</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span> <span class="ow">or</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">same_dtype_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="c1"># Note [Handling mutations on an input that aliases other inputs]</span>
<span class="c1"># The easiest example to show-case this edge case is here:</span>
<span class="c1">#</span>
<span class="c1"># def f(a, b):</span>
<span class="c1">#     a.mul_(2)</span>
<span class="c1">#     out = a + b</span>
<span class="c1">#     return out</span>
<span class="c1"># b = torch.ones(...)</span>
<span class="c1"># a = b.view(-1)</span>
<span class="c1"># f(a, b)</span>
<span class="c1">#</span>
<span class="c1"># In this situation, if a and b happened to be aliased, we need to trace something different!</span>
<span class="c1"># Suppose we had b = a.view(-1)</span>
<span class="c1"># (In this case, that means that `a._base is b`)</span>
<span class="c1">#</span>
<span class="c1"># We need to ensure that the aliasing relationship between a and b is preserved.</span>
<span class="c1"># We do that detecting the specific situation above (mutate an input that aliases another input),</span>
<span class="c1"># and when we do that, we create a synthetic base argument. Then inside of the traced forward,</span>
<span class="c1"># we regenerate a and b off of that base.</span>
<span class="c1"># The complete example of the transformed function looks like this:</span>
<span class="c1">#</span>
<span class="c1"># // The traced forward takes in a synthetic base, and regenerates the aliased inputs as views</span>
<span class="c1"># // We could consider getting view-replay support here to minimize as_strided_scatter ops in the graph</span>
<span class="c1"># def traced_forward(base):</span>
<span class="c1">#     a = base.as_strided(...)</span>
<span class="c1">#     b = base.as_strided(...)</span>
<span class="c1">#     a_updated = a.mul(2)</span>
<span class="c1">#     base_updated = torch.as_strided_scatter(base, a_updated, ...)</span>
<span class="c1">#     b_updated = base_updated.as_strided(...)</span>
<span class="c1">#     out = a_updated + b_updated</span>
<span class="c1">#     return a_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_fn(a, b):</span>
<span class="c1">#     // we detect that a is the &quot;differentiable base&quot; here</span>
<span class="c1">#     base = a</span>
<span class="c1">#     // In other situations, we might do either:</span>
<span class="c1">#     // (1) a and b are both views off of some larger differentiable base</span>
<span class="c1">#     //     assert a._base is b._base and a._base is not None</span>
<span class="c1">#     //     base = a._base</span>
<span class="c1">#     // (2) a and b both don&#39;t require gradients. Create a base from the storage</span>
<span class="c1">#     //     assert a._base is None and b._base is None</span>
<span class="c1">#     //     base = torch.Tensor(a.storage())</span>
<span class="c1">#     a_updated, out = traced_forward(base)</span>
<span class="c1">#     a.copy_(a_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># This function:</span>
<span class="c1"># (1) Merges input views into a synthetic base argument, when any of those input views are mutated</span>
<span class="c1"># (2) Returns metadata telling the autograd.Function how to modify their arguments properly,</span>
<span class="c1">#     to respect the new calling convention.</span>
<span class="c1">#</span>
<span class="c1"># The calling convention is as follows.</span>
<span class="c1"># Any inputs that were originally views of one another get yanked, and replaced with a synthetic base.</span>
<span class="c1"># The argument list ordering goes [base1, ..., baseN], [arg1, ..., argN],</span>
<span class="c1"># Where the ordering of the bases is determined from the ordering of the original view args.</span>
<span class="c1"># baseA will come before baseB if the earliest original argument coming from baseA</span>
<span class="c1"># showed up earlier in the argument list than the earliest original argument coming from baseB.</span>
<span class="c1">#</span>
<span class="c1"># Example, given some tensors a, b, c, d</span>
<span class="c1"># call site:</span>
<span class="c1">#   f(a, c.view(-1), b.view(-1), b, c, d)</span>
<span class="c1"># Modified argument list:</span>
<span class="c1">#   c_base comes first because the first c view came earlier in arg list than the first b view</span>
<span class="c1">#   a and d still show up in the modified arg list, but b and c don&#39;t- they&#39;re regenerated from their bases</span>
<span class="c1">#   b_base = torch.Tensor(b.storage())</span>
<span class="c1">#   c_base = torch.Tensor(c.storage())</span>
<span class="c1">#   f(c_base, b_base, a, d)</span>
<span class="k">def</span> <span class="nf">merge_view_inputs</span><span class="p">(</span>
    <span class="n">fwd_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">mutated_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># The autograd case currently has more restrictions than the inference case.</span>
    <span class="n">is_inference</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]]:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_input_info</span><span class="p">)</span>
    <span class="n">storage_ref_to_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">base_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">other_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">storage_ref</span> <span class="o">=</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
            <span class="n">storage_ref_to_idx</span><span class="p">[</span><span class="n">storage_ref</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span>
    <span class="c1"># Note [Synthetic Base Info Metadata]</span>
    <span class="c1"># This list contains metadata that tells you what the i&#39;th argument in the inner calling convention should be.</span>
    <span class="c1"># It&#39;s either:</span>
    <span class="c1"># - another int (corresponding to the index in the argument list of the element from the outer calling convention)</span>
    <span class="c1"># - idx, view_tensor, where we can generate the new output with view_tensor._view_func(old_args[idx])</span>
    <span class="c1">#   idx corresponds to which synthetic base from the outer calling context to view</span>
    <span class="n">inner_calling_convention_meta</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">aliased_input_indices</span> <span class="ow">in</span> <span class="n">storage_ref_to_idx</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
            <span class="c1"># We only care about mutations that affect all aliases,</span>
            <span class="c1"># so metadata mutations on an input doesn&#39;t require us to do synthetic base handling.</span>
            <span class="n">mutated_input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
            <span class="k">for</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
        <span class="p">):</span>
            <span class="k">for</span> <span class="n">curr_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
                <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">])</span>
            <span class="k">continue</span>
        <span class="c1"># We detected an input that was mutated, AND aliases with another input.</span>
        <span class="c1"># we need to replace this set of aliased inputs with a single synthetic base.</span>
        <span class="c1"># For now, I&#39;m banning a bunch of cases. We expect dynamo to properly detect these cases</span>
        <span class="c1"># and error out. We can fix them later.</span>
        <span class="c1"># These checks are transitive, so we don&#39;t need to check every pair.</span>
        <span class="k">for</span> <span class="n">idx1</span><span class="p">,</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">,</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">view1</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx1</span><span class="p">]</span>
            <span class="n">view2</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span>
            <span class="c1"># The &quot;inputs that are aliased but have different differentiable bases&quot; case</span>
            <span class="c1"># is more complicated and hopefully pretty rare. Not currently handled.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_inference</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">are_differentiable_views</span><span class="p">(</span>
                    <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="c1"># Regenerating views when reinterpreting complex / real tensors seems non-trivial,</span>
            <span class="c1"># not handling for now</span>
            <span class="k">assert</span> <span class="n">same_dtype_views</span><span class="p">(</span>
                <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
            <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle input mutations on views with different dtypes.&quot;</span>
        <span class="n">non_none_bases</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
            <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="n">aliases_with_none_bases</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span> <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_none_bases</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Case where none of the aliases have a ._base</span>
            <span class="c1"># we generate a synthetic base without gradients, and generate views off of it</span>
            <span class="c1"># We hit this case when we have input tensors to the graph that share a storage,</span>
            <span class="c1"># but do not have a ._base field.</span>
            <span class="c1"># Wondering when we hit this case?</span>
            <span class="c1"># The _base field simply says that autograd knows about the aliasing relationship,</span>
            <span class="c1"># but sometimes we create tensors which are aliased out of the same storage but guaranteed</span>
            <span class="c1"># to be disjoint. In these cases, we will skip setting up the _base relationship</span>
            <span class="c1"># for performance reasons (because the fact that the tensors share the same storage</span>
            <span class="c1"># is unobservable unless you (1) do naughty things with resize_/as_strided</span>
            <span class="c1"># or (2) look at the storage--as we are doing here.)</span>
            <span class="c1"># One particular example of this is optimizer steps on the LSTM module:</span>
            <span class="c1"># LSTM parameters are packed into a contiguous storage for efficiency reasons when</span>
            <span class="c1"># calling cuDNN kernels, so when these parameters get passed to the optimizer we will</span>
            <span class="c1"># find they share the same storage, but do not have _base set since they are all disjoint.</span>
            <span class="c1">#</span>
            <span class="c1"># NOTE: There is one case where this is unsafe:</span>
            <span class="c1"># torch.Tensor(storage) will ALWAYS create a 1D tensor, which is not necessarily</span>
            <span class="c1"># the same shape as the &quot;actual&quot; base that the tensor came from.</span>
            <span class="c1"># For the most part this is fine, because we always use as_strided()</span>
            <span class="c1"># to generate the original aliased inputs again.</span>
            <span class="c1"># If we were to use view-replay though, this could cause the aliased views</span>
            <span class="c1"># to have incorrect sizes.</span>
            <span class="n">example_idx</span> <span class="o">=</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">example_alias</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span>
            <span class="c1"># Note that this function is re-used at both trace time and rutnime.</span>
            <span class="c1"># At trace time, we&#39;re under a FakeMode so synthetic_base becomes a FakeTensor.</span>
            <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">example_alias</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">example_alias</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># We don&#39;t actually have a convenient way of going from storage -&gt; tensor,</span>
            <span class="c1"># So using set_() here (we suffer some minor overhead, but this case is rare).</span>
            <span class="n">synthetic_base</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">example_alias</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Case where all of the aliases require gradients, and have the same _base.</span>
            <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">other_base</span> <span class="ow">in</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">other_base</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="k">for</span> <span class="n">alias</span> <span class="ow">in</span> <span class="n">aliases_with_none_bases</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">alias</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
        <span class="n">base_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">synthetic_base</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">curr_view_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
            <span class="n">curr_view</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span>
            <span class="n">base_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="c1"># We store just enough info here so that we can regenerate the view later.</span>
            <span class="c1"># Regeneration: curr_view._view_func(args[base_idx])</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">base_idx</span><span class="p">,</span> <span class="n">curr_view</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">other_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span>
        <span class="c1"># If no synthetic bases are necessary, just return the original inputs.</span>
        <span class="k">return</span> <span class="n">fwd_inputs</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, return:</span>
        <span class="c1"># (1) The new args according to the updated calling convention: (synthetic_bases, other_args)</span>
        <span class="c1"># (2) Metadata telling functionalization how to generate the inner argument list given the outer calling convention.</span>
        <span class="c1">#     We post-process it into a list, where meta[i] tells you info about the i&#39;th argument in the inner calling convention.</span>
        <span class="n">args_to_functionalization</span> <span class="o">=</span> <span class="n">base_args</span> <span class="o">+</span> <span class="n">other_args</span>
        <span class="n">arg_to_old_idx_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">other_arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">other_args</span><span class="p">):</span>
            <span class="n">new_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span>
            <span class="n">old_idx</span> <span class="o">=</span> <span class="n">arg_to_old_idx_map</span><span class="p">[</span><span class="n">other_arg</span><span class="p">]</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">old_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_idx</span>
        <span class="c1"># post process into a list</span>
        <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_calling_convention_meta</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inner_calling_convention_meta</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">post_processed_calling_convention_meta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># Quick assert: every argument in the inner calling convention should be accounted for.</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">args_to_functionalization</span><span class="p">,</span> <span class="n">post_processed_calling_convention_meta</span>


<span class="k">def</span> <span class="nf">format_guard_bug_msg</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;At compilation time, graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> was compiled under the &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;assumption that </span><span class="si">{</span><span class="n">expected</span><span class="si">}</span><span class="s2">, but at runtime this was not the case.  &quot;</span>
        <span class="s2">&quot;This indicates a guard bug in AOTAutograd or Dynamo, please file a bug to PyTorch.&quot;</span>
    <span class="p">)</span>


<span class="c1"># MOTIVATION:</span>
<span class="c1">#</span>
<span class="c1"># When tracing functions for future execution, one must be careful not to pass</span>
<span class="c1"># in the same input tensor multiple times (e.g., f(x, x), as this can result</span>
<span class="c1"># in graphs that are ONLY valid if you later pass a new tensor in exactly the</span>
<span class="c1"># same way (e.g., f(y, y)).  (NB: we really mean duplicate; two distinct</span>
<span class="c1"># tensors that alias each other is a different situation that is covered by</span>
<span class="c1"># aot_dispatch_deduplicated_autograd). Here are two examples:</span>
<span class="c1">#</span>
<span class="c1"># (1) Suppose you have a function:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return x + y</span>
<span class="c1">#</span>
<span class="c1"># If you make_fx(f)(x, x), you will trace out:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return y + y</span>
<span class="c1">#</span>
<span class="c1"># Oops!</span>
<span class="c1">#</span>
<span class="c1"># (2) For most tensors x and y, you can compute f&#39;s gradient with respect to</span>
<span class="c1"># these to inputs by saying torch.autograd.grad(f(x, y), (x, y)).  However,</span>
<span class="c1"># if x is y, you will trace out a program that gets incorrect gradients:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; x = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + x, (x, x))</span>
<span class="c1">#   (tensor([2.]), tensor([2.]))</span>
<span class="c1">#</span>
<span class="c1"># In other words, the gradient is double-counted.  Deduplicating the arguments</span>
<span class="c1"># gives you an appropriate gradient:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; y = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + y, (x, y))</span>
<span class="c1">#   (tensor([1.]), tensor([1.]))</span>
<span class="c1">#</span>
<span class="c1"># HOW TO DEDUPLICATE:</span>
<span class="c1">#</span>
<span class="c1"># There are a few strategies, in order of preference:</span>
<span class="c1">#</span>
<span class="c1"># 1. For every duplicate argument to the function, detach it into</span>
<span class="c1">#    a separate leaf tensor, so that it is no longer duplicated.</span>
<span class="c1">#</span>
<span class="c1">#       PRO: The resulting compiled graph works for any configuration</span>
<span class="c1">#       of duplicated arguments.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the metadata of inputs:</span>
<span class="c1">#</span>
<span class="c1">#           def f(x, y):</span>
<span class="c1">#               x.transpose_(0, 1)</span>
<span class="c1">#               y.transpose_(0, 2)</span>
<span class="c1">#</span>
<span class="c1">#           x = torch.randn(2, 3, 4)</span>
<span class="c1">#           f(x, x)</span>
<span class="c1">#</span>
<span class="c1">#       The ordering of the transposes inside f dictates whether or not</span>
<span class="c1">#       you get [4, 2, 3] or [3, 4, 2].  This means that you cannot precompute</span>
<span class="c1">#       what metadata mutations should get applied to each input; you need to</span>
<span class="c1">#       assume they aren&#39;t duplicates (what we do today) or preserve</span>
<span class="c1">#       the original metadata mutations exactly in order, so that they work</span>
<span class="c1">#       for any duplicate configuration.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the data of inputs.</span>
<span class="c1">#       In particular, leaf tensors that require grad cannot be mutated,</span>
<span class="c1">#       this makes it impossible to differentiate with respect to the original</span>
<span class="c1">#       base.</span>
<span class="c1">#</span>
<span class="c1"># 2. For every duplicate argument to the function, remove it, so it is</span>
<span class="c1">#    no longer part of the &quot;true&quot; signature:</span>
<span class="c1">#</span>
<span class="c1">#       PRO: Implemented naively, it still works for metadata/data mutation.</span>
<span class="c1">#</span>
<span class="c1">#       CON: The resulting compiled graph is duplicate-specialized: it only</span>
<span class="c1">#       works if future calls duplicate arguments in exactly the same way.</span>
<span class="c1">#       Horribly, Dynamo doesn&#39;t guard on this at the moment.  But even if</span>
<span class="c1">#       it did, you could still end up recompiling a bunch of each duplicate.</span>
<span class="c1">#</span>
<span class="c1"># Our strategy is to do (1) if we can, and do (2) otherwise, erroring if</span>
<span class="c1"># Dynamo&#39;s guards are not enough.  In practice, this seems to cover</span>
<span class="c1"># everything.</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">aot_wrapper_dedupe</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">compiler_fn</span>
<span class="p">):</span>
    <span class="c1"># Get information about whether or not flat_fn mutates its arguments</span>
    <span class="c1"># or not</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="n">fw_metadata</span><span class="p">,</span> <span class="n">_out</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="c1"># For the purpose of checking for dupes that are mutated,</span>
                <span class="c1"># we always want our metadata to correctly reflect input mutations</span>
                <span class="n">keep_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)(</span>
                <span class="o">*</span><span class="n">flat_args</span>
            <span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Failed to collect metadata on function, produced code may be suboptimal.  &quot;</span>
            <span class="s2">&quot;Known situations this can occur are inference mode only compilation involving &quot;</span>
            <span class="s2">&quot;resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); &quot;</span>
            <span class="s2">&quot;if your situation looks different please file a bug to PyTorch.&quot;</span><span class="p">,</span>
            <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Analysis failed, fall back to duplicate specialize</span>
        <span class="c1"># TODO: Known analysis problems:</span>
        <span class="c1">#   - resize_: TestInductorOpInfoCPU.test_comprehensive_resize__cpu_bool</span>
        <span class="c1">#   - prims: test_tmp_not_defined_issue1_cpu</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Strategy 1: For any input that is not mutated, we can leafify it if we</span>
        <span class="c1"># need to remove a duplicate.</span>
        <span class="n">leaf_flat_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">args_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">ok</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">a</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args_set</span><span class="p">:</span>
                <span class="n">args_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
                <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ok</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">ok</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">leaf_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

    <span class="c1"># Strategy 2: Duplicate specialize.</span>
    <span class="c1">#</span>
    <span class="c1"># In Haskell types, suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   add_dupe_args :: DedupedArgs -&gt; Args</span>
    <span class="c1">#   remove_dupe_args :: Args -&gt; DedupedArgs</span>
    <span class="c1">#</span>
    <span class="c1">#   compiler_fn</span>
    <span class="c1">#       :: (DedupedArgs -&gt; R) -&gt; DedupedArgs -&gt; AOTConfig -&gt; (DedupedArgs -&gt; R)</span>
    <span class="c1">#   deped_compiler_fn</span>
    <span class="c1">#       :: (Args -&gt; R) -&gt; Args -&gt; AOTConfig -&gt; (Args -&gt; R)</span>
    <span class="c1">#</span>
    <span class="c1"># Then the code below can be written in point-free style as:</span>
    <span class="c1">#</span>
    <span class="c1">#   deduped_compiler_fn f a c =</span>
    <span class="c1">#       compiler_fn (f . add_dupe_args) (remove_dupe_args a) c . remove_dupe_args</span>
    <span class="c1">#</span>
    <span class="c1"># Suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># We want:</span>
    <span class="c1">#</span>
    <span class="c1">#   remove_dupe_args([a, b, a, c]) == [a, b, c]</span>
    <span class="c1">#   add_dupe_args([a, b, c]) == [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># This is done via (respectively):</span>
    <span class="c1">#</span>
    <span class="c1">#   seen_args = {a: 0, b: 1, c: 2}</span>
    <span class="c1">#   add_dupe_map = {  # how to get args from the deduped list</span>
    <span class="c1">#       0: 0,</span>
    <span class="c1">#       1: 1,</span>
    <span class="c1">#       2: 0,</span>
    <span class="c1">#       3: 2,</span>
    <span class="c1">#   }</span>
    <span class="c1">#   keep_arg_mask = [True, True, False, True]</span>

    <span class="n">seen_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">keep_arg_mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">add_dupe_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">duped_arg_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># index into deduped_flat_args</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seen_args</span><span class="p">:</span>
            <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="k">continue</span>
        <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">unique_args</span> <span class="o">=</span> <span class="n">j</span>

    <span class="c1"># NB: Hot path, avoid set lookups here</span>
    <span class="c1"># TODO: Can avoid the zip here too, probably</span>
    <span class="k">def</span> <span class="nf">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">keep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duped_arg_len</span><span class="p">)]</span>

    <span class="n">deduped_flat_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">tracing_context</span> <span class="o">=</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">tracing_context</span><span class="p">:</span>
        <span class="c1"># TODO(voz): This structure is 1:1, we could consider an alternate structure like</span>
        <span class="c1"># kept_pos:[dupe_arg_pos], however, add_dupe_map is 1:1 so we would need a new structure there,</span>
        <span class="c1"># which feels like needless complexity for a tiny bit of efficiency at this point.</span>
        <span class="k">for</span> <span class="n">dupe_arg_pos</span><span class="p">,</span> <span class="n">kept_pos</span> <span class="ow">in</span> <span class="n">add_dupe_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">dupe_arg_dict</span> <span class="o">=</span> <span class="n">flat_args</span><span class="p">[</span><span class="n">dupe_arg_pos</span><span class="p">]</span><span class="o">.</span><span class="vm">__dict__</span>
            <span class="n">kept_arg_dict</span> <span class="o">=</span> <span class="n">flat_args</span><span class="p">[</span><span class="n">kept_pos</span><span class="p">]</span><span class="o">.</span><span class="vm">__dict__</span>
            <span class="k">if</span> <span class="s1">&#39;graph_arg_pos&#39;</span> <span class="ow">in</span> <span class="n">dupe_arg_dict</span> <span class="ow">and</span> <span class="s1">&#39;graph_arg_pos&#39;</span> <span class="ow">in</span> <span class="n">kept_arg_dict</span><span class="p">:</span>
                <span class="n">d_positions</span> <span class="o">=</span> <span class="n">dupe_arg_dict</span><span class="p">[</span><span class="s1">&#39;graph_arg_pos&#39;</span><span class="p">]</span>
                <span class="n">k_positions</span> <span class="o">=</span> <span class="n">kept_arg_dict</span><span class="p">[</span><span class="s1">&#39;graph_arg_pos&#39;</span><span class="p">]</span>
                <span class="k">assert</span><span class="p">(</span><span class="n">d_positions</span> <span class="o">==</span> <span class="n">k_positions</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">d_positions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">d_positions</span><span class="p">)):</span>
                        <span class="n">pos</span> <span class="o">=</span> <span class="n">d_positions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                        <span class="n">pre_pos</span> <span class="o">=</span> <span class="n">d_positions</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                        <span class="n">tracing_context</span><span class="o">.</span><span class="n">guards_context</span><span class="o">.</span><span class="n">aotautograd_guards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">DuplicateInputs</span><span class="p">(</span><span class="n">pre_pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">))</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">wrapped_flat_fn</span><span class="p">,</span> <span class="n">deduped_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">deduped_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">deduped_args</span><span class="p">)</span>

    <span class="n">wrapped_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># This can be uncommented when we properly guard for duplicates,</span>
    <span class="c1"># but right now we must not do it.</span>
    <span class="c1"># if not config.debug_assert:</span>
    <span class="c1">#     return wrapped_compiled_fn</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">wrapped_compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debugged_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Test that the computed remove/add arg functions are an inverse</span>
        <span class="n">new_args</span> <span class="o">=</span> <span class="n">add_dupe_args</span><span class="p">(</span><span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_args</span><span class="p">,</span> <span class="n">args</span><span class="p">)):</span>
            <span class="n">seen</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">y</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                <span class="n">aot_config</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would be a duplicate of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># This is only an error if there is metadata mutation on both of</span>
        <span class="c1"># the duped arguments; in this case, we need to know what order</span>
        <span class="c1"># the metadata mutation applies in.  You&#39;ll get the correct result</span>
        <span class="c1"># otherwise, because a graph that assumes distinct inputs works if</span>
        <span class="c1"># you dupe the inputs (the gradient contributions from each input</span>
        <span class="c1"># will get summed up appropriately.)</span>
        <span class="c1">#</span>
        <span class="c1"># TODO: work out how to setup this assert correctly</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        assert len(seen) == unique_args, format_guard_bug_msg(aot_config,</span>
<span class="sd">            f&quot;there would be {unique_args} distinct arguments&quot;</span>
<span class="sd">        )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">debugged_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">debugged_compiled_fn</span>


<span class="k">def</span> <span class="nf">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;parameter/buffer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;input </span><span class="si">{</span><span class="n">i</span> <span class="o">-</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># The wrapper created by this function handles all of the runtime aliasing and mutation &quot;epilogue&quot; logic</span>
<span class="c1"># that needs to run after the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># This function accepts a trace_joint flag, indicating whether or not we&#39;re generating the runtime</span>
<span class="c1"># epilogue for a forward-only inference graph, or for an autograd.Function.apply function.</span>
<span class="c1"># This is because there are some minor differences in how we treat these cases at runtime:</span>
<span class="c1"># - resize_() is currently handled in the inference case, but not fully handled in the autograd case.</span>
<span class="c1"># - the autograd cases inserts TensorAlias wrapper objects for outputs that alias inputs</span>
<span class="k">def</span> <span class="nf">create_runtime_wrapper</span><span class="p">(</span>
    <span class="n">compiled_fn</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">runtime_metadata</span><span class="p">:</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">,</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">runtime_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Step 2: remove aliased inputs that are mutated, replace with synthetic bases</span>
        <span class="c1"># Only happens if our graph mutates an input that aliases another input.</span>
        <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Given: the original args, including at least one pair of inputs that are aliased</span>
            <span class="c1"># and get subsequently mutated.</span>
            <span class="c1"># Generate: the updated args, including (potentially multiple) synthetic bases</span>
            <span class="c1"># that replace the views. The input views are regenerated manually in the compiled function.</span>
            <span class="c1"># TODO: think harder about what happens if (a view of) one of these mutated input views is ALSO returned</span>
            <span class="n">new_inputs</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="ow">not</span> <span class="n">trace_joint</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># We&#39;re just re-running the original-args-to-synthetic-base transformation</span>
            <span class="c1"># that we ran during compilation.</span>
            <span class="c1"># This returns metadata that we use during tracing to recover the input views,</span>
            <span class="c1"># which we don&#39;t actually need at runtime.</span>
            <span class="k">assert</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">args_with_synthetic_bases</span> <span class="o">=</span> <span class="n">new_inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args_with_synthetic_bases</span> <span class="o">=</span> <span class="n">args</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_force_original_view_tracking</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">all_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">compiled_fn</span><span class="p">,</span>
                <span class="n">args_with_synthetic_bases</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
        <span class="n">num_metadata_mutated_inps</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_inputs</span>
        <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>

        <span class="k">if</span> <span class="n">keep_input_mutations</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
                <span class="o">==</span> <span class="n">num_metadata_mutated_inps</span> <span class="o">+</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_metadata_mutated_inps</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
                <span class="o">==</span> <span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_mutated_inps</span>
            <span class="p">)</span>
        <span class="c1"># Step 3: After running the compiled fw, apply updates to mutated inputs</span>
        <span class="n">num_mutations_to_apply</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_mutations_to_apply</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">updated_inputs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[:</span> <span class="n">num_mutations_to_apply</span><span class="p">]</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[</span><span class="n">num_mutations_to_apply</span> <span class="p">:]</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
            <span class="p">):</span>
                <span class="n">meta</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">original_inpt</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="c1"># TODO: add better resize_() support for autograd case.</span>
                <span class="c1"># Check for the case when an input has been resized.</span>
                <span class="c1"># Note: One important thing to check for is user code that calls inpt.storage().resize_().</span>
                <span class="c1"># We can&#39;t trace operations on storage into the graph, so we should get dynamo to graph break.</span>
                <span class="c1"># TODO: handle resize_() on inputs to a larger size.</span>
                <span class="c1"># This is actually non-trivial to detect, so we should probably just handle it</span>
                <span class="c1"># (or make dynamo detect).</span>
                <span class="c1"># We can&#39;t just check of original_inpt.storage_size != updated_inpt.storage_size,</span>
                <span class="c1"># Because the original_inpt might be a view of some larger tensor,</span>
                <span class="c1"># and updated_inpt is always densely packed.</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">trace_joint</span> <span class="ow">and</span> <span class="n">original_inpt</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span>
                        <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">alias</span>
                    <span class="c1"># We need to grab the size/stride/storage_offset from the compiled forward,</span>
                    <span class="c1"># and use that to mutate the metadata of the input</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">all_outs</span>

        <span class="c1"># Step 4: Manually regenerate any outputs that are aliased to inputs, instead of</span>
        <span class="c1"># compiling them.</span>
        <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># The compiled forward also returned intermediate bases. We don&#39;t want to return them to the user.</span>
            <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">fw_outs_no_intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                    <span class="p">:</span> <span class="o">-</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
                <span class="p">]</span>
                <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fw_outs_no_intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span>
                <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_no_intermediate_bases</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>

            <span class="n">fw_outs_including_aliases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
                <span class="n">fw_outs_no_intermediate_bases</span><span class="p">,</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span>
            <span class="p">)):</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">:</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span>
                    <span class="n">o_</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">alias</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">o_</span> <span class="o">=</span> <span class="n">o</span>
                <span class="n">o_grad</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">:</span>
                    <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">o_grad</span><span class="p">)</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">:</span>
                    <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span><span class="p">:</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">intermediate_bases</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">:</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">intermediate_bases</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">fw_outs_no_intermediate_bases</span>
                <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">base_tensor_list</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                <span class="c1"># TODO: handle the custom autograd function case here.</span>
                <span class="c1"># We need a way to check whether a tensor came from a custom autograd fn from python,</span>
                <span class="c1"># AND a way to replay that custom view fn.</span>
                <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">o_grad</span><span class="p">)</span>
                <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">fw_outs_including_aliases</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fw_outs</span>
    <span class="k">return</span> <span class="n">runtime_wrapper</span>

<span class="c1"># Has the precondition that there</span>
<span class="c1"># are no duplicate arguments in flat_args (e.g., the same Tensor</span>
<span class="c1"># object never shows up twice.  However, two tensor inputs MAY alias</span>
<span class="c1"># the same storage, so long as they have separate TensorImpls.)</span>
<span class="k">def</span> <span class="nf">aot_dispatch_autograd</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
        <span class="n">_fw_metadata</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
            <span class="n">flat_fn</span><span class="p">,</span>
            <span class="c1"># Note: in the non-inference path, we are currently not passing input mutations into the graph directly.</span>
            <span class="c1"># This is mainly difficult due to the partitioner, but we are leaving (a bit of) perf on the table.</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)(</span>
            <span class="o">*</span><span class="n">flat_args</span>
        <span class="p">)</span>


    <span class="c1"># out here corresponds to the set of outputs in the traced forward that should get grad_outputs in the traced backward.</span>
    <span class="c1"># It includes outputs of the original forward, *and* any updated inputs due to input mutations.</span>
    <span class="c1"># However, it does *not* include any outputs that are aliases of inputs or intermediates, or any metadata-only input mutations.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># merge_view_inputs() is used again at runtime to create synthetic bases out of aliased inputs.</span>
    <span class="c1"># This code only executes at runtime if we have graph inputs that alias each other, and one of those inputs</span>
    <span class="c1"># gets its data mutated.</span>
    <span class="c1"># When that happens, we replace the aliased inputs with a synthetic base, and in the traced forward</span>
    <span class="c1"># we later generate the input views</span>
    <span class="n">flat_args_with_views_handled</span><span class="p">,</span> <span class="n">_synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># pre-compute, so we can bail out quickly in the hotpath</span>
    <span class="n">metadata_</span> <span class="o">=</span> <span class="n">CompiledRuntimeMetadata</span><span class="p">(</span>
        <span class="n">synthetic_base_info</span><span class="o">=</span><span class="n">_synthetic_base_info</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="o">=</span><span class="n">_fw_metadata</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">_fw_metadata</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">)</span> <span class="o">==</span> <span class="n">metadata_</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">metadata_</span><span class="o">.</span><span class="n">num_outputs</span>

    <span class="n">joint_forward_backward</span> <span class="o">=</span> <span class="n">create_forward_or_joint_functionalized</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="n">metadata_</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># For now in the autograd case, we NEVER keep input mutations (we could eventually fix this for slightly better perf</span>
        <span class="c1"># in some cases, but it&#39;s annoying to fix the partitioner)</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">joint_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">flat_args_with_views_handled</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_functionalize</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="n">flattened_joints</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">joint_inputs</span><span class="p">)</span>
            <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">joint_forward_backward</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span>
                <span class="o">*</span><span class="n">joint_inputs</span>
            <span class="p">)</span>

        <span class="c1"># There should be *NO* mutating ops in the graph at this point.</span>
        <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
        <span class="c1"># Redudant with the check above, but worth having in case tracing introduced</span>
        <span class="c1"># a fake tensor. Unlikely.</span>
        <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># joint_forward_backward() now always runs with functionalization, and factoring it out</span>
        <span class="c1"># to make that toggleable is a bit painful.</span>
        <span class="c1"># aot autograd without functionalization is wrong anyway, so we error.</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s2">&quot;Graph partitioning without functionalization is not sound, we may introduce errors&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_joint</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;====== Joint graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;joint&quot;</span><span class="p">):</span>
            <span class="n">num_inner_fwd_outputs</span> <span class="o">=</span> <span class="n">metadata_</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">metadata_</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">partition_fn</span><span class="p">(</span>
                <span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">,</span> <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="n">num_inner_fwd_outputs</span>
            <span class="p">)</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># we only need to bookkeep the symints that are saved for bw, not any symints</span>
            <span class="c1"># the user forward might have returned in its own output</span>
            <span class="n">fw_outs_saved_for_bw</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_inner_fwd_outputs</span><span class="p">:]</span>
            <span class="n">symint_outs_saved_for_bw</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_outs_saved_for_bw</span> <span class="k">if</span> <span class="n">is_sym_node</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">_num_symints_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_graphs</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;====== Forward graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;====== Backward graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">bw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">):</span>
            <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span>
                <span class="n">fw_module</span><span class="p">,</span> <span class="n">flat_args_with_views_handled</span>
            <span class="p">)</span>

    <span class="k">class</span> <span class="nc">CompiledFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiled_fw_func</span>
        <span class="n">compiled_bw</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="n">metadata_</span>
        <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">_num_symints_saved_for_bw</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">deduped_flat_tensor_args</span><span class="p">):</span>

            <span class="c1"># There is a pretty complicated calling convention around what the compiled fw returns.</span>
            <span class="c1"># The full list of outputs and their relative order is:</span>
            <span class="c1"># (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)</span>
            <span class="c1"># - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version</span>
            <span class="c1">#   of the original view, and not the synthetic base</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_fw</span><span class="p">,</span>
                <span class="n">deduped_flat_tensor_args</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span>
            <span class="n">num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span>
            <span class="p">)</span>
            <span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span>
            <span class="p">)</span>
            <span class="n">num_outputs_aliased</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span>
            <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span>
            <span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
            <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span>
            <span class="p">)</span>
            <span class="c1"># Our forward() returns both (mutated_inputs, outputs, output_intermediate_bases, saved_tensors, saved_symints)</span>
            <span class="n">num_forward_returns</span> <span class="o">=</span> <span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>

            <span class="k">assert</span> <span class="n">num_forward_returns</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">requires_grad_info</span>
            <span class="p">)</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>

            <span class="c1"># Partitioners must put symint arguments at the end separate from tensor arguments</span>
            <span class="k">if</span> <span class="n">num_symints_saved_for_bw</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">tensors_saved_for_backwards</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                    <span class="n">num_forward_returns</span><span class="p">:</span><span class="o">-</span><span class="n">num_symints_saved_for_bw</span>
                <span class="p">]</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="c1"># See Note [Detaching saved tensors in AOTAutograd]</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_is_view</span><span class="p">()</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">tensors_saved_for_backwards</span><span class="p">))</span>
                <span class="n">symint_outs</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">num_symints_saved_for_bw</span><span class="p">:]</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">))</span>
                        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symint_outs</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="n">symint_outs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tensors_saved_for_backwards</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_forward_returns</span><span class="p">:]</span>
                <span class="c1"># See Note [Detaching saved tensors in AOTAutograd]</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_is_view</span><span class="p">()</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">tensors_saved_for_backwards</span><span class="p">))</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">raw_returns</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_forward_returns</span><span class="p">]</span>

            <span class="c1"># Wrap all autograd.Function.forward() outputs that are aliases</span>
            <span class="c1"># so that autograd.Function doesn&#39;t treat them as tensors</span>
            <span class="k">if</span> <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
                <span class="p">):</span>
                    <span class="c1"># We could make this faster by only looping over inputs with metadata-only mutations</span>
                    <span class="c1"># (instead of looping over inputs with either data or metadata mutations), but there shouldn&#39;t be many.</span>
                    <span class="n">info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                        <span class="n">raw_returns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorAlias</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
                    <span class="n">user_mutated_inputs_raw</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_mutated_inputs</span><span class="p">]</span>
                    <span class="n">mut_inp_infos</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
                    <span class="p">]</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_mutated_inputs_raw</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mut_inp_infos</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">aliased_out_indices</span><span class="p">:</span>
                    <span class="n">raw_return_idx</span> <span class="o">=</span> <span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">idx</span>
                    <span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorAlias</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
                    <span class="n">intermediates_raw</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span><span class="p">:]</span>
                    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">intermediates_raw</span><span class="p">)</span>

            <span class="c1"># invariant: intermediate bases always require gradients, so we don&#39;t have to</span>
            <span class="c1"># consider marking them as non-differentiable.</span>
            <span class="n">raw_returns_not_including_intermediate_bases</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[:</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span><span class="p">]</span>
            <span class="n">fw_outs_not_requiring_grad</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_returns_not_including_intermediate_bases</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs_not_requiring_grad</span><span class="p">)</span>

            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">)</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="c1"># Calling convention: we expect a grad_out passed to the backward:</span>
            <span class="c1"># - for every output of the fw that does *not* alias an input or graph intermediate</span>
            <span class="c1"># - for every updated_input generated by the fw that does *not* alias an input (aka only data-mutations)</span>
            <span class="c1"># - for every graph intermediate that we need to use to generate an output later.</span>
            <span class="c1"># The other outputs in the autograd.Function.forward that do *not* show up in the backward include:</span>
            <span class="c1"># - outputs that alias inputs or graph intermediates</span>
            <span class="c1"># - updated inputs due to metadata-only mutations.</span>
            <span class="c1"># We need to return them in the forward, but ensure that they all do not get gradients in the backward,</span>
            <span class="c1"># and we filter them out here before passing the remaining grad_outputs into the compiled backward.</span>
            <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
            <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">expected_grad_outs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>

            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected_grad_outs</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">or</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">inp_tangents</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">,</span> <span class="n">intermediate_base_tangents</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">flat_args</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_mutated_inps</span><span class="p">],</span>
                    <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span><span class="p">:</span><span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">],</span>
                    <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">:],</span>
                <span class="p">)</span>
                <span class="c1"># input_info contains info on *every* input,</span>
                <span class="c1"># But in the backward(), we are only given grad outputs for every mutated input.</span>
                <span class="c1"># We then need to filter out the grad outputs that correspond to metadata-only mutations.</span>
                <span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
                <span class="n">input_info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_inp_indices</span><span class="p">)</span>
                <span class="n">inp_tangents_filtered</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">x</span>
                    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info_idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inp_tangents</span><span class="p">,</span> <span class="n">mutated_inp_indices</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">input_info</span><span class="p">[</span><span class="n">info_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
                <span class="p">]</span>
                <span class="c1"># We also need to filter out grad outputs that correspond to outputs aliasing inputs/intermediates</span>
                <span class="n">out_info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span>
                <span class="n">out_tangents_filtered</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">x</span>
                    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out_tangents</span><span class="p">,</span> <span class="n">out_info</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="p">]</span>
                <span class="c1"># intermediate bases always require gradients, and always participate in the backward graph.</span>
                <span class="n">flat_bw_args</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">inp_tangents_filtered</span><span class="p">,</span> <span class="n">out_tangents_filtered</span><span class="p">,</span> <span class="n">intermediate_base_tangents</span><span class="p">)</span>

                <span class="c1"># sanity asserts</span>
                <span class="c1"># metadata_only_inps = [</span>
                <span class="c1">#     x for x, info_idx in zip(inp_tangents, mutated_inp_indices)</span>
                <span class="c1">#     if not input_info[info_idx].mutates_data</span>
                <span class="c1"># ]</span>
                <span class="c1"># aliased_outputs = [</span>
                <span class="c1">#     x for x, info in zip(out_tangents, out_info) if info.output_type != OutputType.non_alias]</span>
                <span class="c1"># assert all(x is None for x in metadata_only_inps)</span>
                <span class="c1"># assert all(x is None for x in aliased_outputs)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">flat_bw_args</span> <span class="o">=</span> <span class="n">flat_args</span>

            <span class="n">contiguous_args</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_bw_args</span>
            <span class="p">]</span>

            <span class="n">all_args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">contiguous_args</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">del</span> <span class="n">contiguous_args</span>

            <span class="k">def</span> <span class="nf">call_compiled_backward</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># TODO - pass in fake tensors ?</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                    <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">):</span>
                        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">bw_compiler</span><span class="p">(</span>
                            <span class="n">bw_module</span><span class="p">,</span> <span class="n">all_args</span>
                        <span class="p">)</span>

                <span class="n">ctx</span><span class="o">.</span><span class="n">maybe_clear_saved_tensors</span><span class="p">()</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span><span class="p">,</span>
                    <span class="n">all_args</span><span class="p">,</span>
                    <span class="n">steal_args</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">all_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
                <span class="c1"># Ensure that the graph is connected, and error if double backward is performed.</span>
                <span class="c1"># See comment for why once_differentiable is not sufficient:</span>
                <span class="c1"># https://github.com/pytorch/pytorch/pull/92348/files#r1072962107</span>
                <span class="k">class</span> <span class="nc">CompiledFunctionBackward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
                    <span class="nd">@staticmethod</span>
                    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">unused_args</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">call_compiled_backward</span><span class="p">()</span>

                    <span class="nd">@staticmethod</span>
                    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;torch.compile with aot_autograd does not currently support double backward&quot;</span><span class="p">)</span>
                <span class="c1"># Pass args even though they&#39;re unused, so that the graph is built</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">CompiledFunctionBackward</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="n">all_args</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">call_compiled_backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">out</span>

    <span class="n">compiled_function</span> <span class="o">=</span> <span class="n">create_runtime_wrapper</span><span class="p">(</span>
        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">metadata_</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiled_function</span>

    <span class="n">flat_requires_grad</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span>
    <span class="p">]</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_function</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debug_compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># TODO: Check aliasing relationships</span>
        <span class="c1"># TODO: Check strides for metadata mutation</span>
        <span class="c1"># (NB: ideally, this logic is factored out of this function and</span>
        <span class="c1"># you move these debug checks there)</span>

        <span class="c1"># Check requires grad.  Bad case is when we compiled with</span>
        <span class="c1"># requires_grad = False, but input requires_grad = True</span>
        <span class="c1"># (vice versa is OK; we compute a gradient and then throw</span>
        <span class="c1"># it away when it hits the input.)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">can_require_grad</span> <span class="o">=</span> <span class="n">flat_requires_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">can_require_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">can_require_grad</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                    <span class="n">aot_config</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would not require grad&quot;</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">debug_compiled_function</span>


<span class="nd">@dynamo_timed</span>
<span class="k">def</span> <span class="nf">create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graphs of the attr:`flat_fn` to generate a</span>
<span class="sd">    joint graph. The joint graph is an Fx graph with Aten ops. Please refer to</span>
<span class="sd">    the tracing mechanism to understand the graph capturing details.</span>

<span class="sd">    The joint graph is then passed through attr:`partition_fn` to isolate the</span>
<span class="sd">    forward and backward portions, which are then respectively compiled via the</span>
<span class="sd">    provided attr:`fw_compiler` and attr:`bw_compiler`.</span>

<span class="sd">    The resulting compiled forward and backward graphs are then wrapped up in a</span>
<span class="sd">    ``torch.autograd.Function`` object.</span>

<span class="sd">    The calling convention here is that the first aot_config.num_params_buffers</span>
<span class="sd">    inputs in flat_args are parameters and buffers, and the rest are inputs.</span>

<span class="sd">    We use this to assume that parameters/buffer&#39;s shapes don&#39;t change.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is the main entry point.</span>
    <span class="c1"># TODO: Chillee argues that dynamo itself should pass in fake tensors to</span>
    <span class="c1"># the list of arguments when compiling; at the moment we do not do this</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">aot_autograd_decompositions</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">log</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">)</span>

    <span class="c1"># NB: don&#39;t bother setting allow_fallback_kernels; this should not actually</span>
    <span class="c1"># be configurable in fake tensor, we should automatically do the right</span>
    <span class="c1"># thing</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_fake_cross_ref</span><span class="p">:</span>
        <span class="c1"># This is a little messy but TorchDynamo directly changes `use_fake_tensor`</span>
        <span class="c1"># so it&#39;s not enough for user to change the config manually</span>
        <span class="c1"># TODO: have TorchDynamo read in `use_fake_tensor` from os environ /</span>
        <span class="c1"># coordinate flags</span>
        <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Check flat_args to see if they&#39;re already fake.  If so, use that fake</span>
    <span class="c1"># mode instead.</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span>
            <span class="n">shape_env</span> <span class="o">=</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span>
            <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="n">cross_ref</span> <span class="o">=</span> <span class="n">CrossRefFakeMode</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_fake_cross_ref</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">enable_python_dispatcher</span><span class="p">()</span> <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_multithreading_enabled</span><span class="p">(</span>
        <span class="kc">False</span>
    <span class="p">),</span> <span class="n">preserve_rng_state</span><span class="p">(),</span> <span class="n">cross_ref</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">python_dispatcher_mode</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">,</span> <span class="n">FakeTensorMode</span><span class="p">):</span>

                <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">x</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
                        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span>
                        <span class="k">return</span> <span class="n">x</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
                        <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">static_weight_shapes</span>
                    <span class="p">):</span>
                        <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="k">return</span> <span class="p">[</span><span class="n">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">flat_args</span>

        <span class="n">fake_flat_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="n">needs_autograd</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fake_flat_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)])</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># crappy version of dispatcher</span>
        <span class="c1"># TODO: Do this properly</span>
        <span class="k">if</span> <span class="n">needs_autograd</span><span class="p">:</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_autograd</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_base</span>

        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_wrapper_dedupe</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="o">=</span><span class="n">compiler_fn</span><span class="p">)</span>
        <span class="c1"># You can put more passes here</span>

        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="c1"># Inspired by autodidax (thanks!)</span>
<span class="k">class</span> <span class="nc">PytreeThunk</span><span class="p">:</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># These are some kinda dumb microoptimizations that save about 3-4 us of overhead.</span>
    <span class="n">is_simple</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>  <span class="c1"># if the output spec is a tuple/list, we won&#39;t bother unflattening it.</span>
    <span class="p">)</span>
    <span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># if the output spec is a LeafSpec</span>

    <span class="k">def</span> <span class="nf">set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">==</span> <span class="n">spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">=</span> <span class="n">spec</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">children_specs</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>


<div class="viewcode-block" id="aot_function"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_function.html#functorch.compile.aot_function">[docs]</a><span class="k">def</span> <span class="nf">aot_function</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">hasher_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`fn` using torch dispatch</span>
<span class="sd">    mechanism, and then compiles the generated forward and backward graphs</span>
<span class="sd">    through :attr:`fw_compiler` and :attr:`bw_compiler`.</span>

<span class="sd">    :func:`aot_function` traces the forward and backward graph ahead of time,</span>
<span class="sd">    and generates a joint forward and backward graph.  :attr:`partition_fn` is</span>
<span class="sd">    then used to separate out forward and backward graphs. The partitioner</span>
<span class="sd">    function can be used to perform optimizations such as recomputation. One can</span>
<span class="sd">    set `decompositions` dictionary to decompose the operators into a sequence</span>
<span class="sd">    of core or simpler operators supported by the backend compilers.</span>

<span class="sd">    :func:`aot_function` uses a compilation cache, based on input tensor</span>
<span class="sd">    properties, to detect when there is a need of recompilation.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): A Python function that takes one ore more arguments. Must</span>
<span class="sd">            return one or more Tensors.</span>
<span class="sd">        fw_compiler (Callable): A Python function that accepts an Fx graph with</span>
<span class="sd">            Aten ops and input args, and returns a Callable that semantically is</span>
<span class="sd">            equivalent to the input Fx graph.</span>
<span class="sd">        bw_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph.  Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">        partition_fn (Callable): A Python function that takes a joint forward</span>
<span class="sd">            and backward graph, and partitions it into separate forward and</span>
<span class="sd">            backward graphs.</span>
<span class="sd">        decompositions (Dict): A dictionary to define the decomposition of</span>
<span class="sd">            larger Aten ops into simpler or core Aten ops.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`fn`, but with forward and backward graph compiled via</span>
<span class="sd">        :attr:`fw_compile` and :attr:`bw_compile`.</span>

<span class="sd">    A simple example usage of :func:`aot_function` is as follows. This example</span>
<span class="sd">    will print the forward and backward graphs of the function ``fn``</span>

<span class="sd">        &gt;&gt;&gt; fn = lambda x : x.sin().cos()</span>
<span class="sd">        &gt;&gt;&gt; def print_compile_fn(fx_module, args):</span>
<span class="sd">        &gt;&gt;&gt;     print(fx_module)</span>
<span class="sd">        &gt;&gt;&gt;     return fx_module</span>
<span class="sd">        &gt;&gt;&gt; aot_fn = aot_function(fn, print_compile_fn)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(4, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; aot_fn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;static_argnums has been deprecated - manually wrap your function or use torchdynamo.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span>
    <span class="p">)</span>
    <span class="n">cached_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">returned_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">cached_res</span>
        <span class="c1"># Now flatten the tensor args</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="c1"># Compile the function and save it in the cache</span>
        <span class="k">if</span> <span class="n">cached_res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Save the args_spec for flat_tensor_args to unflatten while tracing</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">tensor_args_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
            <span class="n">out_spec</span> <span class="o">=</span> <span class="n">PytreeThunk</span><span class="p">()</span>

            <span class="k">def</span> <span class="nf">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
                <span class="c1"># The input are flattened tensor args. Prepare the args in the</span>
                <span class="c1"># order that original function expects. Add static args as well.</span>
                <span class="c1"># They will appear as tensor constants in the traced graph.</span>
                <span class="k">nonlocal</span> <span class="n">out_spec</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">tensor_args_spec</span><span class="p">)</span>
                <span class="n">tree_out</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="n">flat_out</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tree_out</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flat_out</span><span class="p">:</span>
                    <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">KNOWN_TYPES</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
                            <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="k">break</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_known_type</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2"> in output, which is not a known type. &quot;</span>
                            <span class="s2">&quot;If this type holds tensors, you need to register a pytree for it. &quot;</span>
                            <span class="s2">&quot;See https://github.com/pytorch/functorch/issues/475 for a brief &quot;</span>
                            <span class="s2">&quot;explanation why. If you don&#39;t need to register a pytree, please &quot;</span>
                            <span class="s2">&quot;leave a comment explaining your use case and we&#39;ll make this more &quot;</span>
                            <span class="s2">&quot;ergonomic to deal with&quot;</span>
                        <span class="p">)</span>
                <span class="n">out_spec</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">flat_out</span>

            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="n">flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cached_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">out_spec</span><span class="p">)</span>

        <span class="n">cached_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">cached_res</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cached_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returned_function</span></div>


<div class="viewcode-block" id="aot_module"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_module.html#functorch.compile.aot_module">[docs]</a><span class="k">def</span> <span class="nf">aot_module</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`mod` using torch dispatch</span>
<span class="sd">    tracing mechanism. It is wrapper function, that underneath uses</span>
<span class="sd">    :func:`aot_function` to perform tracing and compilation.</span>

<span class="sd">    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs</span>
<span class="sd">    to a new callable which is then compiled through :func:`aot_function`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (Callable): A ``nn.Module`` module.</span>
<span class="sd">        args : args to be passed to :func:`aot_function`</span>
<span class="sd">        kwargs : kwargs to be passed to :func:`aot_function`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``nn.Module`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`mod`, but with forward and backward graph compiled.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="n">named_params</span><span class="p">,</span> <span class="n">named_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">named_params</span><span class="p">,</span> <span class="o">**</span><span class="n">named_buffers</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_and_buffers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">named_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_params</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>
    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span> <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="k">class</span> <span class="nc">AOTModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orig_module</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="n">named_params</span><span class="p">,</span>
                <span class="n">named_buffers</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">AOTModule</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">aot_module_simplified</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hasher_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">static_argnums</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the simplified or low overhead version of aot_module. For frontends</span>
<span class="sd">    like TorchDynamo, the input functions/modules to AOT are static and have</span>
<span class="sd">    unpacked inputs/outputs. This gives us an opportunity to remove the</span>
<span class="sd">        (1) pytree overhead to parse inputs/outputs,</span>
<span class="sd">        (2) AOT Autograd cache,</span>
<span class="sd">        (3) Reading of params/buffers in every forward call</span>

<span class="sd">    :func:`aot_module_simplified` removes these overheads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#########################################################</span>

    <span class="c1"># Redudant with dynamo, but worth having in case this gets invoked elsewhere.</span>

    <span class="c1"># Note [Fake Modules and AOTAutograd]</span>
    <span class="c1">#</span>
    <span class="c1"># A simple heuristic for when to use fake versus real tensors is that fake tensors are for compile time</span>
    <span class="c1"># (when we don&#39;t want to actually run the compute, but we do want to know about metadata),</span>
    <span class="c1"># and real tensors are for runtime (when we actually want to do the compute.) However, in AOTAutograd,</span>
    <span class="c1"># modules are the exception: we always pass AOTAutograd modules with real tensors.</span>
    <span class="c1"># This is because AOTAutograd will produce a compiled function which needs to directly access any</span>
    <span class="c1"># parameters the compiled function may need, but these parameters will NOT be passed in by the caller (aka Dynamo).</span>
    <span class="c1"># So at compile time, the compiled function we produce must close over any parameters, and those parameters must be</span>
    <span class="c1"># real parameters, and we cannot do this unless at compile time we get a module with real tensors.</span>

    <span class="c1"># Even if Dynamo did pass all parameters explicitly at runtime, which would eliminate the need to close over</span>
    <span class="c1"># the parameters, it would still be profitable to pass real tensor parameters to the compiler at compile time,</span>
    <span class="c1"># because some compilation strategies like CUDA graphs want to burn in the pointer addresses where the parameter data live,</span>
    <span class="c1"># and of course we can&#39;t do that unless we give the backend a real tensor.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="n">params_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">stateless</span><span class="o">.</span><span class="n">_reparametrize_module</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">args</span><span class="p">[:</span><span class="n">params_len</span><span class="p">],</span> <span class="n">params_spec</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">(),</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                        <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;Anomaly Detection has been enabled.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">(</span><span class="n">check_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">Interpreter</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Graph output must be a tuple(). This is so that we can avoid &quot;</span>
                <span class="s2">&quot;pytree processing of the ouputs. Please change the module to &quot;</span>
                <span class="s2">&quot;have tuple outputs or use aot_module instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">assert</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span>
        <span class="n">full_args</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># TODO: There is something deeply wrong here; compiled_fn running with</span>
    <span class="c1"># the boxed calling convention, but aot_module_simplified somehow</span>
    <span class="c1"># historically returned a function that was not the boxed calling</span>
    <span class="c1"># convention.  This should get fixed...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">runtime_args</span><span class="p">):</span>
        <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span>

    <span class="c1"># Just for convenience</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_buffers</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span>

    <span class="k">return</span> <span class="n">forward</span>


<span class="n">compiled_function</span> <span class="o">=</span> <span class="n">aot_function</span>
<span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aot_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>