


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._functorch.compilers &mdash; functorch 2.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>2.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torch._functorch.compilers</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._functorch.compilers</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">SymInt</span>
<span class="kn">import</span> <span class="nn">torch.fx</span> <span class="k">as</span> <span class="nn">fx</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch._decomp</span> <span class="kn">import</span> <span class="n">get_decompositions</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">bind_symbols</span>

<span class="kn">from</span> <span class="nn">.aot_autograd</span> <span class="kn">import</span> <span class="n">aot_function</span><span class="p">,</span> <span class="n">aot_module</span><span class="p">,</span> <span class="n">make_boxed_compiler</span>
<span class="kn">from</span> <span class="nn">.compile_utils</span> <span class="kn">import</span> <span class="n">strip_overloads</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">draw_graph</span><span class="p">,</span>
    <span class="n">min_cut_rematerialization_partition</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>


<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="c1"># These canonicalizations are needed here (and not decompositions), as the ops</span>
<span class="c1"># we&#39;re trying to canonicalize to CompositeImplicitAutograd.</span>
<span class="k">def</span> <span class="nf">_canonicalize</span><span class="p">(</span><span class="n">fx_g</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span><span class="p">:</span>
            <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">to</span>
    <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fx_g</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">_disable_jit_autocast</span><span class="p">():</span>
    <span class="n">old_jit_autocast_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_autocast_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_autocast_mode</span><span class="p">(</span><span class="n">old_jit_autocast_flag</span><span class="p">)</span>


<div class="viewcode-block" id="ts_compile"><a class="viewcode-back" href="../../../generated/functorch.compile.ts_compile.html#functorch.compile.ts_compile">[docs]</a><span class="nd">@make_boxed_compiler</span>
<span class="k">def</span> <span class="nf">ts_compile</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compiles the :attr:`fx_g` with Torchscript compiler.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fx_g(fx.GraphModule): The input Fx graph module to be compiled.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Torch scripted model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">_disable_jit_autocast</span><span class="p">():</span>
        <span class="n">strip_overloads</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="s2">&quot;dtype&quot;</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">kwargs</span>
            <span class="p">):</span>
                <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">to</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="n">new_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
                    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">type</span>
                <span class="n">new_kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="n">node</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">new_kwargs</span>

        <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">lint</span><span class="p">()</span>

        <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_remove_mutation</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>

        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">optimize_for_inference</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">inps</span><span class="p">):</span>
            <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">inps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span></div>


<span class="k">def</span> <span class="nf">_draw_graph_compile</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">clear_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">code</span><span class="p">)</span>
    <span class="n">draw_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">clear_meta</span><span class="o">=</span><span class="n">clear_meta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span>


<span class="k">def</span> <span class="nf">draw_graph_compile</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">make_boxed_compiler</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">_draw_graph_compile</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="p">)</span>


<div class="viewcode-block" id="nop"><a class="viewcode-back" href="../../../generated/functorch.compile.nop.html#functorch.compile.nop">[docs]</a><span class="nd">@make_boxed_compiler</span>
<span class="k">def</span> <span class="nf">nop</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the :attr:`fx_g` Fx graph module as it is. This is a no-op compiler</span>
<span class="sd">    and can be used to check accuracy.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fx_g</span></div>

<span class="k">class</span> <span class="nc">DebugInterpreter</span><span class="p">(</span><span class="n">fx</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">symbol_mapping</span> <span class="o">=</span> <span class="n">bind_symbols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">sympy</span>

        <span class="k">def</span> <span class="nf">subst_symint</span><span class="p">(</span><span class="n">ni</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">ni</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">ni</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="o">.</span><span class="n">xreplace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol_mapping</span><span class="p">))</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">free_symbols</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">r</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">subst_symint_tuple</span><span class="p">(</span><span class="n">nis</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">subst_symint</span><span class="p">(</span><span class="n">ni</span><span class="p">)</span> <span class="k">for</span> <span class="n">ni</span> <span class="ow">in</span> <span class="n">nis</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">check_significant_strides</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">subst_symint</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">subst_symint</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="ow">and</span> <span class="n">subst_symint</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">return</span> <span class="kc">False</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="n">nv</span><span class="p">,</span> <span class="n">rv</span><span class="p">,</span> <span class="n">desc</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="n">desc</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">nv</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">rv</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">desc</span><span class="p">()</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">nv</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">rv</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">assert</span> <span class="n">subst_symint_tuple</span><span class="p">(</span><span class="n">nv</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">==</span> <span class="n">rv</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> \
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">desc</span><span class="p">()</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">nv</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2"> aka </span><span class="si">{</span><span class="n">subst_symint_tuple</span><span class="p">(</span><span class="n">nv</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">rv</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">same_strides</span> <span class="o">=</span> <span class="n">check_significant_strides</span><span class="p">(</span><span class="n">nv</span><span class="p">,</span> <span class="n">rv</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">same_strides</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">desc</span><span class="p">()</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">nv</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span><span class="si">}</span><span class="s2"> aka </span><span class="si">{</span><span class="n">subst_symint_tuple</span><span class="p">(</span><span class="n">nv</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">rv</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">r</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">run_node</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;val&#39;</span> <span class="ow">in</span> <span class="n">n</span><span class="o">.</span><span class="n">meta</span><span class="p">:</span>
            <span class="n">n_vals</span><span class="p">,</span> <span class="n">n_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">])</span>
            <span class="n">r_vals</span><span class="p">,</span> <span class="n">r_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="c1"># TODO: There is some sort of problem where we record that an</span>
            <span class="c1"># operator returned a tuple/list, and then later it turns out the</span>
            <span class="c1"># real version of the operator returned a list/tuple. Need to</span>
            <span class="c1"># figure out what&#39;s actually going on here, the error itself is</span>
            <span class="c1"># harmless enough as we only getitem out the outputs.</span>
            <span class="c1"># assert n_spec == r_spec, f&quot;{n_spec} != {r_spec}&quot;</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_vals</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">r_vals</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">n_vals</span><span class="p">)</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">r_vals</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">nv</span><span class="p">,</span> <span class="n">rv</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_vals</span><span class="p">)),</span> <span class="n">n_vals</span><span class="p">,</span> <span class="n">r_vals</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rv</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="n">check</span><span class="p">(</span><span class="n">nv</span><span class="p">,</span> <span class="n">rv</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;output </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> where </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol_mapping</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">r</span>


<span class="nd">@make_boxed_compiler</span>
<span class="k">def</span> <span class="nf">debug_nop</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a (slow) interpreter over the FX graph module that also checks</span>
<span class="sd">    various debugging properties (e.g., that tracing strides matched real</span>
<span class="sd">    strides.)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">DebugInterpreter</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span><span class="o">.</span><span class="n">run</span>

<span class="nd">@make_boxed_compiler</span>
<span class="k">def</span> <span class="nf">simple_ts_compile</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
    <span class="n">strip_overloads</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">nnc_jit</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">aot_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">simple_ts_compile</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">)</span>


<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>
<span class="n">default_decompositions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">gelu_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">leaky_relu_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">sigmoid_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">threshold_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">hardtanh_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">hardsigmoid_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">hardswish_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">tanh_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">silu_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">elu_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">cudnn_batch_norm</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">cudnn_batch_norm_backward</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">masked_fill</span><span class="o">.</span><span class="n">Scalar</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">masked_fill</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">hardswish</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">hardsigmoid</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">conj_physical</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">is_same_size</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">default_decompositions</span> <span class="o">=</span> <span class="n">get_decompositions</span><span class="p">(</span><span class="n">default_decompositions</span><span class="p">)</span>


<span class="nd">@make_boxed_compiler</span>
<span class="k">def</span> <span class="nf">print_compile</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">code</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span>


<div class="viewcode-block" id="memory_efficient_fusion"><a class="viewcode-back" href="../../../generated/functorch.compile.memory_efficient_fusion.html#functorch.compile.memory_efficient_fusion">[docs]</a><span class="k">def</span> <span class="nf">memory_efficient_fusion</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper function over :func:`aot_function` and :func:`aot_module` to perform</span>
<span class="sd">    memory efficient fusion. It uses the</span>
<span class="sd">    :func:`min_cut_rematerialization_partition` partitioner to perform efficient</span>
<span class="sd">    recomputation. It uses NVFuser to compile the generated forward and backward</span>
<span class="sd">    graphs.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Union[Callable, nn.Module]): A Python function or a ``nn.Module``</span>
<span class="sd">            that takes one ore more arguments. Must return one or more Tensors.</span>
<span class="sd">        static_argnums (Optional[Tuple[Int]]): An option tuple of ints to mark</span>
<span class="sd">            the arguments of the function as static.</span>
<span class="sd">        **kwargs: Any other overrides you want to make to the settings</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable``  or ``nn.Module`` that retains the eager behavior</span>
<span class="sd">        of the original :attr:`fn`, but whose forward and backward graphs have</span>
<span class="sd">        gone through recomputation optimizations, and the graphs have been</span>
<span class="sd">        compiled with nvfuser.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;fw_compiler&quot;</span><span class="p">:</span> <span class="n">ts_compile</span><span class="p">,</span>
        <span class="s2">&quot;bw_compiler&quot;</span><span class="p">:</span> <span class="n">ts_compile</span><span class="p">,</span>
        <span class="s2">&quot;partition_fn&quot;</span><span class="p">:</span> <span class="n">min_cut_rematerialization_partition</span><span class="p">,</span>
        <span class="s2">&quot;decompositions&quot;</span><span class="p">:</span> <span class="n">default_decompositions</span><span class="p">,</span>
        <span class="s2">&quot;static_argnums&quot;</span><span class="p">:</span> <span class="n">static_argnums</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">aot_module</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">aot_function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">debug_compile</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">):</span>
    <span class="n">fx_g</span><span class="o">.</span><span class="n">to_folder</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">##############################################################</span>
<span class="s2"># To minimize FX graph, copy and paste the below and run it  #</span>
<span class="s2">##############################################################</span>

<span class="s2">import torch</span>
<span class="s2">import torch.fx as fx</span>
<span class="s2">from functorch.compile import minifier, check_nvfuser_subprocess, check_nvfuser_correctness_subprocess</span>

<span class="s2">inps = </span><span class="si">{</span><span class="p">[(</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inps</span><span class="p">]</span><span class="si">}</span><span class="s2"></span>
<span class="s2">inps = [torch.ones(shape, dtype=dtype, device=&#39;cuda&#39;) for (shape, dtype) in inps]</span>
<span class="s2">from foo import FxModule</span>
<span class="s2">mod = FxModule().cuda()</span>

<span class="s2">with torch.jit.fuser(&quot;fuser2&quot;):</span>
<span class="s2">  # check_nvfuser_subprocess can be replaced with check_nvfuser_correctness_subprocess</span>
<span class="s2">  minifier(fx.symbolic_trace(mod), inps, check_nvfuser_subprocess)</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="p">)</span>
    <span class="kn">from</span> <span class="nn">foo</span> <span class="kn">import</span> <span class="n">FxModule</span>

    <span class="n">FxModule</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()(</span><span class="o">*</span><span class="n">inps</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ts_compile</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span>


<span class="n">graph_index</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">get_inputs</span><span class="p">(</span><span class="n">input_data_path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a random input for the given inputs meta generated from _save_fx_default.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">input_data_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">inputs_meta</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">meta</span> <span class="ow">in</span> <span class="n">inputs_meta</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">type</span> <span class="o">=</span> <span class="n">meta</span>
                <span class="nb">input</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">type</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">meta</span>
                <span class="k">if</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">{</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                    <span class="nb">int</span><span class="p">,</span>
                    <span class="nb">float</span><span class="p">,</span>
                <span class="p">}:</span>
                    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>


<span class="k">def</span> <span class="nf">_save_fx_default</span><span class="p">(</span><span class="n">current_name</span><span class="p">,</span> <span class="n">folder_name</span><span class="p">,</span> <span class="n">dump_example_input</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The forward, backward, and joint computation graph will be stored in</span>
<span class="sd">    {folder_name}/{current_name}/{current_name}_forward_{graph_index},</span>
<span class="sd">    {folder_name}/{current_name}/{current_name}_backward_{graph_index}, and</span>
<span class="sd">    {folder_name}/{current_name}/{current_name}_joint_{graph_index} respectively.</span>
<span class="sd">    The input shape of the graphs will be stored in the .input files.</span>
<span class="sd">    These files can be loaded with pickle,</span>
<span class="sd">    and is a list of format (type, shape, stride, dtype, device).</span>
<span class="sd">    In the case of type = int or float, it is just (type,).</span>
<span class="sd">    For joint graph input, it is a nested list [[],[]]</span>
<span class="sd">    where the two inner lists have the same format.</span>
<span class="sd">    If dump_example_input is True, example_inputs will be stored in .pt file.</span>
<span class="sd">    Since each function might produce multiple graphs,</span>
<span class="sd">    the graph_index is used to distinguish difference graphs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">functorch.compile</span> <span class="kn">import</span> <span class="n">aot_module_simplified</span>

    <span class="k">def</span> <span class="nf">get_input_meta</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">input_meta</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>  <span class="c1"># joint input</span>
            <span class="n">input_meta</span> <span class="o">+=</span> <span class="n">get_input_meta</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">input_meta</span> <span class="o">+=</span> <span class="n">get_input_meta</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">input_meta</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">==</span> <span class="nb">float</span><span class="p">:</span>
                <span class="n">input_meta</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">),))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_meta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">),</span> <span class="n">arg</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">arg</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">input_meta</span>

    <span class="k">def</span> <span class="nf">graph_saver_helper</span><span class="p">(</span><span class="n">gm_to_save</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">type_name</span><span class="p">):</span>
        <span class="k">global</span> <span class="n">graph_index</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gm_to_save</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;No nodes in graph </span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">type_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_index</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span>

        <span class="n">gm</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">gm_to_save</span><span class="p">)</span>
        <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">set_codegen</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">CodeGen</span><span class="p">())</span>  <span class="c1"># remove codegen</span>
        <span class="n">gm</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="n">input_meta</span> <span class="o">=</span> <span class="n">get_input_meta</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

        <span class="n">isExist</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">folder_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">isExist</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">folder_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">gm</span><span class="o">.</span><span class="n">to_folder</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">folder_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">type_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_index</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
            <span class="n">input_meta</span><span class="p">,</span>
            <span class="nb">open</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">folder_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">type_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_index</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">type_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_index</span><span class="si">}</span><span class="s2">.input&quot;</span><span class="p">,</span>  <span class="c1"># noqa: B950</span>
                <span class="s2">&quot;wb&quot;</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># noqa: E501</span>
        <span class="k">if</span> <span class="n">dump_example_input</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                <span class="n">args</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">folder_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">type_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_index</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">current_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">type_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_index</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">,</span>  <span class="c1"># noqa: B950</span>
            <span class="p">)</span>  <span class="c1"># noqa: E501</span>

    <span class="k">def</span> <span class="nf">graph_saver_forward</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">fw_args</span><span class="p">):</span>
        <span class="n">graph_saver_helper</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">fw_args</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gm</span>

    <span class="k">def</span> <span class="nf">graph_saver_backward</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">bw_args</span><span class="p">):</span>
        <span class="n">graph_saver_helper</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">bw_args</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">)</span>
        <span class="k">global</span> <span class="n">graph_index</span>
        <span class="n">graph_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">gm</span>

    <span class="k">def</span> <span class="nf">graph_saver_joint</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">joint_args</span><span class="p">):</span>
        <span class="n">graph_saver_helper</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">joint_args</span><span class="p">,</span> <span class="s2">&quot;joint&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">default_partition</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">joint_args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">aot_module_simplified</span><span class="p">(</span>
        <span class="n">gm</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="p">,</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">graph_saver_forward</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">graph_saver_backward</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">graph_saver_joint</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">default_decompositions</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># WARNING: This isn&#39;t tested anywhere!!</span>
<span class="k">def</span> <span class="nf">graph_dumper_aot</span><span class="p">(</span><span class="n">current_name</span><span class="p">,</span> <span class="n">folder_name</span><span class="p">,</span> <span class="n">dump_example_input</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dump the forward, backward, and joint computation graph.</span>
<span class="sd">    Example Usage:</span>
<span class="sd">    save_fx_func = graph_dumper_aot(current_name, folder_name, dump_example_input = False)</span>
<span class="sd">    optimize_ctx = torchdynamo.optimize(</span>
<span class="sd">        save_fx_func</span>
<span class="sd">    )</span>
<span class="sd">    with torch.enable_grad():</span>
<span class="sd">        with optimize_ctx:</span>
<span class="sd">            result = forward_and_backward_pass(model, example_inputs)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">graph_index</span>
    <span class="n">graph_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">_save_fx_default</span><span class="p">,</span> <span class="n">current_name</span><span class="p">,</span> <span class="n">folder_name</span><span class="p">,</span> <span class="n">dump_example_input</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>