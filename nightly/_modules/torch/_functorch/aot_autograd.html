


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._functorch.aot_autograd &mdash; functorch nightly documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                PyTorch Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch">
                  <span class="dropdown-title">ExecuTorch</span>
                </a>
              </div>
            </div>  
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>nightly (2.2.0a0+git74e6c87) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torch._functorch.aot_autograd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._functorch.aot_autograd</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">NewType</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span>

<span class="kn">from</span> <span class="nn">functorch</span> <span class="kn">import</span> <span class="n">make_fx</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx.traceback</span> <span class="k">as</span> <span class="nn">fx_traceback</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.meta_utils</span> <span class="kn">import</span> <span class="n">safe_is_leaf</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">torch._dynamo</span> <span class="kn">import</span> <span class="n">compiled_autograd</span>
<span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">dynamo_timed</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">,</span> <span class="n">preserve_rng_state</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">detect_fake_mode</span><span class="p">,</span> <span class="n">tracing</span>
<span class="kn">from</span> <span class="nn">torch._prims_common</span> <span class="kn">import</span> <span class="n">CUDARngStateHelper</span>
<span class="kn">from</span> <span class="nn">torch._logging</span> <span class="kn">import</span> <span class="n">getArtifactLogger</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">FakeTensor</span><span class="p">,</span> <span class="n">FakeTensorMode</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.fake_tensor</span> <span class="kn">import</span> <span class="n">is_fake</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.functional_tensor</span> <span class="kn">import</span> <span class="n">FunctionalTensor</span><span class="p">,</span> <span class="n">FunctionalTensorMode</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">immutable_collections</span><span class="p">,</span> <span class="n">Interpreter</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="n">is_sym_node</span><span class="p">,</span> <span class="n">py_sym_types</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span><span class="p">,</span> <span class="n">is_concrete_int</span><span class="p">,</span> <span class="n">fx_placeholder_vals</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing.reductions</span> <span class="kn">import</span> <span class="n">StorageWeakRef</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">stateless</span>
<span class="kn">from</span> <span class="nn">torch.utils._python_dispatch</span> <span class="kn">import</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">,</span> <span class="n">transform_subclass</span>
<span class="kn">from</span> <span class="nn">torch._decomp.decompositions_for_rng</span> <span class="kn">import</span> <span class="n">PhiloxStateTracker</span><span class="p">,</span> <span class="n">rng_decompositions</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="n">default_partition</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">TracingContext</span><span class="p">,</span> <span class="n">DuplicateInputs</span><span class="p">,</span> <span class="n">Source</span>


<span class="n">original_zip</span> <span class="o">=</span> <span class="nb">zip</span>

<span class="k">def</span> <span class="nf">strict_zip</span><span class="p">(</span><span class="o">*</span><span class="n">iterables</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">original_zip</span><span class="p">(</span><span class="o">*</span><span class="n">iterables</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">shortest_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">it</span><span class="p">)</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">iterables</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iterable</span> <span class="ow">in</span> <span class="n">iterables</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span> <span class="o">!=</span> <span class="n">shortest_length</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The iterables have different lengths and strict mode is enabled.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">original_zip</span><span class="p">(</span><span class="o">*</span><span class="n">iterables</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nb">zip</span> <span class="o">=</span> <span class="n">strict_zip</span>

<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">aot_joint_log</span> <span class="o">=</span> <span class="n">getArtifactLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;aot_joint_graph&quot;</span><span class="p">)</span>
<span class="n">aot_graphs_log</span> <span class="o">=</span> <span class="n">getArtifactLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;aot_graphs&quot;</span><span class="p">)</span>

<span class="n">MutationType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;MutationType&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;metadata_only&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;data_and_metadata&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">OutputType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;OutputType&quot;</span><span class="p">,</span> <span class="p">(</span>
        <span class="c1"># output is not an alias</span>
        <span class="s2">&quot;non_alias&quot;</span><span class="p">,</span>
        <span class="c1"># output aliases an input</span>
        <span class="s2">&quot;alias_of_input&quot;</span><span class="p">,</span>
        <span class="c1"># output **is** an input tensor</span>
        <span class="s2">&quot;is_input&quot;</span><span class="p">,</span>
        <span class="c1"># output has a ._base tensor, which is a graph intermediate.</span>
        <span class="c1"># We need to return its ._base as a graph output,</span>
        <span class="c1"># so its requires_grad info is populated correctly.</span>
        <span class="c1"># Instructs the runtime code to regenerate the current output</span>
        <span class="c1"># from a base tensor, graph_intermediates[base_idx]</span>
        <span class="s2">&quot;alias_of_intermediate_save_as_output&quot;</span><span class="p">,</span>
        <span class="c1"># Same as above; but we don&#39;t need to explicitly add its ._base</span>
        <span class="c1"># as a graph output, because it already **is** a graph output.</span>
        <span class="s2">&quot;alias_of_intermediate&quot;</span><span class="p">,</span>
        <span class="c1"># Same as above; but the output&#39;s ._base is **already** a user output.</span>
        <span class="c1"># Instructs the runtime code to regenerate the current output from</span>
        <span class="c1"># a base tensor, user_outputs[base_idx]</span>
        <span class="s2">&quot;alias_of_intermediate_base_is_user_output&quot;</span><span class="p">,</span>
        <span class="c1"># See Note [Intermediate Bases Optimization]</span>
        <span class="s2">&quot;unsafe_view_alias&quot;</span><span class="p">,</span>
        <span class="c1"># output is an alias, but has a custom autograd.Function backward.</span>
        <span class="c1"># In this case, we don&#39;t want to do view-replay, since we won&#39;t be able to replay the custom function.</span>
        <span class="c1"># Instead, we&#39;ll treat this output &quot;normally&quot;, and trace its backward into the graph.</span>
        <span class="s2">&quot;custom_function_view&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">(</span>
        <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">partial_asdict</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">is_dataclass</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">fields</span><span class="p">(</span><span class="n">obj</span><span class="p">)}</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="vm">__class__</span><span class="p">([</span><span class="n">partial_asdict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">partial_asdict</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">obj</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="c1"># This global counter increments every time we compile a graph with</span>
<span class="c1"># AOTAutograd.  You can use this to correlate runtime error messages</span>
<span class="c1"># with compile time (e.g., if you get an error at runtime saying</span>
<span class="c1"># compiled graph 3 failed, you can set a breakpoint at compile time</span>
<span class="c1"># for this graph number to investigate further at compile time.)</span>
<span class="c1">#</span>
<span class="c1"># NB: this is different from get_aot_compilation_context, which tracks</span>
<span class="c1"># each underlying graph that is compiled.  In contrast, AOT_COUNTER</span>
<span class="c1"># corresponds to top-level invocations of aot_module/aot_function;</span>
<span class="c1"># one counter is allocated per entire compiled block (but this block</span>
<span class="c1"># may involve compiling multiple subgraphs; e.g., for forwards/backwards)</span>
<span class="n">AOT_COUNTER</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="n">KNOWN_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
    <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">py_sym_types</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Set up hooks so that during backward the fx&#39;s stack_trace is properly set</span>
<span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">def</span> <span class="nf">setup_stacktrace_preservation_hooks</span><span class="p">(</span><span class="n">roots</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">q</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">fn</span><span class="p">,</span> <span class="n">_idx</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">next_functions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">or</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">node</span>

    <span class="k">def</span> <span class="nf">get_callback</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">callback</span><span class="p">():</span>
            <span class="k">global</span> <span class="n">callback_set</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">)</span>
            <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">callback</span>

    <span class="k">def</span> <span class="nf">get_prehook</span><span class="p">(</span><span class="n">stack_</span><span class="p">,</span> <span class="n">seq_nr</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">prehook</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
            <span class="k">global</span> <span class="n">callback_set</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">callback_set</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">variable</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span>
                    <span class="n">get_callback</span><span class="p">(</span><span class="n">fx_traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">stack_</span><span class="p">)</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_grad_fn_seq_nr</span><span class="p">(</span><span class="n">seq_nr</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prehook</span>

    <span class="k">def</span> <span class="nf">get_posthook</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">,</span> <span class="n">seq_nr</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">posthook</span><span class="p">(</span><span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">)</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">reset_grad_fn_seq_nr</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">posthook</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="n">forward_node_stack</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;traceback_&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_prehook</span><span class="p">(</span><span class="n">get_prehook</span><span class="p">(</span><span class="n">forward_node_stack</span><span class="p">,</span>
                              <span class="n">node</span><span class="o">.</span><span class="n">_sequence_nr</span><span class="p">()))</span>

        <span class="n">special_stack</span> <span class="o">=</span> <span class="n">forward_node_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">special_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;Gradient addition node due to multiple use of tensor around:&quot;</span>
        <span class="p">)</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">get_posthook</span><span class="p">(</span><span class="n">special_stack</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">_sequence_nr</span><span class="p">()))</span>


<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd contains a pretty non-trivial amount of logic to handle edge cases around aliasing and mutation</span>
<span class="c1"># that are external to the graph (they show up as side effects in some way when you run the graph).</span>
<span class="c1">#</span>
<span class="c1"># Take a look at `test_aotdispatch.py TestAOTAutograd.test_input_mutation*` tests for some examples functions</span>
<span class="c1"># and what they&#39;re compiled graphs looks like.</span>
<span class="c1"># Below is a very long comment detailing several edge cases, and showing how AOT Autograd handles them.</span>
<span class="c1">#</span>
<span class="c1"># Note [AOT Autograd: input data mutations]</span>
<span class="c1">#</span>
<span class="c1"># If we compile a function that mutates inputs, then those input mutations are real side effects</span>
<span class="c1"># that a user expects to see after running the compiled graph.</span>
<span class="c1"># However, the graph that we want to send to a backend needs to be *entirely* functional.</span>
<span class="c1"># The way we reconcile this difference is that we remove the mutations completely from the graph that we compile</span>
<span class="c1"># but we update the graph to return (updated_inputs, user_outputs).</span>
<span class="c1"># In the epilogue that runs after the compiled graph is executed, we copy the updated inputs back to the originals.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># After AOT Autograd compiles, we end up with a:</span>
<span class="c1"># (a) compiled graph</span>
<span class="c1"># (b) autograd.Function.forward() method, that executes the compiled graph</span>
<span class="c1"># (c) wrapper function, that calls the autograd.Function.forward() and performs the epilogue</span>
<span class="c1">#</span>
<span class="c1"># The output of (a, b, c) are all written below.</span>
<span class="c1">#</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated gets a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_x_updated, grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is that updated inputs (due to data mutations) *do* participate</span>
<span class="c1"># in the compiled backward graph! Since the compiled forward graph gets N extra outputs</span>
<span class="c1"># (due to updated inputs showing up as graph outputs),</span>
<span class="c1"># The compiled backward gets an additional N inputs.</span>
<span class="c1"># That way, during the x.copy_(x_updated) bit in the epilogue, gradients will flow from the updated input</span>
<span class="c1"># back to the original input.</span>


<span class="c1"># Note [AOT Autograd: input metadata mutations]</span>
<span class="c1">#</span>
<span class="c1"># For the same reason as input mutations, we also don&#39;t put input metadata mutations in the graph.</span>
<span class="c1"># Instead, we return the updated version of the input (a view), and mutate the input&#39;s metadata outside of the graph</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.t_()</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.t()</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated does *not* get a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.as_strided_(x_updated)</span>
<span class="c1">#     return out</span>


<span class="c1"># Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd needs special handling for outputs that alias graph inputs or intermediates!</span>
<span class="c1"># Why?</span>
<span class="c1"># (1) autograd.Function.forward() has a limitation, where views that returned in the forward cannot later be mutated.</span>
<span class="c1"># (2) views don&#39;t need to be compiled in the graph anyway - it&#39;s cheap to generate them outside of the compiled graph,</span>
<span class="c1">#     in an epilogue.</span>
<span class="c1"># For outputs that alias inputs, we do the following:</span>
<span class="c1"># (a) *still* return the aliased output as a graph output</span>
<span class="c1"># (b) In the AOT Autograd wrapper/epilogue, we don&#39;t return that aliased output. Instead, we use it to regenerate the output.</span>
<span class="c1">#</span>
<span class="c1"># For outputs that alias *intermediates*, we do the following:</span>
<span class="c1"># (a) Return the output in the compiled forward, **and** return it&#39;s ._base (a graph intermediates) as an output in the forward</span>
<span class="c1"># (b) Use (output, graph_intermediate) to regenerate the alias, and return that to the user (instead of the compiled fw output).</span>
<span class="c1"># You might wonder why we return the aliased output directly in the graph (and making the graph compute it),</span>
<span class="c1"># only to not return it and instead generate a fresh alias off of the intermediate,</span>
<span class="c1"># instead of (say) just storing metadata about the size/stride of the output somewhere to generate the alias. There are two reasons:</span>
<span class="c1"># (1) Getting the actual alias tensor allows us to use view-replay to generate the alias, instead of an as_strided() call</span>
<span class="c1"># (2) Inductor (and other backends) are free to change the memory format of graph outputs, if it results in better performance.</span>
<span class="c1">#     This can result in problems if a user later tries to .view() that output expecting it to have one set of strides,</span>
<span class="c1">#     when it has a different set of strides.</span>
<span class="c1">#     By including the view op directly in the graph, inductor takes that into account when deciding what memory format</span>
<span class="c1">#     the graph intermediate should be.</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is how our traced backward() graph handles aliases.</span>
<span class="c1"># (this applies to outputs aliasing inputs, outputs aliasing intermediates,</span>
<span class="c1">#  *and* updated inputs returned in the compiled forward due to metadata-only mutations).</span>
<span class="c1"># Any outputs that alias (either inputs or intermediates) do NOT participate in the compiled backward graph</span>
<span class="c1"># It would be wasteful to include them in the compiled backward(), because we regenerate them eagerly</span>
<span class="c1"># at the end of the forward.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     return out1, out2</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     # the compiled graph also returns the intermediate</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># # intermediate gets a gradient in the compiled backward.</span>
<span class="c1"># # both output aliases (out1 and out2) do not.</span>
<span class="c1"># def compiled_backward_graph(grad_intermediate):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     out1, out2, intermediate = compiled_forward_graph(x)</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     out1, out2, intermediate = autograd.Function.apply(x)</span>
<span class="c1">#     # regenerate out1 from the input</span>
<span class="c1">#     out1_regenerated = out1._view_func(x)</span>
<span class="c1">#     # regenerate out1 from the intermediate</span>
<span class="c1">#     out2_regenerated = out2._view_func(intermediate)</span>
<span class="c1">#     return out1_regenerated, out2_regenerated</span>


<span class="c1"># Note [AOT Autograd: mutations to inputs that alias other inputs]</span>
<span class="c1">#</span>
<span class="c1"># Another edge case that is (only partially) handled today is when an input is mutated, but itself aliases another input.</span>
<span class="c1"># AOT Autograd needs to **ensure** that functionalization knows that the two inputs are aliased to each other.</span>
<span class="c1"># That way, when the aliased input is accessed later in the graph, functionalization knows to &quot;update&quot; the alias</span>
<span class="c1"># given the mutation that occurred.</span>
<span class="c1">#</span>
<span class="c1"># This is handled by updating the calling convention: we create a &quot;synthetic base&quot; that becomes a new input</span>
<span class="c1"># in the compiled function, and we regenerate the original (aliased) inputs directly off of the base</span>
<span class="c1"># inside of the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># This logic is fully encapsulated in aot_wrapper_synthetic_base()</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x, x_view):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x * x_view</span>
<span class="c1">#     return out</span>
<span class="c1"># f(x, x.view(-1))</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(base)</span>
<span class="c1">#     x = generate_x(base)</span>
<span class="c1">#     x_view = generate_x_view(base)</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     x_view_updated = x_updated.view(-1)</span>
<span class="c1">#     out = x_updated * x_view_updated</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The calling convention change from (aliases) -&gt; (base) happens</span>
<span class="c1"># # *outside* of the autograd.Function.forward().</span>
<span class="c1"># # That means the forward() only has 1 input (base),</span>
<span class="c1"># # and the backward() only has 1 output (grad_base)</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_base = ...</span>
<span class="c1">#     return grad_base</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(base):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(base)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The compiled wrapper is where we create synthetic bases.</span>
<span class="c1"># # The info on which inputs are mutated is also tracked *before* synthetic base creation.</span>
<span class="c1"># def compiled_wrapper(x, x_view):</span>
<span class="c1">#     base = merge_view_inputs(x, x_view)</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(base)</span>
<span class="c1">#     # x and x_view are aliased in eager mode, so this mutation to x will automatically affect x_view.</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>


<span class="c1"># Note [AOT Autograd: Views to avoid tangents aliasing inputs]</span>
<span class="c1">#</span>
<span class="c1"># We view every forward output when creating out tangent tensors to handle the problematic</span>
<span class="c1"># case in which a subclass does extra aliasing between graph outputs/inputs in a way that</span>
<span class="c1"># is not visible above the sublass.</span>
<span class="c1">#</span>
<span class="c1"># Ordinarily, when constructing the joint function that we want to trace in AOTAutograd,</span>
<span class="c1"># we&#39;re guaranteed that the tangent tensors that we pass</span>
<span class="c1"># into the joint are distinct tensors from the primals. This is because when</span>
<span class="c1"># decide which forward outputs to create tangents for, we only create tangents</span>
<span class="c1"># for forward outputs that are not aliases of inputs (See Note</span>
<span class="c1"># [AOT Autograd: outputs aliasing inputs or intermediates!]).</span>
<span class="c1">#</span>
<span class="c1"># However, when wrapper tensor subclasses enter the picture, it is possible</span>
<span class="c1"># to have an output of the forward that is a subclass that is not an</span>
<span class="c1"># input / alias of an input, but one of its inner tensors is an alias!</span>
<span class="c1"># NestedTensor is an example: Performing an out-of-place pointwise op on a</span>
<span class="c1"># NestedTensor constructs a fresh NestedTensor that holds onto the input&#39;s</span>
<span class="c1"># offsets tensor directly.</span>
<span class="c1">#</span>
<span class="c1"># Having tangent tensors that are the same as the (primal) forward inputs,</span>
<span class="c1"># can cause problems during tracing as make_fx() will specialize on our</span>
<span class="c1"># duplicate inputs: If we passed in the same tensor for primals_1 and</span>
<span class="c1"># tangents_1 during tracing, make_fx() will happily sub out all usages of</span>
<span class="c1"># tangents_1 with primals_1 in the graph, which is not what we want.</span>
<span class="c1">#</span>
<span class="c1"># To work around this, we view every forward output when creating out tangent</span>
<span class="c1"># tensors so that tangents can never be the same as forward inputs even if</span>
<span class="c1"># forward inputs alias forward outputs.</span>
<span class="c1">#</span>
<span class="c1">#</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="c1"># This class stores info about every user output.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">OutputAliasInfo</span><span class="p">:</span>
    <span class="c1"># Tells us if this output is:</span>
    <span class="c1"># (1) a regular (non-aliased) output</span>
    <span class="c1"># (2) an alias of a forward input</span>
    <span class="c1"># (3) **is** a forward input (special case of &quot;alias_of_input&quot;)</span>
    <span class="c1"># (4) an alias of an intermediate (aka an alias of an output of the inner traced forward)</span>
    <span class="c1"># (5) an alias of an intermediate, that explicitly requires returning the intermediate</span>
    <span class="c1">#     as a graph output</span>
    <span class="c1"># (6) an alias of an intermediate, where that intermediate is also a user output</span>
    <span class="n">output_type</span><span class="p">:</span> <span class="n">OutputType</span>
    <span class="c1"># The raw type of the output (torch.Tensor, SymInt, etc)</span>
    <span class="n">raw_type</span><span class="p">:</span> <span class="nb">type</span>
    <span class="c1"># If (1) above, then</span>
    <span class="c1"># - base_idx is None</span>
    <span class="c1"># If (2) or (3) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is user_fwd_input[base_idx]</span>
    <span class="c1">#   (This is an index into the inputs *before* we make synthetic bases)</span>
    <span class="c1"># If (4) or (5) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is output_graph_intermediates[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="c1"># If (6) above, then:</span>
    <span class="c1"># - Tells us that the base of this alias is output_user_fwds[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="n">base_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="c1"># If it is a Tensor, what the dynamic dims are (otherwise is None)</span>
    <span class="n">dynamic_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>
    <span class="c1"># requires_grad</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span>


<span class="c1"># This class tells us info about user inputs.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">InputAliasInfo</span><span class="p">:</span>
    <span class="n">is_leaf</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">mutates_data</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">mutates_metadata</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">mutations_hidden_from_autograd</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">SubclassCreationMeta</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Used for AOTDispatch.</span>
<span class="sd">    This dataclass gives us the information we need to reconstruct a tensor subclass</span>
<span class="sd">    from our flat inputs.</span>
<span class="sd">    Why is this important? The graph that we&#39;d like to trace out contains flat tensor inputs,</span>
<span class="sd">    But the user&#39;s original model may have subclass inputs and outputs.</span>
<span class="sd">    So we need to wrap/unwrap subclasses as necessary to translate between the user&#39;s</span>
<span class="sd">    view (subclass inps/outs), and the backend compiler&#39;s view (graph with no subclass args).</span>

<span class="sd">    Complications arise mostly from the fact that a subclass can hold more than one inner tensor;</span>
<span class="sd">    So for a given subclass input/output, we need to carefully track which indices map</span>
<span class="sd">    to the subclass tensor in the corresponding &quot;dense-tensor-only&quot; graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># In the inner graph that only takes in dense tensor inputs,</span>
    <span class="c1"># this maps to the first index of &quot;tensors that should go in this subclass wrapper&quot;</span>
    <span class="n">flat_tensor_start_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># The number of tensors that live in this subclass wrapper</span>
    <span class="n">arg_count</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># Stores the original subclass itself.</span>
    <span class="c1"># This is needed because we need the autograd metadata on the original subclass</span>
    <span class="c1"># (this is guaranteed to be a wrapper subclass that holds a fake tensor,</span>
    <span class="c1">#  so holding onto this at runtime shouldn&#39;t leak memory)</span>
    <span class="n">original_subclass</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="c1"># meta and inner_keys are produced by the subclass&#39;s __tensor_flatten__.</span>
    <span class="c1"># We need to keep them around to plumb them into __tensor_unflatten__.</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">inner_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">any</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">creation_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">all_args</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">is_runtime</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">curr_args</span> <span class="o">=</span> <span class="n">all_args</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">flat_tensor_start_idx</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">flat_tensor_start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">arg_count</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">curr_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inner_keys</span><span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;inner_keys: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inner_keys</span><span class="p">)</span><span class="si">}</span><span class="s1">. len(curr_args): </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">curr_args</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original_subclass</span><span class="p">)</span><span class="o">.</span><span class="n">__tensor_unflatten__</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inner_keys</span><span class="p">,</span> <span class="n">curr_args</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">meta</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_runtime</span><span class="p">:</span>
            <span class="c1"># After wrapping up the inner dense tensors into a subclass, we need to make sure that our new wrapper</span>
            <span class="c1"># has correct autograd metadata, since we&#39;ll be tracing through the autograd engine with the subclass.</span>
            <span class="c1"># We don&#39;t trace through the autograd engine at runtime though, so no need</span>
            <span class="c1"># to compute this extra metadata then!</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_mirror_autograd_meta_to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original_subclass</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># sanity assert to make sure we don&#39;t leak memory</span>
        <span class="k">assert</span> <span class="n">is_fake</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original_subclass</span><span class="p">)</span>

<span class="c1"># This class encapsulates all aliasing + mutation info we need about the forward graph</span>
<span class="c1"># See a more detailed overview of the edge case handling at</span>
<span class="c1"># https://docs.google.com/document/d/19UoIh_SVrMy_b2Sx5ZaeOJttm6P0Qmyss2rdBuyfoic/edit</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="c1"># length = # user inputs</span>
    <span class="c1"># This gives us info about every input, and what sort of mutation happened to it (if any)</span>
    <span class="n">input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]</span>

    <span class="c1"># length = # user outputs</span>
    <span class="c1"># This gives us info about every output (mostly around whether it aliases other tensors)</span>
    <span class="n">output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span>

    <span class="c1"># length = the number of intermediate bases appended as outputs to the end of the forward graph.</span>
    <span class="c1"># Note: this is not necessarily the same thing as:</span>
    <span class="c1">#   len([x for x in output_info if x.output_type == OutputType.alias_of_intermediate])</span>
    <span class="c1"># Because outputs might share a ._base, or an output&#39;s ._base might itself be</span>
    <span class="c1"># another user output (in both cases, we won&#39;t redundantly append bases to the end of the graph)</span>
    <span class="n">num_intermediate_bases</span><span class="p">:</span> <span class="nb">int</span>

    <span class="c1"># For inference only: instructs us to keep data-only input mutations directly in the graph</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">int</span>

    <span class="c1"># length = (# inputs w data mutations) + (# user outputs that are non_aliasing tensors)</span>
    <span class="c1">#        + (# intermediate bases)</span>
    <span class="c1"># These are the FakeTensor (or potential SymInt) outputs that we traced from our</span>
    <span class="c1"># metadata pass of the user&#39;s forward function.</span>
    <span class="c1"># Their only use today is to pass them as a best-guess for tangents when tracing the joint.</span>
    <span class="c1"># Stashing them as part of our &quot;metadata&quot; makes it simpler if we want to run our analysis</span>
    <span class="c1"># pass once, and re-use the output throughout AOTAutograd</span>
    <span class="n">traced_tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>

    <span class="c1"># Each of these is a list telling us about subclasses for the inputs/outputs/grad_outs</span>
    <span class="c1"># They are used throughout AOTDispatch to tell us how to generate a list of subclass tensors,</span>
    <span class="c1"># Given a (potentially larger) list of plain torch tensors.</span>

    <span class="c1"># Taking subclass_inp_meta as an example:</span>
    <span class="c1">#   subclass_inp_meta[i] = j (an int) tells us:</span>
    <span class="c1">#     &quot;The i&#39;th user input is not a subclass, and corresponds to inputs[j] of the plain-tensor graph.&quot;</span>
    <span class="c1">#   subclass_inp_meta[i] = SubclassCreationMeta(flat_tensor_start_idx=3, arg_count=2)</span>
    <span class="c1">#     &quot;The i&#39;th user input is subclass holding two inner tensors, which are</span>
    <span class="c1">#      inputs[3] and inputs[4] of the plain-tensor graph&quot;.</span>

    <span class="c1"># length = # user inputs</span>
    <span class="n">subclass_inp_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]]</span>
    <span class="c1"># So, the full set of outputs to the forward graph looks something like:</span>
    <span class="c1"># (*mutated_inps, *user_outs, *intermediate_bases, *saved_for_bw_tensors)</span>
    <span class="c1"># where the first 3 of those 4 can be subclasses</span>
    <span class="c1"># (but not saved_for_bw tensors, since these are internal to the compiler</span>
    <span class="c1"># and not user visible, so there&#39;s no point in wrapping/unwrapping them at runtime).</span>
    <span class="c1"># This list contains subclass information on all of the fw graph outputs</span>
    <span class="c1"># except for saved_for_bw_tensors.</span>
    <span class="n">subclass_fw_graph_out_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]]</span>
    <span class="c1"># length = # backward graph inputs</span>
    <span class="n">subclass_tangent_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]]</span>
    <span class="c1"># TODO: we should kill this</span>
    <span class="c1"># (need to default it to not break internal)</span>
    <span class="n">is_train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">num_symints_saved_for_bw</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="p">]</span>
        <span class="c1"># pre-compute the indices of the inputs that are mutated.</span>
        <span class="c1"># When keep_input_mutations is set, we don&#39;t need to worry about our epilogue</span>
        <span class="c1"># handling data-only mutations, because we keep them directly in the graph.</span>
        <span class="n">mutated_inp_runtime_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">aliased_out_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="n">unsafe_view_out_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">is</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="n">mutated_inp_indices</span>
        <span class="c1"># This is pre-computed in post_init for perf.</span>
        <span class="c1"># It contains the index of every element</span>
        <span class="c1"># of input_info that corresponds to a mutation (data or metadata or both)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span> <span class="o">=</span> <span class="n">mutated_inp_runtime_indices</span>
        <span class="c1"># This is pre-computed for perf.</span>
        <span class="c1"># It contains the index of every element</span>
        <span class="c1"># of output_info that corresponds to an alias (either of an input or intermediate)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aliased_out_indices</span> <span class="o">=</span> <span class="n">aliased_out_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unsafe_view_out_indices</span> <span class="o">=</span> <span class="n">unsafe_view_out_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_non_aliased</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
             <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_unsafe_view_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unsafe_view_out_indices</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic_outputs</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
        <span class="p">)</span>
        <span class="c1"># See Note: [AOTAutograd Backward Guards]</span>
        <span class="c1"># This is pre-computed for fast asserts on the types of our grad_outputs in the backward.</span>
        <span class="c1"># Eventually, we should kill this and replace with real backward guards.</span>
        <span class="c1"># (we want to precompute the &quot;runtime&quot; types, so replace FakeTensor with torch.Tensor)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">)</span> <span class="k">else</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span>
        <span class="c1"># All of the above metadata is collected by tracing the fw function.</span>
        <span class="c1"># However, extra outputs for rng offsets behave differently. Both fwd</span>
        <span class="c1"># and bwd graphs have their own outputs for the total consumed offsets.</span>
        <span class="c1"># Unlike mutated inputs, we don&#39;t have to worry about sending the right</span>
        <span class="c1"># set of tensors between fwd and bwd. Fwd and bwd offsets are</span>
        <span class="c1"># independent and simpler to handle. Therefore, we track them</span>
        <span class="c1"># separately.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Our forward() returns both (mutated_inputs, outputs, output_intermediate_bases, saved_tensors, saved_symints)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_forward_returns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
        <span class="c1"># In case of functionalization of rng ops, the fw_module returns one</span>
        <span class="c1"># additional output for rng offset. This rng offset is used right</span>
        <span class="c1"># away to advance the rng state, and is not passed on to the raw</span>
        <span class="c1"># outputs. However, we need to know the exact boundary to identify</span>
        <span class="c1"># which tensors to be saved for the bwd graph.  num_forward captures</span>
        <span class="c1"># this information.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_forward_returns</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tensors_saved_for_backwards_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">slice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_forward</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">slice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_forward</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">symints_saved_for_backwards_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># empty slice</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">input_info</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">output_info</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="ow">and</span>
                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">)</span> <span class="ow">and</span>
                <span class="nb">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">)))</span>

<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SubclassMeta</span><span class="p">:</span>
    <span class="c1"># A copy of all forward metadata, but computed on the *dense* tensor forward (after desugaring subclasses)</span>
    <span class="c1"># So for example, if the user had a model containing two `TwoTensor` inputs,</span>
    <span class="c1"># Then `SubclassMeta.fw_metadata.input_infos` would have length 4 here.</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span>

    <span class="c1"># Note: [Computing Subclass Metadata about grad_inputs]</span>
    <span class="c1"># Given a list of flattened, plain tensor grad_inputs, this tells us how to reconstruct the grad_input subclasses</span>
    <span class="c1">#</span>
    <span class="c1"># You might think: why not just assume that all grad_inputs will have the same subclass-ness as the original inputs?</span>
    <span class="c1"># (AOTAutograd generally assumes other properties, e.g. that grad_outputs are contiguous)</span>
    <span class="c1">#</span>
    <span class="c1"># This doesn&#39;t really work though. take this example:</span>
    <span class="c1">#</span>
    <span class="c1"># def f(DoubleTensor, DenseTensor):</span>
    <span class="c1">#     return DoubleTensor  * DenseTensor</span>
    <span class="c1">#</span>
    <span class="c1"># In the above example, the .grad field of *both* DoubleTensor and DenseTensor will be a DoubleTensor.</span>
    <span class="c1"># When we trace out a joint fw-bw graph, we&#39;ll end up returning two subclasses for the two grad_inputs.</span>
    <span class="c1"># This means that our backward graph will return 4 outputs (two dense tensors for each DoubleTensor grad_input)</span>
    <span class="c1"># and we need to properly store the metadata that tells us how to turn these 4 outputs back into DoubleTensors.</span>
    <span class="c1">#</span>
    <span class="c1"># Note that this info **cannot** easily be figured out from ViewAndMutationMeta.</span>
    <span class="c1"># We can only compute this info by tracing the entire joint and examining the grad_inputs that we computed.</span>
    <span class="c1">#</span>
    <span class="c1"># See Note: [AOTAutograd Backward Guards]</span>
    <span class="c1"># This will also eventually require us to install backward guards,</span>
    <span class="c1"># in case we made incorrect assumptions about the subclass-ness of our grad_outputs</span>
    <span class="c1">#</span>
    <span class="c1"># Optional field because we don&#39;t compute for inference graphs</span>
    <span class="n">grad_input_metas</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]]]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># The fields in this class get set after its construction.</span>
        <span class="k">pass</span>


<span class="c1"># This class exists because:</span>
<span class="c1"># - the autograd.Function.forward() in aot autograd returns outputs that might alias inputs</span>
<span class="c1"># - we only care about the metadata on those aliases, so we can regenerate them.</span>
<span class="c1">#   We do not want them to participate in the autograd.Function.</span>
<span class="c1"># We do that by wrapping them in an opaque class, so the autograd.Function</span>
<span class="c1"># does not know to treat them as tensors.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorAlias</span><span class="p">:</span>
    <span class="n">alias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>


<span class="k">def</span> <span class="nf">has_same_metadata</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">t1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">is_conj</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">is_conj</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">is_neg</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">is_neg</span><span class="p">()</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">target_meta_tensor</span><span class="p">,</span> <span class="n">target_requires_grad</span><span class="p">):</span>
    <span class="c1"># Try to do view-replay if possible.</span>
    <span class="c1"># fall back to .as_strided() if we can&#39;t.</span>
    <span class="k">if</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># The base that we want to replay our view off of might have a different shape than the view&#39;s original base.</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_base</span>
        <span class="n">abt</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
        <span class="c1"># Don&#39;t unnecessarily call as_strided if nothing changed; as_strided&#39;s</span>
        <span class="c1"># backward is poorly implemented and slow</span>
        <span class="k">if</span> <span class="n">abt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">b</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="ow">or</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="ow">or</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">reshaped_base_tensor</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
                <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reshaped_base_tensor</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_view_func</span><span class="p">(</span><span class="n">reshaped_base_tensor</span><span class="p">)</span>
        <span class="c1"># This shape mismatch can happen due to a bug in inplace/view handling in autograd.</span>
        <span class="c1"># Try putting a breakpoint here and running</span>
        <span class="c1"># `test/functorch/test_aotdispatch TestAOTAutograd.test_output_all_alias_types`</span>
        <span class="c1"># Also, https://github.com/pytorch/pytorch/issues/49825</span>
        <span class="c1">#</span>
        <span class="c1"># As a stopgap, we&#39;ll fall back to as_strided.</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_requires_grad</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_requires_grad</span><span class="p">:</span>
                <span class="n">out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
    <span class="n">storage_offset</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">)</span>
    <span class="c1"># For outputs aliasing inputs, we need to check if the requires-gradness has changed.</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">aliased_out</span>

<span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="c1"># See Note [Functionalization always runs last]</span>
            <span class="c1"># This means that if we want to &quot;functionalize&quot; a subclass, we need to ensure that the functional wrapper</span>
            <span class="c1"># goes at the bottom.</span>
            <span class="c1"># recurse here, so we can support nested wrapper subclasses</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">transform_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">inner_t</span><span class="p">:</span> <span class="n">to_fun</span><span class="p">(</span><span class="n">inner_t</span><span class="p">))</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_mirror_autograd_meta_to</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">FunctionalTensor</span><span class="o">.</span><span class="n">to_functional</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">t</span>

<span class="k">def</span> <span class="nf">sync_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">attrs</span><span class="p">,</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">:</span>
            <span class="n">sync_functional_tensor</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="c1"># When subclasses are involved, t here will usually look something like:</span>
<span class="c1"># SubclassA(SubclassB(FunctionalTensor(_to_functional_tensor(FakeTensor))))</span>
<span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="c1"># See Note [Functionalization always runs last]</span>
        <span class="c1"># This means that if we want to &quot;functionalize&quot; a subclass, we need to ensure that the functional wrapper</span>
        <span class="c1"># goes at the bottom.</span>
        <span class="c1"># recurse here, so we can support nested wrapper subclasses</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">transform_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">inner_t</span><span class="p">:</span> <span class="n">from_fun</span><span class="p">(</span><span class="n">inner_t</span><span class="p">))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_mirror_autograd_meta_to</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">FunctionalTensor</span><span class="p">):</span>
        <span class="c1"># quick sanity assert</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="n">sync_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">elem</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">is_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="c1"># See Note [Functionalization always runs last]</span>
        <span class="c1"># This means that if we want to &quot;functionalize&quot; a subclass, we need to ensure that the functional wrapper</span>
        <span class="c1"># goes at the bottom.</span>
        <span class="c1"># recurse here, so we can support nested wrapper subclasses</span>
        <span class="n">t_attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="n">t_inners</span> <span class="o">=</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">t_attrs</span><span class="p">]</span>
        <span class="n">any_fun</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">is_fun</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">t_inners</span><span class="p">)</span>
        <span class="n">all_fun</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">is_fun</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">t_inners</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">any_fun</span> <span class="o">==</span> <span class="n">all_fun</span>
        <span class="k">return</span> <span class="n">any_fun</span>

    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">FunctionalTensor</span><span class="p">)</span>

<span class="c1"># t here is either</span>
<span class="c1"># (1) A FunctionalTensor(_to_functional_tensor(FakeTensor))</span>
<span class="c1"># (2) A traceable tensor subclass that holds a FunctionalTensor</span>
<span class="k">def</span> <span class="nf">has_metadata_mutation</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="c1"># A tensor subclass was updated if any of its inner elements were updated</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">has_metadata_mutation</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">FunctionalTensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_functionalize_has_metadata_mutation</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">elem</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">are_all_mutations_hidden_from_autograd</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="c1"># If all inner elements are mutations hidden from autograd, then it is a mutation hidden from autograd.</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">are_all_mutations_hidden_from_autograd</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">FunctionalTensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_functionalize_are_all_mutations_hidden_from_autograd</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">elem</span><span class="p">)</span>

<span class="c1"># new_arg and arg here are either:</span>
<span class="c1"># (1) both a FakeTensor</span>
<span class="c1"># (2) both a traceable tensor subclass that holds a FakeTensor</span>
<span class="c1"># Pre-condition: the two args are the &quot;old&quot; and &quot;new&quot; inputs from running functionalization.</span>
<span class="c1"># When we run functionalization and wrap our inputs into FunctionalTensors,</span>
<span class="c1"># we can detect whether or not an input was mutated by checking to see if the inner tensor has changed</span>
<span class="c1">#</span>
<span class="c1"># Normally it would be enough just to check if arg is new_arg, which is normally enough for functionalization</span>
<span class="c1"># to confirm that inputs were not mutated when running the user&#39;s model with functionalization on.</span>
<span class="c1"># But when we have subclass inputs, we can&#39;t rely on that:</span>
<span class="c1"># `from_fun(to_fun(x)) is x` will return False, because the call to `from_fun` constructs</span>
<span class="c1"># a brand new subclass instance: we are calling __tensor_unflatten__, and going</span>
<span class="c1"># from Subclass(FakeTensor) to Subclass(FunctionalTensor(FakeTensor))</span>
<span class="k">def</span> <span class="nf">was_updated</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">new_arg</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">new_arg</span><span class="p">)</span>
        <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">arg</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="n">new_attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">new_arg</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">attrs</span> <span class="o">==</span> <span class="n">new_attrs</span>
        <span class="c1"># A tensor subclass was updated if any of its inner elements were updated</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">was_updated</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">new_arg</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_arg</span>

<span class="c1"># new_arg and arg here are either:</span>
<span class="c1"># (1) both a FakeTensor</span>
<span class="c1"># (2) both a traceable tensor subclass that holds a FakeTensor</span>
<span class="c1"># Pre-condition: the two args are the &quot;old&quot; and &quot;new&quot; inputs from running functionalization.</span>
<span class="c1"># When we run functionalization and wrap our inputs into FunctionalTensors,</span>
<span class="c1"># we can detect whether or not an input was mutated by checking to see if the inner tensor has changed,</span>
<span class="c1"># but shares storage with the old input</span>
<span class="k">def</span> <span class="nf">was_metadata_updated</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">new_arg</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">new_arg</span><span class="p">)</span>
        <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">arg</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="n">new_attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">new_arg</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">attrs</span> <span class="o">==</span> <span class="n">new_attrs</span>
        <span class="c1"># A tensor subclass was updated if any of its inner elements were updated</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">was_metadata_updated</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">new_arg</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_arg</span> <span class="ow">and</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="o">==</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">new_arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">_get_hints</span><span class="p">(</span><span class="n">exprs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the hints of a list/tuple of int/SymInt.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">exprs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">exprs</span><span class="p">)(</span><span class="n">_get_hints</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">exprs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">exprs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">exprs</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">shape_env</span><span class="o">.</span><span class="n">size_hint</span><span class="p">(</span><span class="n">exprs</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">exprs</span>

<span class="k">def</span> <span class="nf">requires_subclass_dispatch</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">args_flattened</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="n">any_subclass_args</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args_flattened</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span>
    <span class="n">any_subclass_outputs</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangents</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span>
    <span class="c1"># This tells us whether or not we need to perform any unwrapping/wrapping of tensor subclasses at runtime.</span>
    <span class="k">return</span> <span class="n">any_subclass_args</span> <span class="ow">or</span> <span class="n">any_subclass_outputs</span>

<span class="c1"># Given a flat list of arguments, some of which may be tensor subclasses,</span>
<span class="c1"># computes metadata about &quot;how to reconstruct the current list of subclasses,</span>
<span class="c1"># if we were given their flattened dense tensors instead&quot;</span>
<span class="k">def</span> <span class="nf">create_subclass_meta</span><span class="p">(</span><span class="n">curr_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]]:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">infos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">curr_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
            <span class="n">attrs</span><span class="p">,</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">idx</span>
            <span class="n">cnt</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">attrs</span><span class="p">)</span>
            <span class="n">curr_cnt</span> <span class="o">=</span> <span class="n">cnt</span>
            <span class="n">infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SubclassCreationMeta</span><span class="p">(</span>
                <span class="n">flat_tensor_start_idx</span><span class="o">=</span><span class="n">start_idx</span><span class="p">,</span>
                <span class="n">arg_count</span><span class="o">=</span><span class="n">curr_cnt</span><span class="p">,</span>
                <span class="n">original_subclass</span><span class="o">=</span><span class="n">a</span><span class="p">,</span>
                <span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">,</span>
                <span class="n">inner_keys</span><span class="o">=</span><span class="n">attrs</span><span class="p">,</span>
            <span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">cnt</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="n">cnt</span>
    <span class="k">return</span> <span class="n">infos</span>

<span class="c1"># This is a version of functionalization that is specifically designed</span>
<span class="c1"># for the AOTAutograd use case.</span>
<span class="c1">#</span>
<span class="c1"># Unlike functorch&#39;s variant, this doesn&#39;t use the functorch level system,</span>
<span class="c1"># instead it directly uses PyTorch&#39;s conventional dispatcher to hit the</span>
<span class="c1"># functionalization key.  In particular, this means that FunctionalTensorWrapper</span>
<span class="c1"># can have autograd data stored directly on it.</span>
<span class="c1">#</span>
<span class="c1"># In typical AOTAutograd usage, the dispatch key order will look like:</span>
<span class="c1">#</span>
<span class="c1">#   Autograd - Functionalization ~~~~&gt; Proxy Mode - Fake Tensor</span>
<span class="c1">#       outer tensor                        inner tensor</span>
<span class="c1">#</span>
<span class="c1"># Returns:</span>
<span class="c1"># - ViewAndMutationMeta, telling us metadata about the inputs and outputs, and</span>
<span class="c1">#   The list of outputs from the forward, but **only** the outputs that we need</span>
<span class="c1">#   to pass in as tangents into the backward.</span>
<span class="c1">#   Specifically, aliased outputs from the forward get regenerated, and don&#39;t participate</span>
<span class="c1">#   in the compiled backward function.</span>
<span class="k">def</span> <span class="nf">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
    <span class="n">f</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="c1"># TODO: refactor to kill this flag</span>
    <span class="n">is_train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">_to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
            <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
            <span class="k">return</span> <span class="n">r</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">t</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="c1"># This function is meant to be run with the forward, which expects a flat list of tensor/symint/other args.</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">KNOWN_TYPES</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">)</span>

        <span class="n">input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">flat_f_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">_to_fun</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>

        <span class="c1"># See Note [Disabling Functionalize TLS Above Python Functionalization]</span>
        <span class="n">disable_above</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_ExcludeDispatchKeyGuard</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKeySet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKey</span><span class="o">.</span><span class="n">Functionalize</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">disable_above</span><span class="p">,</span> <span class="n">FunctionalTensorMode</span><span class="p">():</span>
            <span class="c1"># precondition: The passed in function already handles unflattening inputs + flattening outputs</span>
            <span class="n">flat_f_outs</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">flat_f_args</span><span class="p">)</span>

        <span class="c1"># Inspect the state of the input tensor functional wrapper to detect input mutation info</span>
        <span class="c1"># If inp[i] has a metadata-only mutation, then maybe_inputs_with_mutated_metadata[i] contains the updated version</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">f_arg</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_f_args</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">arg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">from_fun</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">was_updated</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">new_arg</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">was_metadata_updated</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">new_arg</span><span class="p">):</span>
                    <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="n">has_metadata_mutation</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
                <span class="n">mutations_hidden_from_autograd</span> <span class="o">=</span> <span class="n">are_all_mutations_hidden_from_autograd</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">mutations_hidden_from_autograd</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">InputAliasInfo</span><span class="p">(</span>
                <span class="n">is_leaf</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">safe_is_leaf</span><span class="p">(</span><span class="n">arg</span><span class="p">),</span>
                <span class="n">mutates_data</span><span class="o">=</span><span class="n">mutates_data</span><span class="p">,</span>
                <span class="n">mutates_metadata</span><span class="o">=</span><span class="n">mutates_metadata</span><span class="p">,</span>
                <span class="n">mutations_hidden_from_autograd</span><span class="o">=</span><span class="n">mutations_hidden_from_autograd</span><span class="p">,</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">f_arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">f_arg</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">))</span>

        <span class="c1"># If a function involves creating a tensor, and returning a view of it, such that its _base is the intermediate,</span>
        <span class="c1"># We need to make sure our graph returns the _base as a graph output, and we manually recreate the view</span>
        <span class="c1"># to return to the user. Why? The backend compiler is free to (incorrectly) not set requires_grad</span>
        <span class="c1"># on the base tensor, but we are obligated to properly set requires-gradness on the real output.</span>

        <span class="n">inp_storage_refs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()):</span> <span class="n">idx</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1"># We need inp tensor id&#39;s to be able to tell if an outputs **are** inputs.</span>
        <span class="n">inp_tensor_ids</span> <span class="o">=</span> <span class="p">{</span>
            <span class="nb">id</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span> <span class="k">for</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="n">flat_f_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="c1"># We need output tensor id&#39;s to tell if any output._base` attributes **are** other outputs.</span>
        <span class="c1"># (This is also a dict because we need to know that output&#39;s index, so we can regenerate</span>
        <span class="c1"># the alias from it).</span>
        <span class="n">out_tensor_ids</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">)}</span>

        <span class="c1"># Keep track of which outputs alias other outputs</span>
        <span class="n">out_tensor_alias_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="c1"># This tells us, for a given group of outputs that alias each other,</span>
        <span class="c1"># whether they e.g. all came from an unbind call</span>
        <span class="n">num_aliased_tensors_that_are_multi_output_views</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">out_storage_to_tensors</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">flat_f_outs</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">curr_storage</span> <span class="o">=</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
                <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="c1"># Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]</span>
                <span class="c1"># This is an optimization on top of the &quot;alias of intermediates&quot; logic,</span>
                <span class="c1"># which you can read more about under Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
                <span class="c1">#</span>
                <span class="c1"># Before describing the optimization: this is important for AOTAutograd to have good</span>
                <span class="c1"># perf around, multi-output views. HOWEVER:</span>
                <span class="c1"># - There is a more generic change to AOTAutograd that we&#39;d like to make, that subsumes this case,</span>
                <span class="c1">#   around using pre-dispatch tracing to partition out a graph so we can faithfully replay all</span>
                <span class="c1">#   views without having to regenerate them at runtime.</span>
                <span class="c1"># - It&#39;s loosely described in this doc (more details will be added soon):</span>
                <span class="c1">#   https://docs.google.com/document/d/1DlfFq8TKbuAn2zyJxLfoW-X1qkkm5PLdHFtySo03QAk/edit</span>
                <span class="c1"># - Once that change lands, we should just rip out this &quot;optimization&quot;, since:</span>
                <span class="c1">#   (1) It will be fully unnecessary</span>
                <span class="c1">#   (2) Although it is only a few lines of code, it is a bit difficult to reason about</span>
                <span class="c1">#       its correctness with the autograd engine in all cases.</span>
                <span class="c1">#</span>
                <span class="c1">#</span>
                <span class="c1"># What is this optimization? Consider the below case:</span>
                <span class="c1"># def f(x):</span>
                <span class="c1">#     intermediate = x.mul(2)</span>
                <span class="c1">#     # x and intermediate here require grad</span>
                <span class="c1">#     o1, o2, ... o10 = intermediate.unbind(-1)</span>
                <span class="c1">#     return intermediate, o1, o2, ... o10</span>
                <span class="c1"># Now, the &quot;intermediate base&quot; handling in AOTAutograd implies that we must do the following:</span>
                <span class="c1">#   (1) return &quot;intermediate as an extra output of the compiled graph</span>
                <span class="c1">#   (2) regenerate each aliased output off of &quot;intermediate&quot;, **outside** of the autograd.Function.</span>
                <span class="c1"># The reason AOTAutograd ordinarily does this is for safety: the autograd engine needs to know</span>
                <span class="c1"># that o1 through o10 are all aliased, and if we blindly return o1 through o10 from the autograd.Function,</span>
                <span class="c1"># this information will be hidden.</span>
                <span class="c1"># In particular, mutating one alias might require autograd to update autograd metadata on the other aliases</span>
                <span class="c1"># (like their grad_fn, for example, when the autograd engine needs to do view-replay).</span>
                <span class="c1">#</span>
                <span class="c1"># However, intermediate_base logic can be bad for backward performance (we sometimes generate</span>
                <span class="c1"># as_strided calls during the intermediate base logic, which can have a slow backward formula).</span>
                <span class="c1"># Is it possible to find a set of conditions where it is **safe** to hide the output aliasing from autograd?</span>
                <span class="c1">#</span>
                <span class="c1"># For a set of outputs of the graph that alias each other, o_1...o_k, consider:</span>
                <span class="c1"># (1) They came from the same multi-output view op, e.g. o_1, ..., o_k = intermediate.unbind(0)</span>
                <span class="c1"># (2) If there are any other aliases of o_1 through o_k (in the example above, intermediate),</span>
                <span class="c1">#     **at most** 1 can escape from the graph (e.g. there is not some other graph input/output</span>
                <span class="c1">#     o_other, that aliases these outputs)</span>
                <span class="c1"># (3) o_1...o_k all require_grad, they all share the same ._base, and their ._base requires grad.</span>
                <span class="c1">#     This condition is important because it&#39;s what causes slowness in the intermediate_base</span>
                <span class="c1">#     codepath of aot_autograd. Ordinarily, o_1...o_k would all get a grad_fn, and</span>
                <span class="c1">#     aot_autograd&#39;s view-replay might give each output an AsStridedBackward as its grad_fn.</span>
                <span class="c1">#     &quot;K&quot; AsStridedBackward calls will be *much* slower than a single UnbindBackward.</span>
                <span class="c1"># In this setup, is it possible to mutate one of the outputs o_i in a way that would affect the autograd meta</span>
                <span class="c1"># of the other aliases?</span>
                <span class="c1">#</span>
                <span class="c1"># Claim: No! Consider a few example (which I&#39;m pretty sure cover all cases of mutation w.r.t. autograd):</span>
                <span class="c1"># (a) What happens if we mutate any of o_1 through o_k directly?</span>
                <span class="c1">#     Autograd raises an error:</span>
                <span class="c1">#     &quot;RuntimeError: Output 0 of UnbindBackward0 is a view and is being modified inplace. This view is</span>
                <span class="c1">#      the output of a function that returns multiple views. Such functions do not allow the output</span>
                <span class="c1">#      views to be modified inplace. You should replace the inplace operation by an out-of-place one.&quot;</span>
                <span class="c1"># (b) What if we take a view of o_k and mutate it, o_k.view(o_k.shape).mul_(2)?</span>
                <span class="c1">#     Autograd raises the same error- the &quot;multi-output-view&quot;ness of an alias propagates to future views.</span>
                <span class="c1"># (c) What if we mutate o_k under no_grad?</span>
                <span class="c1">#     Autograd raises the same error</span>
                <span class="c1"># (d) What if we detach and mutate, e.g. o_k.detach().mul_(2)?</span>
                <span class="c1">#     Autograd allows this, *but* autograd updates all alias&#39;s grad_fn&#39;s to be error functions when accessed.</span>
                <span class="c1">#     Autograd raises the same error</span>
                <span class="c1"># (e) What if we try to mutate another alias of o_1...o_k, that was **not** created from a multi-output view?</span>
                <span class="c1">#     We promised that there is at most **one** such alias, e.g. intermediate in the example above.</span>
                <span class="c1">#     You can mutate intermediate, but in eager mode this will change the grad_fn of o_1...o_k</span>
                <span class="c1">#     to be error fn&#39;s.</span>
                <span class="c1">#     Since intermediate was the *only* non-multi-output-alias, there are no other aliases</span>
                <span class="c1">#     of `intermediate` around that were produced by the compiled fn and have a valid grad_fn.</span>
                <span class="c1">#</span>
                <span class="c1"># Coming back to this optimization:</span>
                <span class="c1"># Given that it is not possible for mutating one of these aliases to affect the autograd metadata of another alias</span>
                <span class="c1"># without causing an error in eager mode, we will simple hide the aliasing from autograd during torch.compile</span>
                <span class="c1"># if all of the above conditions are met.</span>
                <span class="c1"># This has the slight downside that it&#39;s possible to write some &quot;bad&quot; code that autograd will raise an error on</span>
                <span class="c1"># in eager but fail to during torch.compile, but it has the benefit that this code has much better performance.</span>
                <span class="c1"># NOTE: if and when we eventually update AOTAutograd to do the &quot;view graph slicing&quot; defined here:</span>
                <span class="c1"># https://docs.google.com/document/d/1DlfFq8TKbuAn2zyJxLfoW-X1qkkm5PLdHFtySo03QAk/edit,</span>
                <span class="c1"># then this optimization will probably matter less and might be ok to remove.</span>
                <span class="n">is_cur_tensor_multi_out_view</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">FunctionalTensor</span><span class="p">)</span> \
                    <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">_functionalize_is_multi_output_view</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">elem</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">is_cur_tensor_multi_out_view</span><span class="p">:</span>
                    <span class="n">num_aliased_tensors_that_are_multi_output_views</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">out_storage_to_tensors</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

        <span class="c1"># maps the id of an intermediate base to its index in the output of the compiled forward</span>
        <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">intermediate_bases</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">flat_f_outs</span><span class="p">:</span>
            <span class="n">curr_storage</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
            <span class="n">outs_with_identical_metadata_that_require_grad</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span>
                <span class="n">curr</span> <span class="k">for</span> <span class="n">curr</span> <span class="ow">in</span> <span class="n">out_storage_to_tensors</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">has_same_metadata</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">curr</span><span class="p">)</span> <span class="ow">and</span> <span class="n">curr</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">curr</span>
            <span class="p">]</span>
            <span class="n">is_result_of_custom_autograd_fn</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="c1"># Need to check for both custom cpp (CppFunction) and python (BackwardCFunction) autograd fns</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;CppFunction&quot;</span><span class="p">:</span>
                    <span class="n">is_result_of_custom_autograd_fn</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">BackwardCFunction</span><span class="p">):</span>
                    <span class="n">is_result_of_custom_autograd_fn</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="n">curr_storage</span> <span class="ow">in</span> <span class="n">inp_storage_refs</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> \
                    <span class="ow">and</span> <span class="n">is_result_of_custom_autograd_fn</span><span class="p">:</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="n">curr_storage</span> <span class="ow">in</span> <span class="n">inp_storage_refs</span><span class="p">:</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="n">inp_storage_refs</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span>
                <span class="n">is_input_tensor</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">in</span> <span class="n">inp_tensor_ids</span>
                <span class="n">num_aliased_outs</span> <span class="o">=</span> <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span>
                <span class="n">num_multi_output_view_outs</span> <span class="o">=</span> <span class="n">num_aliased_tensors_that_are_multi_output_views</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span>
                <span class="n">num_aliased_outs_that_are_not_multi_output_views</span> <span class="o">=</span> <span class="n">num_aliased_outs</span> <span class="o">-</span> <span class="n">num_multi_output_view_outs</span>
                <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_aliased_outs_that_are_not_multi_output_views</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># See Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]</span>
                    <span class="c1"># In particular, given:</span>
                    <span class="c1"># def f(x):</span>
                    <span class="c1">#     return list(x.unbind(0))</span>
                    <span class="c1"># The main reason we ordinarily try to regenerate these output aliases outside of the</span>
                    <span class="c1"># compiled autograd.Function is because if any of the outputs are later mutated,</span>
                    <span class="c1"># autograd needs to perform view-replay to regenerate them.</span>
                    <span class="c1"># However, autograd does not allow users to mutate multi-output views</span>
                    <span class="c1"># in any way that can change the autograd metadata of other aliases.</span>
                    <span class="c1"># So we hide this aliasing from autograd here.</span>
                    <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Encountered AOTAutograd case: differentiable outputs that </span><span class="se">\</span>
<span class="s2">alias each other from a multi-output view call&quot;</span><span class="p">)</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="k">elif</span> <span class="n">is_input_tensor</span><span class="p">:</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>

            <span class="c1"># We only need to handle the intermediate base case when both</span>
            <span class="c1"># the intermediate base and the output require gradients.</span>
            <span class="c1"># See Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="n">o</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">):</span>
                <span class="n">num_aliased_outs</span> <span class="o">=</span> <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span>
                <span class="n">num_multi_output_view_outs</span> <span class="o">=</span> <span class="n">num_aliased_tensors_that_are_multi_output_views</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span>
                <span class="n">num_aliased_outs_that_are_not_multi_output_views</span> <span class="o">=</span> <span class="n">num_aliased_outs</span> <span class="o">-</span> <span class="n">num_multi_output_view_outs</span>
                <span class="c1"># Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]</span>
                <span class="k">if</span> <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">num_aliased_outs_that_are_not_multi_output_views</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Note [Intermediate Bases Optimization]</span>
                    <span class="c1"># Normally if we have an output that aliases an intermediate,</span>
                    <span class="c1"># we need to add the extra &quot;intermediate base&quot; logic further down</span>
                    <span class="c1"># to prevent autograd from yelling at us if the user later tries to</span>
                    <span class="c1"># mutate that output.</span>
                    <span class="c1"># However, the common case here is if we have an output that aliases an intermediate,</span>
                    <span class="c1"># but doesn&#39;t alias any other outputs.</span>
                    <span class="c1"># In that case, autograd shouldn&#39;t have to worry about the aliasing at all</span>
                    <span class="c1"># (if that output is mutated, there are no other live aliases for autograd to worry about).</span>
                    <span class="c1"># The &quot;intermediate bases&quot; can hurt inductor perf by forcing more variables to become outputs.</span>
                    <span class="c1"># So as an optimization, we won&#39;t do intermediate base handling in this case.</span>
                    <span class="c1"># Instead, we&#39;ll hide the aliasing from autograd using aten._unsafe_view().</span>
                    <span class="k">if</span> <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">num_aliased_outs_that_are_not_multi_output_views</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Encountered AOTAutograd case: differentiable outputs that alias each other </span><span class="se">\</span>
<span class="s2">from a multi-output view call&quot;</span><span class="p">)</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span>
                    <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># First, check if o&#39;s ._base is an existing output</span>
                    <span class="n">maybe_existing_out_idx</span> <span class="o">=</span> <span class="n">out_tensor_ids</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_existing_out_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># Special case where the output is an alias of a graph intermediate, but that intermediate</span>
                        <span class="c1"># is itself also a user output.</span>
                        <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                        <span class="n">base_idx</span> <span class="o">=</span> <span class="n">maybe_existing_out_idx</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Next, check if o&#39;s ._base is an intermediate base that we already returned</span>
                        <span class="n">maybe_existing_base_output_idx</span> <span class="o">=</span> <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                            <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">),</span> <span class="kc">None</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">maybe_existing_base_output_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span>
                            <span class="n">base_idx</span> <span class="o">=</span> <span class="n">maybe_existing_base_output_idx</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Otherwise, take o._base and explicitly return it as an output in the compiled graph</span>
                            <span class="n">new_out_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">)</span>
                            <span class="n">base_idx</span> <span class="o">=</span> <span class="n">new_out_idx</span>
                            <span class="c1"># Indicate to the logic later on (when we trace the joint)</span>
                            <span class="c1"># that this particular output should get it&#39;s ._base appended to the forward graph outputs</span>
                            <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span>
                            <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_out_idx</span>
                            <span class="n">intermediate_bases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="c1"># See https://github.com/pytorch/pytorch/issues/100348 for this case.</span>
                <span class="c1"># This protects against the specific case where a user fn returns (output, output.detach())</span>
                <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">curr_storage</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs_with_identical_metadata_that_require_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs_with_identical_metadata_that_require_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="c1"># In theory we could use any of these tensors to regenerate the aliased outputs from,</span>
                <span class="c1"># since they all alias each other and have identical metatadata</span>
                <span class="n">out_alias</span> <span class="o">=</span> <span class="n">outs_with_identical_metadata_that_require_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">existing_out_idx</span> <span class="o">=</span> <span class="n">out_tensor_ids</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">out_alias</span><span class="p">)]</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="n">existing_out_idx</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">dynamic_dims</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_concrete_int</span><span class="p">(</span><span class="n">s</span><span class="p">)}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dynamic_dims</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">out_info</span> <span class="o">=</span> <span class="n">OutputAliasInfo</span><span class="p">(</span>
                <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">,</span>
                <span class="n">raw_type</span><span class="o">=</span><span class="nb">type</span><span class="p">(</span><span class="n">o</span><span class="p">),</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="n">base_idx</span><span class="p">,</span>
                <span class="n">dynamic_dims</span><span class="o">=</span><span class="n">dynamic_dims</span><span class="p">,</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">)</span>
            <span class="n">output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_info</span><span class="p">)</span>

        <span class="c1"># See Note [AOT Autograd: Views to avoid tangents aliasing inputs]</span>
        <span class="k">def</span> <span class="nf">view_avoid_dupes_with_primals</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">transform_subclass</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">inner_t</span><span class="p">:</span> <span class="n">view_avoid_dupes_with_primals</span><span class="p">(</span><span class="n">inner_t</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">t</span>

        <span class="c1"># This analysis function returns *only* the outputs that are meant to be tangents to the backwards.</span>
        <span class="c1"># Anything that aliases (inputs returned in the fw due to metadata mutations, or outputs that alias inputs/intermediates)</span>
        <span class="c1"># are *regenerated* later, and not used directly in the autograd graph</span>
        <span class="n">f_input_tangents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">inp</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">,</span> <span class="n">input_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">info</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">]</span>
        <span class="n">f_output_tangents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">o</span>
            <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">,</span> <span class="n">output_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span><span class="p">]</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">info</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">]</span>
        <span class="c1"># intermediate bases are also included in the backward graph</span>
        <span class="n">f_tangents</span> <span class="o">=</span> <span class="n">f_input_tangents</span> <span class="o">+</span> <span class="n">f_output_tangents</span> <span class="o">+</span> <span class="n">intermediate_bases</span>
        <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_tangents</span><span class="p">)</span>
        <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">view_avoid_dupes_with_primals</span><span class="p">,</span> <span class="n">traced_tangents</span><span class="p">)</span>
        <span class="n">user_outs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_output_tangents</span><span class="p">)</span>

        <span class="n">f_mutated_inputs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">inp</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">,</span> <span class="n">input_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_metadata</span>
        <span class="p">]</span>
        <span class="n">f_metadata_mutated_inputs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">inp</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">,</span> <span class="n">input_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_metadata</span>
        <span class="p">]</span>
        <span class="c1"># This logic (annoyingly) re-figures out exactly what the outputs to the compiled fw graph will be.</span>
        <span class="c1"># When handling subclasses, we need info about **all** outputs of compiled forward graph,</span>
        <span class="c1"># so we know precisely which graph outputs to wrap back into tensor subclasses</span>
        <span class="c1"># Ideally we would refactor this so not have an is_train flag, and have the separate</span>
        <span class="c1"># inference and training paths decide which inputs/output to ask for subclass info on.</span>
        <span class="c1"># However, we currently stash indexing information on each SubclassMeta about its order</span>
        <span class="c1"># in the graph outputs list.</span>
        <span class="n">f_fw_graph_outs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_train</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">keep_input_mutations</span><span class="p">:</span>
            <span class="n">f_fw_graph_outs</span> <span class="o">=</span> <span class="n">f_mutated_inputs</span> <span class="o">+</span> <span class="n">f_fw_graph_outs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># even when &quot;keep_input_mutations&quot; is True,</span>
            <span class="c1"># we never keep metadata-only mutations in the fw graph</span>
            <span class="n">f_fw_graph_outs</span> <span class="o">=</span> <span class="n">f_metadata_mutated_inputs</span> <span class="o">+</span> <span class="n">f_fw_graph_outs</span>
        <span class="k">if</span> <span class="n">is_train</span><span class="p">:</span>
            <span class="n">f_fw_graph_outs</span> <span class="o">=</span> <span class="n">f_fw_graph_outs</span> <span class="o">+</span> <span class="n">intermediate_bases</span>
        <span class="n">fw_graph_outs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_fw_graph_outs</span><span class="p">)</span>

        <span class="n">metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
            <span class="n">input_info</span><span class="o">=</span><span class="n">input_info</span><span class="p">,</span>
            <span class="n">output_info</span><span class="o">=</span><span class="n">output_info</span><span class="p">,</span>
            <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">),</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">keep_input_mutations</span><span class="p">,</span>
            <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
            <span class="n">subclass_inp_meta</span><span class="o">=</span><span class="n">create_subclass_meta</span><span class="p">(</span><span class="n">flat_args</span><span class="p">),</span>
            <span class="n">subclass_fw_graph_out_meta</span><span class="o">=</span><span class="n">create_subclass_meta</span><span class="p">(</span><span class="n">fw_graph_outs</span><span class="p">),</span>
            <span class="n">subclass_tangent_meta</span><span class="o">=</span><span class="n">create_subclass_meta</span><span class="p">(</span><span class="n">traced_tangents</span><span class="p">),</span>
            <span class="n">is_train</span><span class="o">=</span><span class="n">is_train</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">metadata</span>

    <span class="k">return</span> <span class="n">inner</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BackwardSignature</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Provides information about the backward section of an exported</span>
<span class="sd">    joint forward-backward graph.</span>
<span class="sd">    For a particular fx GraphModule, this class contains information on:</span>
<span class="sd">    (1) A mapping from each gradient (backwards output) to the parameter</span>
<span class="sd">        it corresponds to (forward input)</span>
<span class="sd">    (2) A mapping from each gradient (backwards output) to the user input</span>
<span class="sd">        it corresponds to (forward input)</span>
<span class="sd">    (3) Which of the forward outputs corresponds to the loss, that we backprop on.</span>

<span class="sd">    Each string name is the `node.name` of the corresponding node in the fx graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gradients_to_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="n">gradients_to_user_inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="n">loss_output</span><span class="p">:</span> <span class="nb">str</span>

<span class="n">GraphOutputName</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;GraphOutputName&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
<span class="n">GraphInputName</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;GraphInputName&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
<span class="n">FQN</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;FQN&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GraphSignature</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Provides information about an exported module.</span>
<span class="sd">    For a particular fx GraphModule, this class contains information on:</span>
<span class="sd">    (1) Which graph inputs are parameters, buffers, or user inputs</span>
<span class="sd">    (2) (for params/buffers) a mapping from the name of each graph argument</span>
<span class="sd">        to its parameter/buffer FQN in the original nn.Module.</span>
<span class="sd">    (3) If there are input mutations, these are represented as extra outputs</span>
<span class="sd">        in the fx GraphModule. We provide a mapping from these</span>
<span class="sd">        extra output names to the names of the actual inputs.</span>
<span class="sd">    (4) The pytree metadata on how to flatten/unflatten inputs and outputs.</span>
<span class="sd">        The corresponding FX GraphModule only accepts and returns</span>
<span class="sd">        pytree-flattened inputs/outputs.</span>
<span class="sd">    (5) (Optionally) if the FX is a joint forward-backward graph, we provide</span>
<span class="sd">        a signature on the backward section of the joint graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FQN</span><span class="p">]</span>
    <span class="n">buffers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FQN</span><span class="p">]</span>

    <span class="n">user_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">GraphInputName</span><span class="p">]</span>
    <span class="n">user_outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">GraphOutputName</span><span class="p">]</span>
    <span class="n">inputs_to_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">GraphInputName</span><span class="p">,</span> <span class="n">FQN</span><span class="p">]</span>
    <span class="n">inputs_to_buffers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">GraphInputName</span><span class="p">,</span> <span class="n">FQN</span><span class="p">]</span>

    <span class="c1"># If the user&#39;s module mutates a buffer,</span>
    <span class="c1"># it&#39;s represented in the graph as an extra graph output.</span>
    <span class="c1"># This dict is a mapping from</span>
    <span class="c1"># &quot;graph outputs that correspond to updated buffers&quot;</span>
    <span class="c1"># to the FQN names of those mutated buffers.</span>
    <span class="n">buffers_to_mutate</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">GraphOutputName</span><span class="p">,</span> <span class="n">FQN</span><span class="p">]</span>

    <span class="n">in_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span>
    <span class="n">out_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span>

    <span class="n">backward_signature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BackwardSignature</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_tracing_metadata</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">in_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">,</span>
        <span class="n">out_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">,</span>
        <span class="n">graph_input_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">graph_output_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">view_mutation_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
        <span class="n">named_parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">named_buffers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">num_user_inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_user_outputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">loss_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">backward_signature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BackwardSignature</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;GraphSignature&quot;</span><span class="p">:</span>
        <span class="n">graph_inputs</span> <span class="o">=</span> <span class="n">graph_input_names</span>
        <span class="n">graph_outputs</span> <span class="o">=</span> <span class="n">graph_output_names</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">)</span>
        <span class="n">buffers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>

        <span class="c1"># Calling convention assumptions:</span>
        <span class="c1"># (1) graph inputs = (params, buffers, user_inputs)</span>
        <span class="c1"># (2) graph outputs = (mutated_inputs, user_outs, param_gradients)</span>
        <span class="c1"># (If we are capturing an inference graph, this convention is identical</span>
        <span class="c1">#  except that param_gradients is empty)</span>
        <span class="n">user_inputs</span> <span class="o">=</span> <span class="n">graph_inputs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffers</span><span class="p">)</span> <span class="p">:]</span>
        <span class="k">assert</span> <span class="n">num_user_inputs</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_inputs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">graph_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffers</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_inputs</span><span class="p">))</span>

        <span class="n">inputs_to_parameters</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">graph_inputs</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)],</span> <span class="n">parameters</span><span class="p">))</span>
        <span class="n">inputs_to_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
            <span class="n">graph_inputs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffers</span><span class="p">)],</span>
            <span class="n">buffers</span><span class="p">,</span>
        <span class="p">))</span>

        <span class="n">state_names</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">parameters</span><span class="p">,</span> <span class="o">*</span><span class="n">buffers</span><span class="p">]</span>
        <span class="n">mutated_buffers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">input_info</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">view_mutation_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">input_info</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                <span class="c1"># Only buffers can be mutated, not parameters</span>
                <span class="k">assert</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
                <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">state_names</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">mutated_buffers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buffer_name</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_buffers</span><span class="p">)</span> <span class="o">==</span> <span class="n">view_mutation_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>

        <span class="n">start</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">view_mutation_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
        <span class="n">buffers_to_mutate</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">graph_outputs</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">],</span> <span class="n">mutated_buffers</span><span class="p">))</span>

        <span class="n">start</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">stop</span><span class="p">,</span> <span class="n">stop</span> <span class="o">+</span> <span class="n">num_user_outputs</span>
        <span class="n">user_outputs</span> <span class="o">=</span> <span class="n">graph_outputs</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">]</span>

        <span class="n">unused_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">graph_outputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">stop</span>
        <span class="k">if</span> <span class="n">backward_signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unused_outputs</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">backward_signature</span><span class="o">.</span><span class="n">gradients_to_parameters</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">backward_signature</span><span class="o">.</span><span class="n">gradients_to_user_inputs</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="n">unused_outputs</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">GraphSignature</span><span class="p">(</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
            <span class="n">buffers</span><span class="o">=</span><span class="n">buffers</span><span class="p">,</span>
            <span class="n">user_inputs</span><span class="o">=</span><span class="n">user_inputs</span><span class="p">,</span>
            <span class="n">user_outputs</span><span class="o">=</span><span class="n">user_outputs</span><span class="p">,</span>
            <span class="n">inputs_to_buffers</span><span class="o">=</span><span class="n">inputs_to_buffers</span><span class="p">,</span>
            <span class="n">inputs_to_parameters</span><span class="o">=</span><span class="n">inputs_to_parameters</span><span class="p">,</span>
            <span class="n">buffers_to_mutate</span><span class="o">=</span><span class="n">buffers_to_mutate</span><span class="p">,</span>
            <span class="n">in_spec</span><span class="o">=</span><span class="n">in_spec</span><span class="p">,</span>
            <span class="n">out_spec</span><span class="o">=</span><span class="n">out_spec</span><span class="p">,</span>
            <span class="n">backward_signature</span><span class="o">=</span><span class="n">backward_signature</span><span class="p">,</span>
        <span class="p">)</span>

<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">AOTConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for AOTDispatcher</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">aot_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">is_export</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">no_tangents</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">aot_autograd_arg_pos_to_source</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Source</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">enable_log</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># This function takes in a tensor t, and returns one of t, t.view(), or t.clone().</span>
<span class="c1"># When tracing the joint forward + backward, for any inputs in the graph that are mutated,</span>
<span class="c1"># we need to clone them first (and similarly for metadata-only mutations, we need to view them first).</span>
<span class="c1"># The idea is that when we trace the backward, we need to pass in the *original* primals</span>
<span class="c1"># to autograd.grad(), before they were mutated.</span>
<span class="c1"># Note: when we have synthetic base inputs, we need to clone them *before* creating views off of them.</span>
<span class="c1"># This means that &quot;idx&quot; here represents the index of the (potentially) synthetic base.</span>
<span class="c1"># What we need to do is:</span>
<span class="c1"># (1) map the current (post-synthetic-base calling convention) input argument index</span>
<span class="c1">#     to int index pre-synthetic-base-calling-convention.</span>
<span class="c1"># (2) There could be multiple, if this index corresponds to a synthetic base</span>
<span class="c1">#     that has multiple input aliases.</span>
<span class="c1"># (3) If any of those corresponding inputs get metadata mutations, then we clone the base.</span>
<span class="k">def</span> <span class="nf">maybe_to_fresh_input</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">meta</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">:</span>
        <span class="c1"># We only need to bother cloning mutated inputs that participate in autograd.</span>
        <span class="n">mutated_inp_idx</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
            <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
            <span class="c1"># sees the tensor before the mutation</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
            <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
            <span class="c1"># sees the tensor before the metadata mutation</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span>

<span class="c1"># This function returns a new function that returns mutated inputs as outputs.</span>
<span class="c1"># if keep_data_input_mutations is set, then we assume that data-only mutations</span>
<span class="c1"># will be left in the graph, and we only return metadata-mutated inputs as outputs.</span>
<span class="k">def</span> <span class="nf">fn_input_mutations_to_outputs</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">keep_data_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="c1"># The compiled fw will return mutated input tensors, *including* metadata-only mutation.</span>
        <span class="c1"># However, if keep_data_input_mutations is set, the compiled fw only needs to return metadata-mutated inputs.</span>
        <span class="c1"># (because data-only input mutations are handled directly in the compiled graph)</span>
        <span class="n">mutated_inputs_to_return</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">keep_data_input_mutations</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="o">*</span><span class="n">mutated_inputs_to_return</span><span class="p">,</span> <span class="o">*</span><span class="n">outs</span>
    <span class="k">return</span> <span class="n">inner_fn</span>

<span class="c1"># This function takes in a fn with external aliasing and mutation,</span>
<span class="c1"># and returns a new fn with no external aliasing and mutation,</span>
<span class="c1"># as needed for autograd.</span>
<span class="c1"># The main transformations are:</span>
<span class="c1"># - Return mutated inputs as extra outputs</span>
<span class="c1"># - Clone mutated inputs that require gradients,</span>
<span class="c1">#   because autograd will require us to pass the pre-mutated inputs into autograd.grad</span>
<span class="c1"># - Return intermediate bases of outputs as additional outputs,</span>
<span class="c1">#   needed to appease autograd.Function</span>
<span class="c1"># The new function returns:</span>
<span class="c1"># (1) The updated outputs</span>
<span class="c1"># (2) A boolean mask of len(new_fn_outputs),</span>
<span class="c1">#     that can be used to tell autograd.grad which outputs should get tangents</span>
<span class="c1">#     if we trace the backward.</span>
<span class="k">def</span> <span class="nf">fn_prepped_for_autograd</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">args_maybe_cloned</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">maybe_to_fresh_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args_maybe_cloned</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>

        <span class="n">mutated_inputs_to_return</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args_maybe_cloned</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="p">]</span>

        <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">:</span>
                <span class="n">intermediate_bases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">meta</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">)</span>

        <span class="c1"># the compiled forward should return (mutated_inputs, user_outs, intermediate_bases)</span>
        <span class="n">fw_outs_to_return</span> <span class="o">=</span> <span class="o">*</span><span class="n">mutated_inputs_to_return</span><span class="p">,</span> <span class="o">*</span><span class="n">outs</span><span class="p">,</span> <span class="o">*</span><span class="n">intermediate_bases</span>

        <span class="c1"># Also return a boolean mask specifying which outputs to this function will be used as tangents</span>
        <span class="n">mutated_inputs_grad_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mutated_inputs_to_return</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Pass any (non-aliased) outputs in as tangents, since they&#39;ll be returned as outputs in the fw</span>
        <span class="c1"># For outputs that are aliases of intermediates, we will have returned the output&#39;s _base as an output in the graph instead,</span>
        <span class="c1"># which we *should* send to grad()</span>
        <span class="n">output_grad_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span><span class="p">]</span>
            <span class="c1"># Also, only tensor outputs should participate in the backward</span>
            <span class="c1"># (in particular, Symint outputs in the forward graph shouldn&#39;t get tangents)</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">intermediate_base_grad_mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">))]</span>

        <span class="n">out_grad_mask</span> <span class="o">=</span> <span class="n">mutated_inputs_grad_mask</span> <span class="o">+</span> <span class="n">output_grad_mask</span> <span class="o">+</span> <span class="n">intermediate_base_grad_mask</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_grad_mask</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_to_return</span><span class="p">)</span>

        <span class="c1"># Take care to grab and sync the updated inputs from primals_after_cloning (the inputs we actually mutate!)</span>
        <span class="c1"># and not primals (the preserved inputs, pre-mutation, that we pass to grad())</span>
        <span class="c1"># This is annoying: our joint function needs to be aware of functionalization</span>
        <span class="c1"># (syncing mutated inputs before calling autograd.grad())</span>
        <span class="c1"># In theory, we could make the autograd engine do this automatically, although that probably isn&#39;t any cleaner.</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args_maybe_cloned</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">sync_functional_tensor</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fw_outs_to_return</span><span class="p">,</span> <span class="n">out_grad_mask</span>
    <span class="k">return</span> <span class="n">inner_fn</span>

<span class="c1"># Given a fn, computes the joint.</span>
<span class="c1"># NOTE: fn is expects the following behavior:</span>
<span class="c1"># (1) fn() needs to return a tuple of (outs, mask),</span>
<span class="c1">#     where `mask` tells us which outputs are meant to have tangents.</span>
<span class="c1">#     we don&#39;t know this info automatically, because we don&#39;t actually want to blindly</span>
<span class="c1">#     compute tangents for every output that requires grad.</span>
<span class="c1">#     Specifically, outputs that alias inputs won&#39;t participate in the backward and get tangents.</span>
<span class="c1"># (2) fn() cannot mutate any inputs that require gradient.</span>
<span class="c1">#     otherwise, when we compute autograd.grad(), we will not take those input mutations into account</span>
<span class="c1">#     (the way this is handled is that we ensure any inputs that normally get mutated are cloned first)</span>
<span class="k">def</span> <span class="nf">create_joint</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]):</span>
        <span class="n">outs</span><span class="p">,</span> <span class="n">tangent_mask</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangent_mask</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="n">outs_to_grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span> <span class="k">for</span> <span class="n">needs_tangent</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tangent_mask</span><span class="p">,</span> <span class="n">outs</span><span class="p">)</span> <span class="k">if</span> <span class="n">needs_tangent</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs_to_grad</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span>

        <span class="c1"># Get the inputs that need gradients</span>
        <span class="n">grad_primals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">inputs_needs_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Note that we&#39;re not using primals here,</span>
        <span class="c1"># being carefully not to pass any mutated inputs into autograd.grad()</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primals</span><span class="p">:</span>
            <span class="n">is_grad_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">inputs_needs_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_grad_tensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_grad_tensor</span><span class="p">:</span>
                <span class="n">grad_primals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># Get the outputs that need gradients</span>
        <span class="n">needed_outs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">needed_tangents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outs_to_grad</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="c1"># A bit sketchy, but fixes e.g. test_aot_autograd_exhaustive_matmul_cpu_float32</span>
                <span class="c1"># The issue is that we are sensitive to decomps that don&#39;t accurately maintain</span>
                <span class="c1"># their output&#39;s _base.shape compared to eager mode, and this helps mitigate a bit.</span>
                <span class="n">needed_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">out</span> <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">tangent</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tangent</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">needed_tangents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent</span><span class="p">)</span>

        <span class="n">setup_stacktrace_preservation_hooks</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">grad_fn</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">needed_outs</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
            <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">mark_beginning_of_backward</span><span class="p">()</span>
        <span class="n">backward_out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Call the backwards pass</span>
        <span class="k">if</span> <span class="n">grad_primals</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">():</span>
                <span class="c1"># for full graph export, we always export a joint graph where we assume no tangents are needed.</span>
                <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">no_tangents</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">needed_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">needed_tangents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
                    <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                        <span class="n">needed_outs</span><span class="p">,</span>
                        <span class="n">grad_primals</span><span class="p">,</span>
                        <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                        <span class="n">needed_outs</span><span class="p">,</span>
                        <span class="n">grad_primals</span><span class="p">,</span>
                        <span class="n">grad_outputs</span><span class="o">=</span><span class="n">needed_tangents</span><span class="p">,</span>
                        <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">)</span>
        <span class="n">backward_out_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backward_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">,</span> <span class="p">[</span>
            <span class="nb">next</span><span class="p">(</span><span class="n">backward_out_iter</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs_needs_grads</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">inner_fn_with_anomaly</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">(),</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;Anomaly Detection has been enabled.&quot;</span>
            <span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">(</span><span class="n">check_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">inner_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner_fn_with_anomaly</span>

<span class="c1"># This creates the final function that we want to trace using make_fx(),</span>
<span class="c1"># in both aot_dispatch_autograd and aot_dispatch_base.</span>
<span class="c1"># Preconditions:</span>
<span class="c1"># - fn corresponds to the user&#39;s fw function</span>
<span class="c1"># - fn arguments have been flattened, duplicate arguments have been handled</span>
<span class="c1"># - In the returned function, the &quot;primals&quot; arguments *includes* synthetic bases.</span>
<span class="c1"># This function does the work of functionalizing the input function,</span>
<span class="c1"># and performing copy_() calls at the end of the function if `keep_input_mutations` is set.</span>
<span class="c1"># The function returned has signature that is either:</span>
<span class="c1"># (1) &quot;traced_fn(primals: List[Any])&quot; if trace_joint is False</span>
<span class="c1"># (2) &quot;traced_fn(primals: List[Any], tangents: List[Any])&quot; if trace_joint is True</span>
<span class="c1"># Returns a new (functionalized) function, and updated arguments to call it with.</span>
<span class="k">def</span> <span class="nf">create_functionalized_fn</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]:</span>
    <span class="k">def</span> <span class="nf">functionalized_f_helper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Wrap inputs into functional wrappers</span>
        <span class="n">f_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

        <span class="c1"># See Note [Disabling Functionalize TLS Above Python Functionalization]</span>
        <span class="n">disable_above</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_ExcludeDispatchKeyGuard</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKeySet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKey</span><span class="o">.</span><span class="n">Functionalize</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">disable_above</span><span class="p">,</span> <span class="n">FunctionalTensorMode</span><span class="p">():</span>
            <span class="c1"># Run the joint</span>
            <span class="n">f_outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">f_args</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">trace_joint</span><span class="p">:</span>
            <span class="c1"># Note: This is a bit annoying. There&#39;s a layering issue here, where:</span>
            <span class="c1"># (1) functionalization needs to operate on **synthetic base** inputs, before unpacking them into the &quot;real&quot; inputs.</span>
            <span class="c1"># (2) For keep_input_mutations, we support tracing a call to copy_() directly on mutated inputs.</span>
            <span class="c1">#     However, we **only** want to support this for inputs that have data-only (and no metadata) mutations,</span>
            <span class="c1">#     because inductor (and backends in generally) would prefer not to see these (e.g. as_strided_(), resize_()).</span>
            <span class="c1">#     This makes it pretty difficult for this logic to operate on synthetic bases.</span>
            <span class="c1"># (3) In addition, there are cases where it&#39;s significantly cheaper to perform the copy on the individual</span>
            <span class="c1">#     (unpacked) input aliases, instead of the synthetic base.</span>
            <span class="c1"># Example case where (3) could be important:</span>
            <span class="c1">#</span>
            <span class="c1">#     def f(x, y):</span>
            <span class="c1">#         x.mul_(2)</span>
            <span class="c1">#         y.mul_(3)</span>
            <span class="c1">#         return x, y</span>
            <span class="c1">#    a = torch.ones(1&#39;000&#39;000)</span>
            <span class="c1">#    x, y = out(a[0:9], a[1:10])</span>
            <span class="c1">#</span>
            <span class="c1"># It would be much better to add copy_() calls into the graph for the two tiny slices, instead of materializing</span>
            <span class="c1"># a giant &quot;updated synthetic base&quot; and copying into a&#39;s entire storage.</span>
            <span class="c1">#</span>
            <span class="c1"># For now, we are pessimistically not performing the optimization from (3);</span>
            <span class="c1"># we will materialize an &quot;updated&quot; synthetic base, and copy it back to the synthetic input base.</span>
            <span class="c1"># This allows us to factor aot autograd much more nicely, since only one area of the code needs to worry</span>
            <span class="c1"># about synthetic bases.</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inpt_old</span><span class="p">,</span> <span class="n">inpt_f</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">f_args</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="k">assert</span> <span class="n">is_fun</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">)</span>
                <span class="n">inpt_new</span> <span class="o">=</span> <span class="n">from_fun</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                    <span class="c1"># We found an input that had a (data-only) mutation.</span>
                    <span class="c1"># Since keep_input_mutations is set, we need to faithfully apply a copy_()</span>
                    <span class="c1"># so the compiler will see the input mutation in the graph.</span>
                    <span class="k">assert</span> <span class="n">inpt_new</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">inpt_old</span>
                    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutations_hidden_from_autograd</span><span class="p">:</span>
                        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_unsafe_preserve_version_counter</span><span class="p">(</span><span class="n">inpt_old</span><span class="p">):</span>
                            <span class="n">inpt_old</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inpt_new</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">inpt_old</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inpt_new</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_outs</span><span class="p">)</span>

    <span class="c1"># Kinda annoying, but needed to make sure that the fx graph we trace out has &quot;primals&quot;</span>
    <span class="c1"># and &quot;tangents&quot; as its input names (which are special-cased by the partitioner)</span>
    <span class="k">def</span> <span class="nf">joint_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fwd_helper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">helper</span> <span class="o">=</span> <span class="n">joint_helper</span> <span class="k">if</span> <span class="n">trace_joint</span> <span class="k">else</span> <span class="n">fwd_helper</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
        <span class="c1"># Setup the wrapper for functionalization of rng ops</span>
        <span class="n">helper</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">create_functionalized_rng_ops_wrapper</span><span class="p">(</span><span class="n">helper</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">trace_joint</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">helper</span><span class="p">,</span> <span class="n">args</span>

<span class="k">def</span> <span class="nf">create_graph</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">decomposition_table</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fx_g</span>


<span class="k">def</span> <span class="nf">normalize_as_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>


<span class="n">aot_autograd_decompositions</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># This is a list since looking forward, we can have this arbitrarily nested.</span>
<span class="n">graph_being_compiled</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># TODO: It would be nice to reset the numbering every time aot_id goes</span>
<span class="c1"># up, but this is annoying to do right now (because we don&#39;t know if</span>
<span class="c1"># an aot_id will come back from the dead), so right now this also happens</span>
<span class="c1"># to be a globally unique number too (at the cost of wobbling if you change</span>
<span class="c1"># how the graphs compile)</span>
<span class="n">nth_graph</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>


<span class="k">def</span> <span class="nf">set_model_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">model_name</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">get_aot_compilation_context</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">),</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">nth_graph</span>


<span class="k">def</span> <span class="nf">get_aot_graph_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the name of the graph being compiled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">graph_being_compiled</span><span class="p">,</span> <span class="n">nth_graph</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">__</span><span class="si">{</span><span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">nth_graph</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="n">get_graph_being_compiled</span> <span class="o">=</span> <span class="n">get_aot_graph_name</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">graph_being_compiled</span>
    <span class="c1"># TODO: Don&#39;t shove the aot_id in here; set it in the context</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">global</span> <span class="n">nth_graph</span>
        <span class="n">nth_graph</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">make_boxed_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">make_boxed_compiler</span><span class="p">(</span><span class="n">compiler</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiler</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">):</span>
        <span class="n">out_f</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fx_g</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">call_func_with_args</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">steal_args</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">steal_args</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>

    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
    <span class="k">with</span> <span class="n">context</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Please remove soon</span>
            <span class="c1"># https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Your compiler for AOTAutograd is returning a function that doesn&#39;t take boxed arguments. &quot;</span>
                <span class="s2">&quot;Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. &quot;</span>
                <span class="s2">&quot;See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.&quot;</span>
            <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">aot_dispatch_base_graph</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SubclassMeta</span><span class="p">]]:</span>
    <span class="c1"># aot_dispatch_base requires functionalization, but doesn&#39;t need to handle as many cases as the autograd case.</span>
    <span class="c1"># The cases that aot_dispatch_base doesn&#39;t need to handle include:</span>
    <span class="c1"># - outputs that are aliases of graph intermediates</span>
    <span class="c1"># - outputs that are aliases of graph inputs</span>
    <span class="c1"># While cases that it does need to handle include:</span>
    <span class="c1"># - input mutations (including when inputs are aliases of each other)</span>
    <span class="c1"># - input metadata mutations</span>
    <span class="n">fn_to_trace</span> <span class="o">=</span> <span class="n">fn_input_mutations_to_outputs</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">keep_data_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">fn_to_trace</span><span class="p">,</span> <span class="n">updated_flat_args</span> <span class="o">=</span> <span class="n">create_functionalized_fn</span><span class="p">(</span>
        <span class="n">fn_to_trace</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">fn_to_trace</span><span class="p">,</span> <span class="n">updated_flat_args_subclasses_desugared</span><span class="p">,</span> <span class="n">maybe_subclass_meta</span> <span class="o">=</span> <span class="n">aot_dispatch_subclass</span><span class="p">(</span>
        <span class="n">fn_to_trace</span><span class="p">,</span> <span class="n">updated_flat_args</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">fw_only</span><span class="o">=</span><span class="n">flat_fn</span><span class="p">)</span>

    <span class="n">fw_module</span> <span class="o">=</span> <span class="n">create_graph</span><span class="p">(</span>
        <span class="n">fn_to_trace</span><span class="p">,</span>
        <span class="n">updated_flat_args_subclasses_desugared</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># As long as we opted to remove input mutations, then</span>
    <span class="c1"># there should be *NO* mutating ops in the graph at this point.</span>
    <span class="n">copy_count</span> <span class="o">=</span> <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">allow_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">)</span>

    <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
    <span class="n">fw_module</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

    <span class="n">copy_count2</span> <span class="o">=</span> <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">allow_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">copy_count</span> <span class="o">==</span> <span class="n">copy_count2</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">enable_log</span><span class="p">:</span>
        <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Forward graph&quot;</span><span class="p">,</span> <span class="n">fw_module</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>

    <span class="c1"># TODO: should factor this into a separate function for export that always only returns just the graph.</span>
    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;aot_export_module does not support tensor subclass inputs for now.&quot;</span>
        <span class="k">return</span> <span class="n">fw_module</span>
    <span class="k">return</span> <span class="n">fw_module</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">updated_flat_args_subclasses_desugared</span><span class="p">),</span> <span class="n">maybe_subclass_meta</span>

<span class="k">def</span> <span class="nf">aot_dispatch_base</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
    <span class="n">fw_module</span><span class="p">,</span> <span class="n">updated_flat_args</span><span class="p">,</span> <span class="n">maybe_subclass_meta</span> <span class="o">=</span> <span class="n">aot_dispatch_base_graph</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>

    <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;inference&quot;</span><span class="p">):</span>
        <span class="n">compiler</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">inference_compiler</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">inference_compiler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
            <span class="c1"># Add the seed and offset as example inputs to pass to the compiler</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">()</span>
            <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
            <span class="n">updated_flat_args</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span><span class="o">.</span><span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">fw_metadata</span> \
                <span class="k">if</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">maybe_subclass_meta</span><span class="o">.</span><span class="n">fw_metadata</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fw_module</span><span class="p">,</span> <span class="n">updated_flat_args</span><span class="p">)</span>

    <span class="c1"># This boxed_call handling happens inside create_runtime_wrapper as well.</span>
    <span class="c1"># However, create_runtime_wrapper does not expect the rng offsets in the</span>
    <span class="c1"># output. So, we have to create another wrapper and take out the offset. As</span>
    <span class="c1"># a result, we have to account for not boxed_call compilers as well.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">)</span>

    <span class="c1"># Create a wrapper to set up the rng functionalize bits</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">rng_functionalization_wrapper</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># args is a list because compiled_fw is boxed_call</span>
        <span class="k">if</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
            <span class="c1"># Add the seed and offset to args</span>
            <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">()</span>
            <span class="n">args</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">])</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">compiled_fw</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">compiled_fw</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_dispatch_subclass_wrapper</span><span class="p">(</span>
            <span class="n">rng_functionalization_wrapper</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_fw_graph_out_meta</span><span class="p">,</span> <span class="n">num_fw_outs_saved_for_bw</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">rng_functionalization_wrapper</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fw_func</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fw_func</span><span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_runtime_wrapper</span><span class="p">(</span>
        <span class="n">compiled_fw_func</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">indices_of_inps_to_detach</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="c1"># Returns the number of detected copy_</span>
<span class="k">def</span> <span class="nf">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">allow_input_mutations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">placeholders</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">copy_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># NB: It would also be nice to verify that the mutations all happen at the</span>
    <span class="c1"># end, but we also do some administrative views after mutations so this</span>
    <span class="c1"># isn&#39;t actually true.  (TODO: Could this cause problems for Inductor?)</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
            <span class="n">placeholders</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">copy_</span><span class="o">.</span><span class="n">default</span> <span class="ow">and</span> <span class="n">allow_input_mutations</span><span class="p">:</span>
                <span class="n">suffix</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># Can only copy_ into an input, and can only do so once</span>
                <span class="k">assert</span> <span class="n">n</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">placeholders</span>
                <span class="n">placeholders</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">copy_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">,</span> \
                    <span class="sa">f</span><span class="s1">&#39;aot_autograd expected to have an entirely functional graph, but found </span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">format_node</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="k">return</span> <span class="n">copy_count</span>


<span class="k">def</span> <span class="nf">are_differentiable_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">or</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span> <span class="ow">or</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">same_dtype_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>



<span class="c1"># Assumption: x and y are known to share a storage, and we are trying to determine</span>
<span class="c1"># if their memory is actually completely disjoint, based on sizes/strides/storage_offset</span>
<span class="k">def</span> <span class="nf">tensors_definitely_do_not_overlap</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">y</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># Make x always on the left</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">():</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span>
    <span class="c1"># Short-circuit in the &quot;obvious&quot; overlapping case: both tensors are contiguous</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">+</span> <span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">():</span>
            <span class="c1"># definitely overlap</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># definitely no overlap</span>
            <span class="k">return</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># This cases is needed for the shampoo optimizer.</span>
        <span class="c1"># All tensors are 2d (non-contiguous), have the same outer stride, and have an inner stride of 1</span>
        <span class="c1"># (so rows are contiguous)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">offset_delta</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">offset_delta</span> <span class="o">&lt;</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
                <span class="c1"># definitely overlaps (row 0 of y overlaps with row 0 of x)</span>
                <span class="c1"># Example:</span>
                <span class="c1">#   base = torch.arange(32).reshape(4, 8)</span>
                <span class="c1">#   x = base.narrow(1, 0, 4)</span>
                <span class="c1">#     x: size=(4, 4), stride=(8, 1), offset=0</span>
                <span class="c1">#   y = base.narrow(1, 3, 4)</span>
                <span class="c1">#     y: size=(4, 4), stride=(8, 1), offset=3</span>
                <span class="k">return</span> <span class="kc">False</span>
            <span class="n">x_total_elems_covered</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">x_total_elems_covered</span> <span class="o">&lt;=</span> <span class="n">offset_delta</span><span class="p">:</span>
                <span class="c1"># definitely does not overlap (last byte of x is before start of y)</span>
                <span class="c1"># Example:</span>
                <span class="c1">#   x: size=(4, 4), stride=(8, 1), offset=0 (last byte is 27)</span>
                <span class="c1">#   y: size=(4, 4), stride=(8, 1), offset=28 (start byte is 28)</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="c1"># At this point, we want to check if the 0th row of y</span>
            <span class="c1"># overlaps with **some** row of x.</span>
            <span class="c1"># We can check this by shifting y backward by the shared stride, repeatedly,</span>
            <span class="c1"># until the first row of y is before the first row of x.</span>
            <span class="c1"># Then we can check if these rows overlap.</span>
            <span class="c1"># We can accomplish this by modding our offset by the stride.</span>
            <span class="n">offset_delta_mod</span> <span class="o">=</span> <span class="n">offset_delta</span> <span class="o">%</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Example:</span>
            <span class="c1"># 0 1 2 3</span>
            <span class="c1"># 9 10 11 12</span>
            <span class="c1"># 18 19 20 21</span>
            <span class="c1"># 27 28 29 30</span>
            <span class="c1">#   x: size=(4, 4), stride=(9, 1), offset=0</span>
            <span class="c1">#   y: size=(4, 4), stride=(9, 1), offset=22 (this would not overlap)</span>
            <span class="c1">#   y: size=(4, 4), stride=(9, 1), offset=23 (this would not overlap)</span>
            <span class="c1">#   y: size=(4, 4), stride=(9, 1), offset=24 (this would overlap)</span>
            <span class="c1">#   y: size=(4, 4), stride=(9, 1), offset=25 (this would overlap)</span>
            <span class="c1"># If the interval [modded_offset, modded_offset + x_size] falls entirely</span>
            <span class="c1"># without</span>
            <span class="k">if</span> <span class="n">offset_delta_mod</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">compute_overlapping_inputs</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">,</span> <span class="n">aliased_input_indices</span><span class="p">):</span>
    <span class="n">actual_aliased_indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="p">):</span>
            <span class="n">i_</span> <span class="o">=</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">j_</span> <span class="o">=</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">tensors_definitely_do_not_overlap</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i_</span><span class="p">],</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">j_</span><span class="p">]):</span>
                <span class="n">actual_aliased_indices</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">i_</span><span class="p">)</span>
                <span class="n">actual_aliased_indices</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">j_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">actual_aliased_indices</span>

<span class="c1"># Note [Handling mutations on an input that aliases other inputs]</span>
<span class="c1"># The easiest example to show-case this edge case is here:</span>
<span class="c1">#</span>
<span class="c1"># def f(a, b):</span>
<span class="c1">#     a.mul_(2)</span>
<span class="c1">#     out = a + b</span>
<span class="c1">#     return out</span>
<span class="c1"># b = torch.ones(...)</span>
<span class="c1"># a = b.view(-1)</span>
<span class="c1"># f(a, b)</span>
<span class="c1">#</span>
<span class="c1"># In this situation, if a and b happened to be aliased, we need to trace something different!</span>
<span class="c1"># Suppose we had b = a.view(-1)</span>
<span class="c1"># (In this case, that means that `a._base is b`)</span>
<span class="c1">#</span>
<span class="c1"># We need to ensure that the aliasing relationship between a and b is preserved.</span>
<span class="c1"># We do that detecting the specific situation above (mutate an input that aliases another input),</span>
<span class="c1"># and when we do that, we create a synthetic base argument. Then inside of the traced forward,</span>
<span class="c1"># we regenerate a and b off of that base.</span>
<span class="c1"># The complete example of the transformed function looks like this:</span>
<span class="c1">#</span>
<span class="c1"># // The traced forward takes in a synthetic base, and regenerates the aliased inputs as views</span>
<span class="c1"># // We could consider getting view-replay support here to minimize as_strided_scatter ops in the graph</span>
<span class="c1"># def traced_forward(base):</span>
<span class="c1">#     a = base.as_strided(...)</span>
<span class="c1">#     b = base.as_strided(...)</span>
<span class="c1">#     a_updated = a.mul(2)</span>
<span class="c1">#     base_updated = torch.as_strided_scatter(base, a_updated, ...)</span>
<span class="c1">#     b_updated = base_updated.as_strided(...)</span>
<span class="c1">#     out = a_updated + b_updated</span>
<span class="c1">#     return a_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_fn(a, b):</span>
<span class="c1">#     // we detect that a is the &quot;differentiable base&quot; here</span>
<span class="c1">#     base = a</span>
<span class="c1">#     // In other situations, we might do either:</span>
<span class="c1">#     // (1) a and b are both views off of some larger differentiable base</span>
<span class="c1">#     //     assert a._base is b._base and a._base is not None</span>
<span class="c1">#     //     base = a._base</span>
<span class="c1">#     // (2) a and b both don&#39;t require gradients. Create a base from the storage</span>
<span class="c1">#     //     assert a._base is None and b._base is None</span>
<span class="c1">#     //     base = torch.Tensor(a.storage())</span>
<span class="c1">#     a_updated, out = traced_forward(base)</span>
<span class="c1">#     a.copy_(a_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># This function:</span>
<span class="c1"># (1) Merges input views into a synthetic base argument, when any of those input views are mutated</span>
<span class="c1"># (2) Returns metadata telling the autograd.Function how to modify their arguments properly,</span>
<span class="c1">#     to respect the new calling convention.</span>
<span class="c1">#</span>
<span class="c1"># The calling convention is as follows.</span>
<span class="c1"># Any inputs that were originally views of one another get yanked, and replaced with a synthetic base.</span>
<span class="c1"># The argument list ordering goes [base1, ..., baseN], [arg1, ..., argN],</span>
<span class="c1"># Where the ordering of the bases is determined from the ordering of the original view args.</span>
<span class="c1"># baseA will come before baseB if the earliest original argument coming from baseA</span>
<span class="c1"># showed up earlier in the argument list than the earliest original argument coming from baseB.</span>
<span class="c1">#</span>
<span class="c1"># Example, given some tensors a, b, c, d</span>
<span class="c1"># call site:</span>
<span class="c1">#   f(a, c.view(-1), b.view(-1), b, c, d)</span>
<span class="c1"># Modified argument list:</span>
<span class="c1">#   c_base comes first because the first c view came earlier in arg list than the first b view</span>
<span class="c1">#   a and d still show up in the modified arg list, but b and c don&#39;t- they&#39;re regenerated from their bases</span>
<span class="c1">#   b_base = torch.Tensor(b.storage())</span>
<span class="c1">#   c_base = torch.Tensor(c.storage())</span>
<span class="c1">#   f(c_base, b_base, a, d)</span>
<span class="k">def</span> <span class="nf">merge_view_inputs</span><span class="p">(</span>
    <span class="n">fwd_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">mutated_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># The autograd case currently has more restrictions than the inference case.</span>
    <span class="n">is_inference</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]]:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_input_info</span><span class="p">)</span>
    <span class="n">storage_ref_to_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">base_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">other_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">storage_ref</span> <span class="o">=</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
            <span class="n">storage_ref_to_idx</span><span class="p">[</span><span class="n">storage_ref</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span>
    <span class="c1"># Note [Synthetic Base Info Metadata]</span>
    <span class="c1"># This list contains metadata that tells you what the i&#39;th argument in the inner calling convention should be.</span>
    <span class="c1"># It&#39;s either:</span>
    <span class="c1"># - another int (corresponding to the index in the argument list of the element from the outer calling convention)</span>
    <span class="c1"># - idx, view_tensor, where we can generate the new output with view_tensor._view_func(old_args[idx])</span>
    <span class="c1">#   idx corresponds to which synthetic base from the outer calling context to view</span>
    <span class="n">inner_calling_convention_meta</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">aliased_input_indices</span> <span class="ow">in</span> <span class="n">storage_ref_to_idx</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
            <span class="c1"># We only care about mutations that affect all aliases,</span>
            <span class="c1"># so metadata mutations on an input doesn&#39;t require us to do synthetic base handling.</span>
            <span class="n">mutated_input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
            <span class="k">for</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
        <span class="p">):</span>
            <span class="k">for</span> <span class="n">curr_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
                <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">])</span>
            <span class="k">continue</span>

        <span class="c1"># Here, we attempt to do a more complicated check to detect false aliasing</span>
        <span class="c1"># (e.g. if all the tensors have the same storage, but don&#39;t actually overlap)</span>
        <span class="c1"># In theory, we could have a large group of tensors that all share storages, where only *some* of them</span>
        <span class="c1"># have overlapping memory.</span>
        <span class="c1"># I don&#39;t bother with that case for now: here, we only bail out earlier if we detect that **every** pair</span>
        <span class="c1"># of tensors in the current group that shares a storage is non-overlapping.</span>
        <span class="n">aliased_input_indices_no_false_sharing</span> <span class="o">=</span> <span class="n">compute_overlapping_inputs</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">,</span> <span class="n">aliased_input_indices</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_input_indices_no_false_sharing</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">curr_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
                <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">])</span>
            <span class="k">continue</span>

        <span class="c1"># We detected an input that was mutated, AND aliases with another input.</span>
        <span class="c1"># we need to replace this set of aliased inputs with a single synthetic base.</span>
        <span class="c1"># For now, I&#39;m banning a bunch of cases. We expect dynamo to properly detect these cases</span>
        <span class="c1"># and error out. We can fix them later.</span>
        <span class="c1"># These checks are transitive, so we don&#39;t need to check every pair.</span>
        <span class="k">for</span> <span class="n">idx1</span><span class="p">,</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">,</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">view1</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx1</span><span class="p">]</span>
            <span class="n">view2</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span>
            <span class="c1"># The &quot;inputs that are aliased but have different differentiable bases&quot; case</span>
            <span class="c1"># is more complicated and hopefully pretty rare. Not currently handled.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_inference</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">are_differentiable_views</span><span class="p">(</span>
                    <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="c1"># Regenerating views when reinterpreting complex / real tensors seems non-trivial,</span>
            <span class="c1"># not handling for now</span>
            <span class="k">assert</span> <span class="n">same_dtype_views</span><span class="p">(</span>
                <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
            <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle input mutations on views with different dtypes.&quot;</span>
        <span class="n">non_none_bases</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
            <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="n">aliases_with_none_bases</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span> <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_none_bases</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Case where none of the aliases have a ._base</span>
            <span class="c1"># we generate a synthetic base without gradients, and generate views off of it</span>
            <span class="c1"># We hit this case when we have input tensors to the graph that share a storage,</span>
            <span class="c1"># but do not have a ._base field.</span>
            <span class="c1"># Wondering when we hit this case?</span>
            <span class="c1"># The _base field simply says that autograd knows about the aliasing relationship,</span>
            <span class="c1"># but sometimes we create tensors which are aliased out of the same storage but guaranteed</span>
            <span class="c1"># to be disjoint. In these cases, we will skip setting up the _base relationship</span>
            <span class="c1"># for performance reasons (because the fact that the tensors share the same storage</span>
            <span class="c1"># is unobservable unless you (1) do naughty things with resize_/as_strided</span>
            <span class="c1"># or (2) look at the storage--as we are doing here.)</span>
            <span class="c1"># One particular example of this is optimizer steps on the LSTM module:</span>
            <span class="c1"># LSTM parameters are packed into a contiguous storage for efficiency reasons when</span>
            <span class="c1"># calling cuDNN kernels, so when these parameters get passed to the optimizer we will</span>
            <span class="c1"># find they share the same storage, but do not have _base set since they are all disjoint.</span>
            <span class="c1">#</span>
            <span class="c1"># NOTE: There is one case where this is unsafe:</span>
            <span class="c1"># torch.Tensor(storage) will ALWAYS create a 1D tensor, which is not necessarily</span>
            <span class="c1"># the same shape as the &quot;actual&quot; base that the tensor came from.</span>
            <span class="c1"># For the most part this is fine, because we always use as_strided()</span>
            <span class="c1"># to generate the original aliased inputs again.</span>
            <span class="c1"># If we were to use view-replay though, this could cause the aliased views</span>
            <span class="c1"># to have incorrect sizes.</span>
            <span class="n">example_idx</span> <span class="o">=</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">example_alias</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span>
            <span class="c1"># Note that this function is re-used at both trace time and runtime.</span>
            <span class="c1"># At trace time, we&#39;re under a FakeMode so synthetic_base becomes a FakeTensor.</span>
            <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">example_alias</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">example_alias</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># We don&#39;t actually have a convenient way of going from storage -&gt; tensor,</span>
            <span class="c1"># So using set_() here (we suffer some minor overhead, but this case is rare).</span>
            <span class="n">synthetic_base</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">example_alias</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Case where all of the aliases require gradients, and have the same _base.</span>
            <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">other_base</span> <span class="ow">in</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">other_base</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="k">for</span> <span class="n">alias</span> <span class="ow">in</span> <span class="n">aliases_with_none_bases</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">alias</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
        <span class="n">base_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">synthetic_base</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">curr_view_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
            <span class="n">curr_view</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span>
            <span class="n">base_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="c1"># We store just enough info here so that we can regenerate the view later.</span>
            <span class="c1"># Regeneration: curr_view._view_func(args[base_idx])</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">base_idx</span><span class="p">,</span> <span class="n">curr_view</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">other_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span>
        <span class="c1"># If no synthetic bases are necessary, just return the original inputs.</span>
        <span class="k">return</span> <span class="n">fwd_inputs</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, return:</span>
        <span class="c1"># (1) The new args according to the updated calling convention: (synthetic_bases, other_args)</span>
        <span class="c1"># (2) Metadata telling functionalization how to generate the inner argument list given the outer calling convention.</span>
        <span class="c1">#     We post-process it into a list, where meta[i] tells you info about the i&#39;th argument in the inner calling convention.</span>
        <span class="n">args_to_functionalization</span> <span class="o">=</span> <span class="n">base_args</span> <span class="o">+</span> <span class="n">other_args</span>
        <span class="n">arg_to_old_idx_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">other_arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">other_args</span><span class="p">):</span>
            <span class="n">new_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span>
            <span class="n">old_idx</span> <span class="o">=</span> <span class="n">arg_to_old_idx_map</span><span class="p">[</span><span class="n">other_arg</span><span class="p">]</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">old_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_idx</span>
        <span class="c1"># post process into a list</span>
        <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_calling_convention_meta</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inner_calling_convention_meta</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">post_processed_calling_convention_meta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># Quick assert: every argument in the inner calling convention should be accounted for.</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">args_to_functionalization</span><span class="p">,</span> <span class="n">post_processed_calling_convention_meta</span>


<span class="k">def</span> <span class="nf">format_guard_bug_msg</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;At compilation time, graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> was compiled under the &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;assumption that </span><span class="si">{</span><span class="n">expected</span><span class="si">}</span><span class="s2">, but at runtime this was not the case.  &quot;</span>
        <span class="s2">&quot;This indicates a guard bug in AOTAutograd or Dynamo, please file a bug to PyTorch.&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">remove_dupe_metadata</span><span class="p">(</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">keep_arg_mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="n">add_dupe_map</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_arg_mask</span><span class="p">)</span>
    <span class="c1"># Easy invariant: the first argument should never be a dupe (it will be kept)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_arg_mask</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Filter dupe&#39;d mutated inputs out of traced_tangents</span>
    <span class="n">num_data_mutations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">])</span>
    <span class="n">other_traced_tangents</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">[</span><span class="n">num_data_mutations</span><span class="p">:]</span>
    <span class="n">inp_traced_tangents</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">[:</span><span class="n">num_data_mutations</span><span class="p">]</span>
    <span class="n">filtered_inp_traced_tangents</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inp_traced_tangents</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]]</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">filtered_inp_traced_tangents</span> <span class="o">+</span> <span class="n">other_traced_tangents</span>

    <span class="k">return</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
        <span class="n">input_info</span><span class="o">=</span><span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span>
        <span class="c1"># For outputs that are views of inputs, we store the index of the input that the output</span>
        <span class="c1"># was generated from. Need to update that index to account for removed dupes.</span>
        <span class="n">output_info</span><span class="o">=</span><span class="p">[</span>
            <span class="n">OutputAliasInfo</span><span class="p">(</span>
                <span class="n">output_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">output_type</span><span class="p">,</span>
                <span class="n">raw_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span>
                <span class="n">dynamic_dims</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span><span class="p">,</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">base_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">],</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">output_info</span>
        <span class="p">],</span>
        <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
        <span class="c1"># We are guaranteed not to get here, since dupes are not supported today with subclass inputs.</span>
        <span class="n">subclass_inp_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">subclass_fw_graph_out_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">subclass_tangent_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_train</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">is_train</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Given our ViewAndMutation metadata, this fn constructs a new set of metadata,</span>
<span class="c1"># after adding synthetic base arguments to the function.</span>
<span class="c1"># Most of the work in this fn is slogging through all of the metadata corresponding to inputs,</span>
<span class="c1"># and updating it with our synthetic base calling convention.</span>
<span class="c1">#</span>
<span class="c1"># When config.debug_assert is set, we automatically regenerate the metadata</span>
<span class="c1"># and compare it to this output for sanity.</span>
<span class="c1">#</span>
<span class="c1"># In addition to the updated metadata, also return the list of input indices</span>
<span class="c1"># that will need to be updated in the synthetic base epilogue</span>
<span class="k">def</span> <span class="nf">create_synthetic_base_metadata</span><span class="p">(</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="c1"># Maps each outer argument idx to its inner idx (or, if this outer arg is generated from a</span>
    <span class="c1"># synthetic base, you get a tuple of (i, TensorMeta), telling you the base tensor idx, and view metadata)</span>
    <span class="n">synthetic_base_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span>
    <span class="n">outer_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">inner_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ViewAndMutationMeta</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>

    <span class="n">S_Outer</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;S_Outer&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="n">S_Inner</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;S_Inner&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="n">synthetic_base_to_indices</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">S_Inner</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">S_Outer</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">inner_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_args</span><span class="p">)):</span>
        <span class="n">outer_aliased_indices_of_current_base_arg</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">outer_idx</span> <span class="k">for</span> <span class="n">outer_idx</span><span class="p">,</span> <span class="n">inner_idx_or_tuple</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_idx_or_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inner_idx_or_tuple</span> <span class="o">==</span> <span class="n">inner_idx</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_idx_or_tuple</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inner_idx_or_tuple</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">inner_idx</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">synthetic_base_to_indices</span><span class="p">[</span><span class="n">inner_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">outer_aliased_indices_of_current_base_arg</span>

    <span class="c1"># given the requires_grad info on mutated inputs,</span>
    <span class="c1"># generate the requires_grad info on those same mutated inputs, but after constructing synthetic bases.</span>
    <span class="n">input_infos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">outer_indices</span> <span class="ow">in</span> <span class="n">synthetic_base_to_indices</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="c1"># leaf-ness should be all-or-nothing for aliased tensor.</span>
        <span class="c1"># (aka if &quot;a&quot; and &quot;b&quot; are views, then a.is_leaf == b.is_leaf)</span>
        <span class="n">any_leaf</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">is_leaf</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">)</span>
        <span class="n">all_leaf</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">is_leaf</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">any_leaf</span> <span class="o">==</span> <span class="n">all_leaf</span>
        <span class="n">inpt_info</span> <span class="o">=</span> <span class="n">InputAliasInfo</span><span class="p">(</span>
            <span class="c1"># If len(outer_indices) &gt; 1, then this input is a synthetic base.</span>
            <span class="c1"># The invariant is that to the rest of aot autograd, synthetic bases only show up if</span>
            <span class="c1"># one of their aliases gets a data mutation. And if any of their aliases get metadata</span>
            <span class="c1"># mutations, they will be hidden from the rest of aot autograd.</span>
            <span class="n">mutates_data</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outer_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">outer_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">,</span>
            <span class="n">mutates_metadata</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outer_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">outer_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">,</span>
            <span class="n">mutations_hidden_from_autograd</span><span class="o">=</span><span class="nb">all</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">mutations_hidden_from_autograd</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">),</span>
            <span class="n">is_leaf</span><span class="o">=</span><span class="n">any_leaf</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="nb">any</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">input_infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt_info</span><span class="p">)</span>

    <span class="c1"># Find any inputs that fulfill the following criteria:</span>
    <span class="c1"># (1) They are part of a synthetic base (because they alias another input,</span>
    <span class="c1">#      and at least one input experiences a data mutation)</span>
    <span class="c1"># (2) They experience a metadata mutation</span>
    <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">outer_idx</span> <span class="k">for</span> <span class="n">outer_idx</span><span class="p">,</span> <span class="n">inpt_info</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inpt_info</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># grab the original requires grad info on the outputs, except the ones from the mutated inputs</span>
    <span class="n">input_metadata_output_info</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">OutputAliasInfo</span><span class="p">(</span>
            <span class="n">output_type</span><span class="o">=</span><span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">,</span>
            <span class="n">raw_type</span><span class="o">=</span><span class="n">FunctionalTensor</span><span class="p">,</span>
            <span class="n">dynamic_dims</span><span class="o">=</span><span class="p">{</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outer_args</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_concrete_int</span><span class="p">(</span><span class="n">s</span><span class="p">)},</span>
            <span class="n">base_idx</span><span class="o">=</span><span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">outer_args</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">)</span> <span class="k">for</span> <span class="n">outer_idx</span> <span class="ow">in</span> <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
    <span class="n">existing_output_infos</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">OutputAliasInfo</span><span class="p">(</span>
            <span class="n">output_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">output_type</span><span class="p">,</span>
            <span class="n">raw_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span>
            <span class="n">dynamic_dims</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span><span class="p">,</span>
            <span class="c1"># Map the input idx pre-synthetic-bases to the new idx post-synthetic-bases</span>
            <span class="n">base_idx</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">base_idx</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">output_info</span><span class="p">]</span>

    <span class="n">inner_mutated_tangents</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span>
        <span class="k">for</span> <span class="n">inner_idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inner_args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_infos</span><span class="p">[</span><span class="n">inner_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">input_infos</span><span class="p">[</span><span class="n">inner_idx</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
    <span class="p">]</span>

    <span class="n">output_info</span> <span class="o">=</span> <span class="n">existing_output_infos</span> <span class="o">+</span> <span class="n">input_metadata_output_info</span>
    <span class="c1"># Regenerate traced tangents to include mutated inputs including synthetic bases</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">inner_mutated_tangents</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_mutated_tangents</span><span class="p">):]</span>

    <span class="k">return</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
        <span class="n">input_info</span><span class="o">=</span><span class="n">input_infos</span><span class="p">,</span>
        <span class="n">output_info</span><span class="o">=</span><span class="n">output_info</span><span class="p">,</span>
        <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
        <span class="c1"># We are guaranteed not to get here, since synthetic_base codepaths are not supported today with subclass inputs.</span>
        <span class="n">subclass_inp_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">subclass_fw_graph_out_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">subclass_tangent_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_train</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">is_train</span><span class="p">,</span>
    <span class="p">),</span> <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span>

<span class="c1"># MOTIVATION:</span>
<span class="c1">#</span>
<span class="c1"># When tracing functions for future execution, one must be careful not to pass</span>
<span class="c1"># in the same input tensor multiple times (e.g., f(x, x), as this can result</span>
<span class="c1"># in graphs that are ONLY valid if you later pass a new tensor in exactly the</span>
<span class="c1"># same way (e.g., f(y, y)).  (NB: we really mean duplicate; two distinct</span>
<span class="c1"># tensors that alias each other is a different situation that is covered by</span>
<span class="c1"># aot_dispatch_deduplicated_autograd). Here are two examples:</span>
<span class="c1">#</span>
<span class="c1"># (1) Suppose you have a function:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return x + y</span>
<span class="c1">#</span>
<span class="c1"># If you make_fx(f)(x, x), you will trace out:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return y + y</span>
<span class="c1">#</span>
<span class="c1"># Oops!</span>
<span class="c1">#</span>
<span class="c1"># (2) For most tensors x and y, you can compute f&#39;s gradient with respect to</span>
<span class="c1"># these to inputs by saying torch.autograd.grad(f(x, y), (x, y)).  However,</span>
<span class="c1"># if x is y, you will trace out a program that gets incorrect gradients:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; x = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + x, (x, x))</span>
<span class="c1">#   (tensor([2.]), tensor([2.]))</span>
<span class="c1">#</span>
<span class="c1"># In other words, the gradient is double-counted.  Deduplicating the arguments</span>
<span class="c1"># gives you an appropriate gradient:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; y = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + y, (x, y))</span>
<span class="c1">#   (tensor([1.]), tensor([1.]))</span>
<span class="c1">#</span>
<span class="c1"># HOW TO DEDUPLICATE:</span>
<span class="c1">#</span>
<span class="c1"># There are a few strategies, in order of preference:</span>
<span class="c1">#</span>
<span class="c1"># 1. For every duplicate argument to the function, detach it into</span>
<span class="c1">#    a separate leaf tensor, so that it is no longer duplicated.</span>
<span class="c1">#</span>
<span class="c1">#       PRO: The resulting compiled graph works for any configuration</span>
<span class="c1">#       of duplicated arguments.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the metadata of inputs:</span>
<span class="c1">#</span>
<span class="c1">#           def f(x, y):</span>
<span class="c1">#               x.transpose_(0, 1)</span>
<span class="c1">#               y.transpose_(0, 2)</span>
<span class="c1">#</span>
<span class="c1">#           x = torch.randn(2, 3, 4)</span>
<span class="c1">#           f(x, x)</span>
<span class="c1">#</span>
<span class="c1">#       The ordering of the transposes inside f dictates whether or not</span>
<span class="c1">#       you get [4, 2, 3] or [3, 4, 2].  This means that you cannot precompute</span>
<span class="c1">#       what metadata mutations should get applied to each input; you need to</span>
<span class="c1">#       assume they aren&#39;t duplicates (what we do today) or preserve</span>
<span class="c1">#       the original metadata mutations exactly in order, so that they work</span>
<span class="c1">#       for any duplicate configuration.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the data of inputs.</span>
<span class="c1">#       In particular, leaf tensors that require grad cannot be mutated,</span>
<span class="c1">#       this makes it impossible to differentiate with respect to the original</span>
<span class="c1">#       base.</span>
<span class="c1">#</span>
<span class="c1"># 2. For every duplicate argument to the function, remove it, so it is</span>
<span class="c1">#    no longer part of the &quot;true&quot; signature:</span>
<span class="c1">#</span>
<span class="c1">#       PRO: Implemented naively, it still works for metadata/data mutation.</span>
<span class="c1">#</span>
<span class="c1">#       CON: The resulting compiled graph is duplicate-specialized: it only</span>
<span class="c1">#       works if future calls duplicate arguments in exactly the same way.</span>
<span class="c1">#       Horribly, Dynamo doesn&#39;t guard on this at the moment.  But even if</span>
<span class="c1">#       it did, you could still end up recompiling a bunch of each duplicate.</span>
<span class="c1">#</span>
<span class="c1"># Our strategy is to do (1) if we can, and do (2) otherwise, erroring if</span>
<span class="c1"># Dynamo&#39;s guards are not enough.  In practice, this seems to cover</span>
<span class="c1"># everything.</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">aot_wrapper_dedupe</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">compiler_fn</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Use information about whether or not flat_fn mutates its arguments</span>
    <span class="c1"># or not to handle dupe args</span>

    <span class="c1"># Strategy 1: For any input that is not mutated, we can leafify it if we</span>
    <span class="c1"># need to remove a duplicate.</span>
    <span class="n">leaf_flat_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">args_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">ok</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">a</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args_set</span><span class="p">:</span>
            <span class="n">args_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
            <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ok</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="n">ok</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">leaf_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">requires_subclass_dispatch</span><span class="p">(</span><span class="n">leaf_flat_args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Encountered duplicate inputs that are mutated in the graph, but at least one input/output</span>
<span class="s2">to the graph is a tensor subclass. This is not supported today. You can try to</span>
<span class="s2">remove the aliasing yourself as a workaround, or otherwise file an issue on github.&quot;&quot;&quot;</span><span class="p">)</span>

    <span class="c1"># export path: ban duplicate inputs for now, add later if requested.</span>
    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Encountered duplicated inputs that are mutated in the graph you are trying to export.</span>
<span class="s2">This functionality is currently not supported. If needed, please file a github issue.</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="c1"># Strategy 2: Duplicate specialize.</span>
    <span class="c1">#</span>
    <span class="c1"># In Haskell types, suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   add_dupe_args :: DedupedArgs -&gt; Args</span>
    <span class="c1">#   remove_dupe_args :: Args -&gt; DedupedArgs</span>
    <span class="c1">#</span>
    <span class="c1">#   compiler_fn</span>
    <span class="c1">#       :: (DedupedArgs -&gt; R) -&gt; DedupedArgs -&gt; AOTConfig -&gt; (DedupedArgs -&gt; R)</span>
    <span class="c1">#   deped_compiler_fn</span>
    <span class="c1">#       :: (Args -&gt; R) -&gt; Args -&gt; AOTConfig -&gt; (Args -&gt; R)</span>
    <span class="c1">#</span>
    <span class="c1"># Then the code below can be written in point-free style as:</span>
    <span class="c1">#</span>
    <span class="c1">#   deduped_compiler_fn f a c =</span>
    <span class="c1">#       compiler_fn (f . add_dupe_args) (remove_dupe_args a) c . remove_dupe_args</span>
    <span class="c1">#</span>
    <span class="c1"># Suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># We want:</span>
    <span class="c1">#</span>
    <span class="c1">#   remove_dupe_args([a, b, a, c]) == [a, b, c]</span>
    <span class="c1">#   add_dupe_args([a, b, c]) == [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># This is done via (respectively):</span>
    <span class="c1">#</span>
    <span class="c1">#   seen_args = {a: 0, b: 1, c: 2}</span>
    <span class="c1">#   enumerate(add_dupe_map) = [  # how to get args from the deduped list</span>
    <span class="c1">#       (0, 0),</span>
    <span class="c1">#       (1, 1),</span>
    <span class="c1">#       (2, 0),</span>
    <span class="c1">#       (3, 2),</span>
    <span class="c1">#   ]</span>
    <span class="c1">#   keep_arg_mask = [True, True, False, True]</span>

    <span class="n">seen_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">keep_arg_mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Implicitly map duped arg position (list index) to de-duped arg position</span>
    <span class="n">add_dupe_map</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">duped_arg_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># index into deduped_flat_args</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seen_args</span><span class="p">:</span>
                <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">add_dupe_map</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
                <span class="k">continue</span>
            <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>

        <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">add_dupe_map</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">)</span> <span class="o">==</span> <span class="n">duped_arg_len</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Expects add_dupe_map to have length </span><span class="si">{</span><span class="n">duped_arg_len</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># NB: Hot path, avoid set lookups here</span>
    <span class="c1"># TODO: Can avoid the zip here too, probably</span>
    <span class="k">def</span> <span class="nf">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">keep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duped_arg_len</span><span class="p">)]</span>

    <span class="n">deduped_flat_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="c1"># Update our input metadata to remove duped input metadata.</span>
    <span class="n">updated_fw_metadata</span> <span class="o">=</span> <span class="n">remove_dupe_metadata</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">,</span> <span class="n">add_dupe_map</span><span class="p">)</span>

    <span class="n">tracing_context</span> <span class="o">=</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">tracing_context</span> <span class="ow">and</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">:</span>
        <span class="c1"># TODO(voz): This structure is 1:1, we could consider an alternate structure like</span>
        <span class="c1"># kept_pos:[dupe_arg_pos], however, add_dupe_map is 1:1 so we would need a new structure there,</span>
        <span class="c1"># which feels like needless complexity for a tiny bit of efficiency at this point.</span>
        <span class="k">for</span> <span class="n">dupe_arg_pos</span><span class="p">,</span> <span class="p">(</span><span class="n">kept_pos</span><span class="p">,</span> <span class="n">keep_arg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">keep_arg</span><span class="p">:</span>
                <span class="n">dupe_arg_source</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">[</span><span class="n">dupe_arg_pos</span><span class="p">]</span>
                <span class="n">kept_arg_source</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">[</span><span class="n">kept_pos</span><span class="p">]</span>
                <span class="n">tracing_context</span><span class="o">.</span><span class="n">guards_context</span><span class="o">.</span><span class="n">aotautograd_guards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">DuplicateInputs</span><span class="p">(</span><span class="n">kept_arg_source</span><span class="p">,</span> <span class="n">dupe_arg_source</span><span class="p">))</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="n">ref_fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
            <span class="n">wrapped_flat_fn</span><span class="p">,</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
            <span class="n">is_train</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">is_train</span><span class="p">,</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">deduped_flat_args</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">ref_fw_metadata</span> <span class="o">==</span> <span class="n">updated_fw_metadata</span><span class="p">,</span> \
            <span class="sa">f</span><span class="s1">&#39;ref_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ref_fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s1">, actual_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">updated_fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">wrapped_flat_fn</span><span class="p">,</span> <span class="n">deduped_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">updated_fw_metadata</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">deduped_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">deduped_args</span><span class="p">)</span>

    <span class="n">wrapped_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># This can be uncommented when we properly guard for duplicates,</span>
    <span class="c1"># but right now we must not do it.</span>
    <span class="c1"># if not config.debug_assert:</span>
    <span class="c1">#     return wrapped_compiled_fn</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">wrapped_compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debugged_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Test that the computed remove/add arg functions are an inverse</span>
        <span class="n">new_args</span> <span class="o">=</span> <span class="n">add_dupe_args</span><span class="p">(</span><span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_args</span><span class="p">,</span> <span class="n">args</span><span class="p">)):</span>
            <span class="n">seen</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">y</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                <span class="n">aot_config</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would be a duplicate of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># This is only an error if there is metadata mutation on both of</span>
        <span class="c1"># the duped arguments; in this case, we need to know what order</span>
        <span class="c1"># the metadata mutation applies in.  You&#39;ll get the correct result</span>
        <span class="c1"># otherwise, because a graph that assumes distinct inputs works if</span>
        <span class="c1"># you dupe the inputs (the gradient contributions from each input</span>
        <span class="c1"># will get summed up appropriately.)</span>
        <span class="c1">#</span>
        <span class="c1"># TODO: work out how to setup this assert correctly</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        assert len(seen) == unique_args, format_guard_bug_msg(aot_config,</span>
<span class="sd">            f&quot;there would be {unique_args} distinct arguments&quot;</span>
<span class="sd">        )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">debugged_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">debugged_compiled_fn</span>

<span class="c1"># This layer handles the situation where you have two inputs that alias each other,</span>
<span class="c1"># and one of the inputs is mutated.</span>
<span class="c1"># We need to take special care to ensure that the mutation is applied to the other aliases in the graph.</span>
<span class="c1">#</span>
<span class="c1"># pre-condition: aot_wrapper_dedup has already run.</span>
<span class="c1"># (This function will in theory work if there are duplicate args.</span>
<span class="c1"># However, the synthetic base code path is a bit sub-optimal, and running with dupe&#39;d inputs</span>
<span class="c1"># would cause us to hit that path more frequently).</span>
<span class="k">def</span> <span class="nf">aot_wrapper_synthetic_base</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="c1"># Currently, the only reason we need to plumb this bool is because</span>
    <span class="c1"># the synthetic base code prohibits more cases in the autograd case than the inference case.</span>
    <span class="n">needs_autograd</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">compiler_fn</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">is_inference</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">needs_autograd</span>
    <span class="n">flat_args_with_synthetic_bases</span><span class="p">,</span> <span class="n">synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="n">is_inference</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Happy path: we don&#39;t need synthetic bases</span>
    <span class="k">if</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

    <span class="c1"># export path: ban synthetic bases for now, add later if requested.</span>
    <span class="k">if</span> <span class="n">requires_subclass_dispatch</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Encountered aliased inputs that are mutated in the graph, but at least one input/output</span>
<span class="s2">to the graph is a tensor subclass. This is not supported today. You can try to</span>
<span class="s2">remove the aliasing yourself as a workaround, or otherwise file an issue on github.&quot;&quot;&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Encountered aliased inputs that are mutated in the graph you are trying to export.</span>
<span class="s2">This functionality is currently not supported. If needed, please file a github issue.</span>

<span class="s2">synthetic_base_info=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">)</span><span class="si">}</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">)</span>

    <span class="c1"># Update our forward metadata to take synthetic bases into account</span>
    <span class="n">fw_metadata_updated</span><span class="p">,</span> <span class="n">aliased_arg_idx_with_metadata_mutations</span> <span class="o">=</span> \
        <span class="n">create_synthetic_base_metadata</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">synthetic_base_info</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_args_with_synthetic_bases</span><span class="p">)</span>

    <span class="n">num_aliased_args_with_metadata_mutations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_arg_idx_with_metadata_mutations</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">unpack_synthetic_bases</span><span class="p">(</span><span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="n">f_args_inner</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">inner_idx_or_tuple</span> <span class="ow">in</span> <span class="n">synthetic_base_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_idx_or_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primals</span><span class="p">[</span><span class="n">inner_idx_or_tuple</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">inner_base_idx</span><span class="p">,</span> <span class="n">view_tensor</span> <span class="o">=</span> <span class="n">inner_idx_or_tuple</span>
                <span class="n">base</span> <span class="o">=</span> <span class="n">primals</span><span class="p">[</span><span class="n">inner_base_idx</span><span class="p">]</span>
                <span class="n">view_arg</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span>
                    <span class="n">base</span><span class="p">,</span> <span class="n">view_tensor</span><span class="p">,</span> <span class="n">view_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="p">)</span>
                <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">view_arg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f_args_inner</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">unpacked_args</span> <span class="o">=</span> <span class="n">unpack_synthetic_bases</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="c1"># This is a bit subtle. The goal of this entire function (aot_dispatch_synthetic_bases)</span>
        <span class="c1"># is to relieve the downstream logic from having to reason about mutations on inputs that alias</span>
        <span class="c1"># each other, by replacing aliased inputs with a synthetic base.</span>
        <span class="c1"># One area where this breaks down a bit however is if one of those aliased inputs</span>
        <span class="c1"># experienced a metadata mutation.</span>
        <span class="c1"># We are now obligated to reapply the metadata mutation directly to the user&#39;s input;</span>
        <span class="c1"># it isn&#39;t enough to apply mutations back to the synthetic base in the downstream logic.</span>
        <span class="c1">#</span>
        <span class="c1"># The way we handle this is by pretending that those aliased inputs that experience metadata mutations</span>
        <span class="c1"># are additional outputs in the user&#39;s forward function.</span>
        <span class="c1"># The downstream logic will just treat these as &quot;user outputs that alias inputs&quot;.</span>
        <span class="c1"># However, we will manually grab them at runtime here, use them to reapply the metadata mutation</span>
        <span class="c1"># to the user inputs, and not return them to the user.</span>
        <span class="n">aliased_args_with_metadata_mutations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">unpacked_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_args_with_metadata_mutations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">*</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">unpacked_args</span><span class="p">)),</span> <span class="o">*</span><span class="n">aliased_args_with_metadata_mutations</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">unpacked_args</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="n">ref_fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
            <span class="n">wrapped_flat_fn</span><span class="p">,</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
            <span class="n">is_train</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">is_train</span><span class="p">,</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">flat_args_with_synthetic_bases</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">ref_fw_metadata</span> <span class="o">==</span> <span class="n">fw_metadata_updated</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;ref_metadata=</span><span class="si">{</span><span class="n">pprint</span><span class="o">.</span><span class="n">pformat</span><span class="p">(</span><span class="n">partial_asdict</span><span class="p">(</span><span class="n">ref_fw_metadata</span><span class="p">))</span><span class="si">}</span><span class="s1">, &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">actual_metadata=</span><span class="si">{</span><span class="n">pprint</span><span class="o">.</span><span class="n">pformat</span><span class="p">(</span><span class="n">partial_asdict</span><span class="p">(</span><span class="n">fw_metadata_updated</span><span class="p">))</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">wrapped_flat_fn</span><span class="p">,</span> <span class="n">flat_args_with_synthetic_bases</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata_updated</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">args_with_synthetic_bases</span><span class="p">,</span> <span class="n">synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="n">is_inference</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">aliased_args_w_metadata_mutations</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
        <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">args_with_synthetic_bases</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_aliased_args_with_metadata_mutations</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># This code does not handle **all** input metadata mutations.</span>
            <span class="c1"># Instead, it only handles metadata mutations on inputs that were converted into synthetic bases</span>
            <span class="c1"># (which only happens if at least one aliased input experienced a data mutation).</span>
            <span class="c1"># e.g:</span>
            <span class="c1"># def f(a, b):</span>
            <span class="c1">#     a.mul_(2)</span>
            <span class="c1">#     b.t_(1, 0)</span>
            <span class="c1"># f(x.view(2, 2), x.view(2, 2))</span>
            <span class="n">mutated_metadata_inps</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="o">-</span><span class="n">num_aliased_args_with_metadata_mutations</span><span class="p">:]</span>
            <span class="n">user_outs</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[:</span><span class="o">-</span><span class="n">num_aliased_args_with_metadata_mutations</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">mutated_inp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aliased_args_w_metadata_mutations</span><span class="p">,</span> <span class="n">mutated_metadata_inps</span><span class="p">):</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span><span class="n">mutated_inp</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">mutated_inp</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">mutated_inp</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">user_outs</span>
        <span class="k">return</span> <span class="n">outs</span>

    <span class="k">return</span> <span class="n">wrapped_compiled_fn</span>


<span class="k">def</span> <span class="nf">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;parameter/buffer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;input </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># The wrapper created by this function handles all of the runtime aliasing and mutation &quot;epilogue&quot; logic</span>
<span class="c1"># that needs to run after the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># This function accepts a trace_joint flag, indicating whether or not we&#39;re generating the runtime</span>
<span class="c1"># epilogue for a forward-only inference graph, or for an autograd.Function.apply function.</span>
<span class="c1"># This is because there are some minor differences in how we treat these cases at runtime:</span>
<span class="c1"># - resize_() is currently handled in the inference case, but not fully handled in the autograd case.</span>
<span class="c1"># - the autograd cases inserts TensorAlias wrapper objects for outputs that alias inputs</span>
<span class="k">def</span> <span class="nf">create_runtime_wrapper</span><span class="p">(</span>
    <span class="n">compiled_fn</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">runtime_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">indices_of_inps_to_detach</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">disable_amp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">runtime_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
            <span class="n">args_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="c1"># See Note [Detaching inputs that never need gradients]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices_of_inps_to_detach</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args_</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">args_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">args_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_force_original_view_tracking</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
                <span class="n">all_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                    <span class="n">compiled_fn</span><span class="p">,</span>
                    <span class="n">args_</span><span class="p">,</span>
                    <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># When we have an inference graph, we run with torch.no_grad.</span>
            <span class="c1"># It&#39;s possible to get an inference graph with inputs that require grad,</span>
            <span class="c1"># in which case we want to make sure autograd is disabled</span>
            <span class="c1"># (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">all_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                    <span class="n">compiled_fn</span><span class="p">,</span>
                    <span class="n">args</span><span class="p">,</span>
                    <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
        <span class="n">num_metadata_mutated_inps</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_inputs</span>
        <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>

        <span class="k">if</span> <span class="n">keep_input_mutations</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
                <span class="o">==</span> <span class="n">num_metadata_mutated_inps</span> <span class="o">+</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_metadata_mutated_inps</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
                <span class="o">==</span> <span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_mutated_inps</span>
            <span class="p">)</span>
        <span class="c1"># Step 3: After running the compiled fw, apply updates to mutated inputs</span>
        <span class="n">num_mutations_to_apply</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_mutations_to_apply</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">updated_inputs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[:</span> <span class="n">num_mutations_to_apply</span><span class="p">]</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[</span><span class="n">num_mutations_to_apply</span> <span class="p">:]</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span>
            <span class="p">):</span>
                <span class="n">meta</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">original_inpt</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span>
                        <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">alias</span>
                    <span class="c1"># We need to grab the size/stride/storage_offset from the compiled forward,</span>
                    <span class="c1"># and use that to mutate the metadata of the input</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span>
                    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">is_leaf</span> <span class="ow">and</span> <span class="n">original_inpt</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                        <span class="c1"># We can hit this situation in this case:</span>
                        <span class="c1">#   def f(x):</span>
                        <span class="c1">#       x.detach().mul_(2)</span>
                        <span class="c1">#       return x + 1</span>
                        <span class="c1"># AOTAutograd will see a mutation in the above case, and try to</span>
                        <span class="c1"># apply a copy_() here, in the epilogue.</span>
                        <span class="c1"># But if x required gradients, and is a leaf, then autograd</span>
                        <span class="c1"># will yell at us for trying to mutate it.</span>
                        <span class="c1"># However, it&#39;s only possible to end up in this scenario (like the above)</span>
                        <span class="c1"># if all of the mutations to the leaf input were non-autograd-tracking mutations</span>
                        <span class="c1"># (aka mutations under no_grad(), or on detached views).</span>
                        <span class="c1"># In that case, we fully want to hide the mutation from autograd, so detaching is ok.</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">all_outs</span>

        <span class="c1"># Step 4: Manually regenerate any outputs that are aliased to inputs, instead of</span>
        <span class="c1"># compiling them.</span>
        <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># The compiled forward also returned intermediate bases. We don&#39;t want to return them to the user.</span>
            <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">fw_outs_no_intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                    <span class="p">:</span> <span class="o">-</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
                <span class="p">]</span>
                <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fw_outs_no_intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span>
                <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_no_intermediate_bases</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>

            <span class="n">fw_outs_including_aliases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
                <span class="n">fw_outs_no_intermediate_bases</span><span class="p">,</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span>
            <span class="p">)):</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span><span class="p">]:</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span>
                    <span class="n">o_</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">alias</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">o_</span> <span class="o">=</span> <span class="n">o</span>

                <span class="n">o_grad</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">:</span>
                    <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">o_grad</span><span class="p">)</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">:</span>
                    <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span><span class="p">:</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">intermediate_bases</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">:</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">intermediate_bases</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">fw_outs_no_intermediate_bases</span>
                <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">base_tensor_list</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                <span class="c1"># TODO: handle the custom autograd function case here.</span>
                <span class="c1"># We need a way to check whether a tensor came from a custom autograd fn from python,</span>
                <span class="c1"># AND a way to replay that custom view fn.</span>
                <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">o_grad</span><span class="p">)</span>
                <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
            <span class="n">ret_outs</span> <span class="o">=</span> <span class="n">fw_outs_including_aliases</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret_outs</span> <span class="o">=</span> <span class="n">fw_outs</span>

        <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">dynamic_outputs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ret_outs</span><span class="p">,</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;_dynamo_weak_dynamic_indices&#39;</span><span class="p">):</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">_dynamo_weak_dynamic_indices</span> <span class="o">|=</span> <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">_dynamo_weak_dynamic_indices</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ret_outs</span>
    <span class="k">return</span> <span class="n">runtime_wrapper</span>

<span class="c1"># Calling convention: If we are running functionalized RNG, then outs consists</span>
<span class="c1"># of (user_outs, rng_offset)</span>
<span class="k">def</span> <span class="nf">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">outs</span><span class="p">,</span> <span class="n">return_new_outs</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">new_rng_offset</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">set_new_offset</span><span class="p">(</span><span class="n">new_rng_offset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_new_outs</span><span class="p">:</span>
            <span class="n">user_outs</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">user_outs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">outs</span>


<span class="k">def</span> <span class="nf">create_functionalized_rng_ops_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Functionalization of rng ops changes the calling convention of the joint graph.</span>
    <span class="c1"># It goes from (primals, tangents) to (seed, offset, primals, tangents)</span>
    <span class="c1"># At runtime, we pass on the current seed and offset. This is hidden from</span>
    <span class="c1"># the user.</span>
    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">override_get_rng_state</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_state_as_tensor</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">override_set_rng_state</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">set_state_from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">append_rng_offsets</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
            <span class="c1"># args signature before: Tuple(fwd_outputs), Tuple(bwd_outputs)</span>
            <span class="c1"># args signature after: Tuple(fwd_outputs, new_fwd_rng_offset), Tuple(bwd_offset, new_bwd_rng_offset)</span>
            <span class="k">return</span> <span class="p">((</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_updated_fwd_offset</span><span class="p">()),</span>
                    <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_updated_bwd_offset</span><span class="p">()))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># args signature before: Tuple(fwd_outputs)</span>
            <span class="c1"># args signature after: Tuple(fwd_outputs, new_fwd_rng_offset)</span>
            <span class="k">return</span> <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_updated_fwd_offset</span><span class="p">())</span>


    <span class="k">def</span> <span class="nf">traced_joint</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.get_rng_state&quot;</span><span class="p">,</span> <span class="n">override_get_rng_state</span><span class="p">),</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="n">override_set_rng_state</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">append_rng_offsets</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">traced_forward</span><span class="p">(</span><span class="o">*</span><span class="n">primals_fwd_seed_fwd_base_offset</span><span class="p">):</span>
        <span class="c1"># The signature is (*primals, seed, offset)</span>
        <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.get_rng_state&quot;</span><span class="p">,</span> <span class="n">override_get_rng_state</span><span class="p">),</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="n">override_set_rng_state</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">append_rng_offsets</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">primals_fwd_seed_fwd_base_offset</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]))</span>

    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="c1"># Get the current seed and offset to setup tracing.</span>
        <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
        <span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">record_state</span><span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">)</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">record_state</span><span class="p">(</span><span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">traced_joint</span><span class="p">,</span> <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Get the current seed and offset to setup tracing.</span>
        <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">record_state</span><span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">traced_forward</span><span class="p">,</span> <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">)</span>


<span class="c1"># Output structure:</span>
<span class="c1"># - List[Tensor] if tracing an inference graph</span>
<span class="c1"># - Tuple[List[Tensor], List[Tensor]] if tracing a joint graph.</span>
<span class="c1"># This function effectively concats each inner list of subclass tensors</span>
<span class="c1"># into a (potentially longer) list of inner tensors.</span>
<span class="c1">#</span>
<span class="c1"># This function takes in a pytree of arguments and unwraps any tensor subclasses.</span>
<span class="c1"># Annoyingly, we can&#39;t use pytrees to perform the unwrapping, because unwrapping returns</span>
<span class="c1"># a list of tensors that we would then need to concat together.</span>
<span class="c1"># Instead, we specialize the logic for the inference vs. joint graph case.</span>
<span class="c1"># NOTE: this function is hot, since we unwrap tensor subclass inputs at runtime</span>
<span class="k">def</span> <span class="nf">unwrap_tensor_subclasses</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">concat_inner_tensors_from_subclasses</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
        <span class="n">xs_inner</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
                <span class="n">xs_inner</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">xs_inner</span> <span class="o">+=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">xs_inner</span>

    <span class="k">if</span> <span class="n">is_joint_structure</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="n">unwrapped_args_fw</span> <span class="o">=</span> <span class="n">concat_inner_tensors_from_subclasses</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">unwrapped_args_tangents</span> <span class="o">=</span> <span class="n">concat_inner_tensors_from_subclasses</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">unwrapped_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">unwrapped_args_fw</span><span class="p">,</span> <span class="n">unwrapped_args_tangents</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="n">unwrapped_args_fw</span> <span class="o">=</span> <span class="n">concat_inner_tensors_from_subclasses</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">)</span>
        <span class="n">unwrapped_args</span> <span class="o">=</span> <span class="n">unwrapped_args_fw</span>
    <span class="k">return</span> <span class="n">unwrapped_args</span>

<span class="c1"># Turns a flattened list of tensor arguments into (maybe) subclass tensors.</span>
<span class="c1"># This function is used both at trace time and runtime, so we have an is_runtime flag telling us which context we&#39;re in.</span>
<span class="k">def</span> <span class="nf">wrap_tensor_subclasses</span><span class="p">(</span>
    <span class="n">unwrapped_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">subclass_metas</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]],</span>
    <span class="n">num_fw_outs_saved_for_bw</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_runtime</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
    <span class="n">wrapped_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_args_tallied</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">subclass_meta</span> <span class="ow">in</span> <span class="n">subclass_metas</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subclass_meta</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">wrapped_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">[</span><span class="n">subclass_meta</span><span class="p">])</span>
            <span class="n">num_args_tallied</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subclass_meta</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">)</span>
            <span class="n">wrapped_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subclass_meta</span><span class="o">.</span><span class="n">creation_fn</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">,</span> <span class="n">is_runtime</span><span class="o">=</span><span class="n">is_runtime</span><span class="p">))</span>
            <span class="n">num_args_tallied</span> <span class="o">+=</span> <span class="n">subclass_meta</span><span class="o">.</span><span class="n">arg_count</span>

    <span class="c1"># Note: [Partitioner handling for Subclasses, Part 2]</span>
    <span class="c1"># At the beginning of AOTAutograd, we collect metadata on the inputs and outputs of the user fw,</span>
    <span class="c1"># to figure out which inputs/outputs are subclasses, and how to reconstruct the subclasses after flattening them.</span>
    <span class="c1">#</span>
    <span class="c1"># When this function is called at runtime in the forward,</span>
    <span class="c1"># we have been passed a list of (flattened) dense-tensor fw-outs, and need to reconstruct any subclass fw outs.</span>
    <span class="c1">#</span>
    <span class="c1"># One reasonable question that you should ask: when should the dense_tensor -&gt; subclass_tensor wrapping happen?</span>
    <span class="c1"># Answer: we do it **inside of our compiled autograd.Function**.</span>
    <span class="c1"># This seems like morally the right place: autograd happens above subclass desugaring,</span>
    <span class="c1"># so autograd should see actual tensor subclasses at runtime, and not flattened dense tensors.</span>
    <span class="c1">#</span>
    <span class="c1"># This causes a tricky interaction though: when we run the min-cut partitioner to divvy up the joint graph</span>
    <span class="c1"># into a forward and backward graph, we end up with some activations that show up as extra outputs</span>
    <span class="c1"># in the compiled forward graph, that are **not** user outputs.</span>
    <span class="c1"># These activations are not visible to the user, and so there&#39;s no need for us to wrap them back into subclasses.</span>
    <span class="c1">#</span>
    <span class="c1"># On top of that, when we first computed subclass metadata (in `run_functionalized_fw_and_collect_metadata`),</span>
    <span class="c1"># we computed subclass metadata on every forward output, but this did **not** include activations</span>
    <span class="c1"># created by the partitioner.</span>
    <span class="c1"># as a result, `unwrapped_args` here will correspond to (*unwrapped_user_fw_outs, *activations),</span>
    <span class="c1"># but `subclass_metas` will only correspond to subclass metatadata on `user_fw_outs`.</span>
    <span class="c1"># We then need to make sure that we return (*wrapped_user_fw_outs, *activations).</span>
    <span class="k">if</span> <span class="n">num_fw_outs_saved_for_bw</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_args_tallied</span> <span class="o">+</span> <span class="n">num_fw_outs_saved_for_bw</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">unwrapped_args</span><span class="p">[</span><span class="n">num_args_tallied</span><span class="p">:]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">wrapped_args</span> <span class="o">+</span> <span class="n">activations</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">activations</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_args_tallied</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">wrapped_args</span><span class="p">)</span>

<span class="c1"># Given a bunch of &quot;dense&quot; tensor arguments, this function (potentially) wraps them into tensor subclasses.</span>
<span class="c1"># This function carefully handles the inference vs. joint cases:</span>
<span class="c1"># - when is_joint_structure is True, args is (primals, tangents)</span>
<span class="c1"># - when is_joint_structure is False, args is [*primals]</span>
<span class="k">def</span> <span class="nf">wrap_tensor_subclasses_maybe_joint</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
    <span class="c1"># Since this function is re-used for both inference and joint graphs,</span>
    <span class="k">if</span> <span class="n">is_joint_structure</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span> <span class="o">=</span> <span class="n">unwrapped_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">unwrapped_args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">wrapped_primals</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">subclass_inp_meta</span><span class="p">)</span>
        <span class="n">wrapped_tangents</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses</span><span class="p">(</span><span class="n">tangents</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">subclass_tangent_meta</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">wrapped_primals</span><span class="p">,</span> <span class="n">wrapped_tangents</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wrapped_args</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">subclass_inp_meta</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_args</span>

<span class="c1"># This wrapper handles the AOTDispatch runtime logic for tensor subclasses.</span>
<span class="c1"># At runtime, we have a compiled function that knows how to operate on the domain of DenseTensor -&gt; DenseTensor,</span>
<span class="c1"># But the user might have passed us some tensor subclass inputs (or expect some subclass tensor outputs).</span>
<span class="c1"># This function handles the wrapping and unwrapping of tensor subclasses at runtime.</span>
<span class="k">def</span> <span class="nf">aot_dispatch_subclass_wrapper</span><span class="p">(</span>
    <span class="n">runtime_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">subclass_metas</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SubclassCreationMeta</span><span class="p">]],</span>
    <span class="n">num_fw_outs_saved_for_bw</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">unwrapped_args</span> <span class="o">=</span> <span class="n">unwrap_tensor_subclasses</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># expectation: runtime_fn is a boxed fn</span>
        <span class="n">unwrapped_outs</span> <span class="o">=</span> <span class="n">runtime_fn</span><span class="p">(</span><span class="n">unwrapped_args</span><span class="p">)</span>
        <span class="n">wrapped_outs</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses</span><span class="p">(</span>
            <span class="n">unwrapped_outs</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">subclass_metas</span><span class="p">,</span> <span class="n">num_fw_outs_saved_for_bw</span><span class="o">=</span><span class="n">num_fw_outs_saved_for_bw</span><span class="p">,</span> <span class="n">is_runtime</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_outs</span>
    <span class="c1"># box it</span>
    <span class="n">inner_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">inner_fn</span>

<span class="k">def</span> <span class="nf">create_metadata_for_subclass</span><span class="p">(</span><span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="c1"># input infos</span>
    <span class="n">input_info</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">subclass_meta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">subclass_inp_meta</span><span class="p">):</span>
        <span class="n">num_inps</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subclass_meta</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">subclass_meta</span><span class="o">.</span><span class="n">arg_count</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_inps</span><span class="p">):</span>
            <span class="n">input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

    <span class="c1"># output infos</span>
    <span class="n">output_info</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">subclass_out_meta_user_outs_only</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">subclass_fw_graph_out_meta</span><span class="p">[</span><span class="n">meta</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span><span class="p">:]</span>
    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">subclass_out_meta_user_outs_only</span> <span class="o">=</span> <span class="n">subclass_out_meta_user_outs_only</span><span class="p">[:</span><span class="o">-</span><span class="n">meta</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">]</span>
    <span class="c1"># sanity assert</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">subclass_out_meta_user_outs_only</span><span class="p">)</span>
    <span class="c1"># Assume that the information on the output is shared by all of its inner tensors.</span>
    <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">subclass_meta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">,</span> <span class="n">subclass_out_meta_user_outs_only</span><span class="p">):</span>
        <span class="n">num_outs</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subclass_meta</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">subclass_meta</span><span class="o">.</span><span class="n">arg_count</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_outs</span><span class="p">):</span>
            <span class="n">output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="c1"># A bit hacky, but we don&#39;t actually care about all of the metadata here.</span>
    <span class="c1"># This metadata is used **underneath** both autograd and subclass de-sugaring,</span>
    <span class="c1"># So all we really care about is stuff like:</span>
    <span class="c1"># - num inputs/outputs (needed by the partitioner)</span>
    <span class="c1"># - input mutations (**not** used today, since we don&#39;t handle input mutations inside the subclass,</span>
    <span class="c1">#   although we should handle this eventually)</span>
    <span class="c1">#   TODO: add a test case to assert we error when this happens, instead of getting silent correctness</span>
    <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">keep_input_mutations</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">keep_input_mutations</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">subclass_inp_meta</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">subclass_fw_graph_out_meta</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">subclass_tangent_meta</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
        <span class="n">input_info</span><span class="o">=</span><span class="n">input_info</span><span class="p">,</span>
        <span class="n">output_info</span><span class="o">=</span><span class="n">output_info</span><span class="p">,</span>
        <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
        <span class="n">subclass_inp_meta</span><span class="o">=</span><span class="n">subclass_inp_meta</span><span class="p">,</span>
        <span class="n">subclass_fw_graph_out_meta</span><span class="o">=</span><span class="n">subclass_fw_graph_out_meta</span><span class="p">,</span>
        <span class="n">subclass_tangent_meta</span><span class="o">=</span><span class="n">subclass_tangent_meta</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">metadata</span>


<span class="n">SubclassTracingInfo</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;SubclassTracingInfo&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="s2">&quot;plain_tensor_trace_fn&quot;</span><span class="p">,</span> <span class="s2">&quot;plain_tensor_args&quot;</span><span class="p">,</span> <span class="s2">&quot;maybe_subclass_meta&quot;</span>
<span class="p">])</span>

<span class="c1"># Given a function operating on Subclass -&gt; Subclass, returns an function that operates on Tensor -&gt; Tensor</span>
<span class="c1"># Also returns:</span>
<span class="c1"># - the new set of arguments to pass into this function (now that tensor subclasses have been eliminated)</span>
<span class="c1"># - the updated ViewAndMutationMeta for this dense -&gt; dense function.</span>
<span class="c1"># The other important arguments are:</span>
<span class="c1"># - flat_fn_maybe_joint: when is_joint_structure=True, this is the joint fw-bw function.</span>
<span class="c1">#                        when is_joint_structure=False, this is just the forward function.</span>
<span class="c1"># - fw_only: this is *always* the forward-only function.</span>
<span class="c1">#   Why do we need this? We need to collect updated ViewAndMutationMeta on our new dense -&gt; dense functions.</span>
<span class="c1">#   In particular, we need this to tell the partitioner how many dense forward outputs there are.</span>
<span class="k">def</span> <span class="nf">aot_dispatch_subclass</span><span class="p">(</span>
    <span class="n">flat_fn_maybe_joint</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">is_joint_structure</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">fw_only</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SubclassTracingInfo&quot;</span><span class="p">:</span>
    <span class="c1"># Skip logic if we don&#39;t need to trace through any subclasses</span>
    <span class="n">req_subclass_dispatch</span> <span class="o">=</span> <span class="n">requires_subclass_dispatch</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">req_subclass_dispatch</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SubclassTracingInfo</span><span class="p">(</span>
            <span class="n">plain_tensor_trace_fn</span><span class="o">=</span><span class="n">flat_fn_maybe_joint</span><span class="p">,</span>
            <span class="n">plain_tensor_args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">maybe_subclass_meta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># TODO: add subclass guards (later PR).</span>

    <span class="c1"># What&#39;s going on here? We need to compute subclass metadata about the outputs of the joint (grad_inputs).</span>
    <span class="c1"># Annoying: we don&#39;t know the grad input metas until we&#39;re in the middle of tracing the joint,</span>
    <span class="c1"># so we set it later, while we&#39;re tracing the joint (see inner_fn() below).</span>
    <span class="c1"># Another option would be to run our run_functionalized_fw_and_collect_metadata() function</span>
    <span class="c1"># directly on the joint, but this would hurt compile time (adding yet another pass through the joint).</span>
    <span class="n">subclass_meta</span> <span class="o">=</span> <span class="n">SubclassMeta</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">use_trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="c1"># Step 1: wrap tensor inputs into subclasses if necessary</span>
        <span class="n">all_args</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses_maybe_joint</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="n">use_trace_joint</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">)</span>

        <span class="c1"># Step 2: call the inner function, with our (maybe subclass) inputs</span>
        <span class="n">wrapped_outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">all_args</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_trace_joint</span><span class="p">:</span>
            <span class="c1"># See Note: [Computing Subclass Metadata about grad_inputs]</span>
            <span class="c1"># We also stash subclass info on our grad_inputs, if we&#39;re tracing the joint.</span>
            <span class="k">nonlocal</span> <span class="n">subclass_meta</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_outs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">wrapped_outs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="c1"># Don&#39;t need fw outs since we already have subclass metadata on them</span>
            <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">wrapped_outs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">subclass_meta</span><span class="o">.</span><span class="n">grad_input_metas</span> <span class="o">=</span> <span class="n">create_subclass_meta</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">)</span>

        <span class="c1"># Step 3: Unwrap any subclass outputs back into dense tensors</span>
        <span class="n">unwrapped_outs</span> <span class="o">=</span> <span class="n">unwrap_tensor_subclasses</span><span class="p">(</span><span class="n">wrapped_outs</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="n">use_trace_joint</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">unwrapped_outs</span>

    <span class="k">def</span> <span class="nf">joint_fn</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inner_fn</span><span class="p">(</span><span class="n">flat_fn_maybe_joint</span><span class="p">,</span> <span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">),</span> <span class="n">use_trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fw_fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inner_fn</span><span class="p">(</span><span class="n">flat_fn_maybe_joint</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">use_trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">metadata_fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inner_fn</span><span class="p">(</span><span class="n">fw_only</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">use_trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">args_unwrapped</span> <span class="o">=</span> <span class="n">unwrap_tensor_subclasses</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="n">is_joint_structure</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_joint_structure</span><span class="p">:</span>
        <span class="n">primals_unwrapped</span> <span class="o">=</span> <span class="n">args_unwrapped</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">fn_to_trace</span> <span class="o">=</span> <span class="n">joint_fn</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">primals_unwrapped</span> <span class="o">=</span> <span class="n">args_unwrapped</span>
        <span class="n">fn_to_trace</span> <span class="o">=</span> <span class="n">fw_fn</span>

    <span class="c1"># Note: [Partitioner handling for Subclasses, Part 1]</span>
    <span class="c1"># The way the partitioner works is that:</span>
    <span class="c1"># (1) we pass is a single graph containing the joint fw/bw,</span>
    <span class="c1">#     where the # of graph outputs corresponds to # fw_outputs + # grad_inputs</span>
    <span class="c1"># (2) The partitioner accepts an arguments, num_fwd_outputs,</span>
    <span class="c1">#     and assumes that the first &quot;num_fwd_outputs&quot; graph outputs correspond</span>
    <span class="c1">#     to outputs of the forward graph.</span>
    <span class="c1"># How do tensor subclasses enter the picture?</span>
    <span class="c1"># the num_fwd_outputs in the final graph is actually non-trivial to compute,</span>
    <span class="c1"># because it can be influenced by input mutations and intermediate bases.</span>
    <span class="c1"># So we compute it by inspecting the current ViewAndMutationMeta object.</span>
    <span class="c1"># However, the original ViewAndMutationMeta that we computed was created</span>
    <span class="c1"># on the subclass -&gt; subclass graph,</span>
    <span class="c1"># which can have a different number of outputs than the dense -&gt; dense graph.</span>
    <span class="c1"># That&#39;s why we createa a fresh metadata object on the dense -&gt; dense function here,</span>
    <span class="c1"># and plumb it back up to the partitioner.</span>
    <span class="c1"># See Note: [Partitioner handling for Subclasses, Part 2] for more info.</span>
    <span class="n">meta_updated</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
        <span class="n">metadata_fn</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="n">is_train</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">is_train</span><span class="p">,</span>
    <span class="p">)(</span><span class="o">*</span><span class="n">primals_unwrapped</span><span class="p">)</span>

    <span class="n">subclass_meta</span><span class="o">.</span><span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">meta_updated</span>

    <span class="k">return</span> <span class="n">SubclassTracingInfo</span><span class="p">(</span>
        <span class="n">plain_tensor_trace_fn</span><span class="o">=</span><span class="n">fn_to_trace</span><span class="p">,</span>
        <span class="n">plain_tensor_args</span><span class="o">=</span><span class="n">args_unwrapped</span><span class="p">,</span>
        <span class="n">maybe_subclass_meta</span><span class="o">=</span><span class="n">subclass_meta</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># Has the precondition that there</span>
<span class="c1"># are no duplicate arguments in flat_args (e.g., the same Tensor</span>
<span class="c1"># object never shows up twice.  However, two tensor inputs MAY alias</span>
<span class="c1"># the same storage, so long as they have separate TensorImpls.)</span>
<span class="k">def</span> <span class="nf">aot_dispatch_autograd_graph</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
    <span class="c1"># traced_tangents corresponds to the set of outputs in the traced forward that should get grad_outputs in the traced backward.</span>
    <span class="c1"># It includes outputs of the original forward, *and* any updated inputs due to input mutations.</span>
    <span class="c1"># However, it does *not* include any outputs that are aliases of inputs or intermediates, or any metadata-only input mutations.</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">joint_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">traced_tangents</span><span class="p">)</span>

    <span class="n">fn_prepared_for_autograd</span> <span class="o">=</span> <span class="n">fn_prepped_for_autograd</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">joint_fn_to_trace</span> <span class="o">=</span> <span class="n">create_joint</span><span class="p">(</span><span class="n">fn_prepared_for_autograd</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">)</span>

    <span class="n">joint_fn_to_trace</span><span class="p">,</span> <span class="n">updated_joint_inputs</span> <span class="o">=</span> <span class="n">create_functionalized_fn</span><span class="p">(</span>
        <span class="n">joint_fn_to_trace</span><span class="p">,</span>
        <span class="n">joint_inputs</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">subclass_tracing_info</span> <span class="o">=</span> <span class="n">aot_dispatch_subclass</span><span class="p">(</span>
        <span class="n">joint_fn_to_trace</span><span class="p">,</span> <span class="n">updated_joint_inputs</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">fw_only</span><span class="o">=</span><span class="n">flat_fn</span><span class="p">)</span>

    <span class="n">joint_fn_to_trace</span> <span class="o">=</span> <span class="n">subclass_tracing_info</span><span class="o">.</span><span class="n">plain_tensor_trace_fn</span>
    <span class="n">updated_joint_inputs</span> <span class="o">=</span> <span class="n">subclass_tracing_info</span><span class="o">.</span><span class="n">plain_tensor_args</span>
    <span class="n">maybe_subclass_meta</span> <span class="o">=</span> <span class="n">subclass_tracing_info</span><span class="o">.</span><span class="n">maybe_subclass_meta</span>

    <span class="n">fx_g</span> <span class="o">=</span> <span class="n">create_graph</span><span class="p">(</span><span class="n">joint_fn_to_trace</span><span class="p">,</span> <span class="n">updated_joint_inputs</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">)</span>

    <span class="c1"># There should be *NO* mutating ops in the graph at this point.</span>
    <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>

    <span class="c1"># Redundant with the check above, but worth having in case tracing introduced</span>
    <span class="c1"># a fake tensor. Unlikely.</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
    <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
    <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
    <span class="c1"># TODO: in AOTAutograd, we create metadata like _indices_of_inps_to_detach to detect</span>
    <span class="c1"># when we need to manually detach() some inputs in the forward.</span>
    <span class="c1"># Higher order ops might eventually need to do the same.</span>
    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;aot_export_module does not support tensor subclass inputs for now.&quot;</span>
        <span class="k">return</span> <span class="n">fx_g</span>
    <span class="k">return</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">updated_joint_inputs</span><span class="p">,</span> <span class="n">maybe_subclass_meta</span>

<span class="k">def</span> <span class="nf">aot_dispatch_autograd</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
    <span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">,</span> <span class="n">maybe_subclass_meta</span> <span class="o">=</span> <span class="n">aot_dispatch_autograd_graph</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

    <span class="c1"># Copied from aot_dispatch_autograd_graph.</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">enable_log</span><span class="p">:</span>
        <span class="n">aot_joint_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Joint graph&quot;</span><span class="p">,</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">inner_meta</span> <span class="o">=</span> <span class="n">fw_metadata</span> <span class="k">if</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">maybe_subclass_meta</span><span class="o">.</span><span class="n">fw_metadata</span>
        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;joint&quot;</span><span class="p">):</span>
            <span class="c1"># See Note: [Partitioner handling for Subclasses, Part 1]</span>
            <span class="n">num_inner_fwd_outputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">inner_meta</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
                <span class="o">+</span> <span class="n">inner_meta</span><span class="o">.</span><span class="n">num_outputs</span>
                <span class="o">+</span> <span class="n">inner_meta</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
                <span class="o">+</span> <span class="n">inner_meta</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span>
            <span class="p">)</span>
            <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">partition_fn</span><span class="p">(</span>
                <span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">,</span> <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="n">num_inner_fwd_outputs</span>
            <span class="p">)</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># we only need to bookkeep the symints that are saved for bw, not any symints</span>
            <span class="c1"># the user forward might have returned in its own output</span>
            <span class="n">fw_outs_saved_for_bw</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_inner_fwd_outputs</span><span class="p">:]</span>
            <span class="n">num_fw_outs_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_saved_for_bw</span><span class="p">)</span>
            <span class="n">symint_outs_saved_for_bw</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_outs_saved_for_bw</span> <span class="k">if</span> <span class="n">is_sym_node</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>
            <span class="n">inner_meta</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>
            <span class="n">_num_symints_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>

        <span class="c1"># Note [Detaching inputs that never need gradients]</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/97745</span>
        <span class="c1"># Suppose we have a function like this that we want to compile:</span>
        <span class="c1">#</span>
        <span class="c1"># def f(x, y):</span>
        <span class="c1">#     return torch.mul(x, y.detach())</span>
        <span class="c1">#</span>
        <span class="c1"># What gradients should we compute for x and y?</span>
        <span class="c1"># By default, AOTAutograd will compute a gradient for **every** input that requires gradients,</span>
        <span class="c1"># and so we&#39;ll compute:</span>
        <span class="c1">#    x_grad_input = y</span>
        <span class="c1">#    y_grad_input = None</span>
        <span class="c1"># Does this preserve the semantics of eager mode?</span>
        <span class="c1"># Unfortunately, no.</span>
        <span class="c1"># Doing the above will cause autograd to **continue** to backprop the autograd tape</span>
        <span class="c1"># that was generated from constructing y.</span>
        <span class="c1">#</span>
        <span class="c1"># This is **different** from what would have happened in eager mode.</span>
        <span class="c1"># In eager mode, if we backprop through the output of this function, autograd will only traverse</span>
        <span class="c1"># the bit of the autograd tape corresponding to &quot;x&quot;.</span>
        <span class="c1"># In particular, if a user had previously backpropped through y&#39;s autograd tape,</span>
        <span class="c1"># And then they try to backprop through the output of the above function,</span>
        <span class="c1"># then we&#39;ll hit the dreaded &quot;Trying to backward through the graph a second time&quot; error.</span>
        <span class="c1">#</span>
        <span class="c1"># You might think: If autograd sees that a gradient is None, shouldn&#39;t it stop early,</span>
        <span class="c1"># instead of continuing the backprop through the ancestors of that node in the graph?</span>
        <span class="c1">#</span>
        <span class="c1"># Autograd has two passes:</span>
        <span class="c1"># (1) a first pass that traverses the autograd graph and figures out which nodes need to be executed</span>
        <span class="c1"># (2) a second pass that actually goes ahead and executes each node when it becomes ready,</span>
        <span class="c1">#     propagating gradients</span>
        <span class="c1"># By the time we&#39;re executing a node and we see that it produces a None, the set of nodes to execute</span>
        <span class="c1"># is already locked-in.</span>
        <span class="c1">#</span>
        <span class="c1"># The fix: instead, we can recognize statically that the graph we&#39;re compiling will never contribute</span>
        <span class="c1"># gradients to y, and prevent autograd from trying to traverse y&#39;s autograd tape at all.</span>
        <span class="c1"># We can do this by manually detach&#39;ing y before sending it through the `CompiledFunction`.</span>
        <span class="c1">#</span>
        <span class="c1"># Note that this solution is not bulletproof.</span>
        <span class="c1"># It&#39;s possible to construct a case where eager may or may not have have tried to autograd through y,</span>
        <span class="c1"># depending on the actual grad_outputs that were passed in during the backward.</span>
        <span class="c1"># There is no easy fix for this: the simplest fix would be to run with `retain_graph=True`,</span>
        <span class="c1"># allowing autograd to re-use the graph.</span>
        <span class="c1">#</span>
        <span class="c1"># An example of this case is:</span>
        <span class="c1"># def f(x):</span>
        <span class="c1">#     return x.detach() * 2, x * 3</span>
        <span class="c1"># If we were to only backprop through outs[0], in eager, we would stop</span>
        <span class="c1"># If we backward only on the first output, we shouldn&#39;t send a grad through x.</span>
        <span class="c1"># But the custom autograd function doesn&#39;t know that: it will materialize zero grads for x * 3</span>
        <span class="c1"># and we will end up with a zero grad at x.</span>
        <span class="c1"># If we later backprop through the second output, this will also require backprop&#39;ing through x.</span>
        <span class="c1"># Meaning we&#39;ll need to use `retain_graph=True` to be able to backprop through x the second time.</span>
        <span class="n">_indices_of_inps_to_detach</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">bw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">bw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># TODO: we should apply the below &quot;detach inputs if their gradients are statically known to be None&quot;</span>
        <span class="c1"># optimization even if we have subclass inputs/outputs (we do not handle this today).</span>
        <span class="c1"># Computing which our our inputs get None gradients is a bit more complicated,</span>
        <span class="c1"># if any of our inputs are subclasses. Why?</span>
        <span class="c1"># (a) we need to make sure that we call .detach() on the input subclasses, since autograd sees subclasses.</span>
        <span class="c1"># (b) The grad_outputs that we AOT computed in our backward graph are the desugared tensor tensors,</span>
        <span class="c1">#     so we need to figure out which subclass fw inputs they map to.</span>
        <span class="k">if</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bw_outs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">+</span> <span class="n">inner_meta</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">bw_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bw_outs</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">bw_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_indices_of_inps_to_detach</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">enable_log</span><span class="p">:</span>
            <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Forward graph&quot;</span><span class="p">,</span> <span class="n">fw_module</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>
            <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Backward graph&quot;</span><span class="p">,</span> <span class="n">bw_module</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">):</span>
            <span class="c1"># flat_args at this point might still be subclasses-</span>
            <span class="c1"># make sure to pass the unwrapped fake tensors into the compiler!</span>
            <span class="n">adjusted_flat_args</span> <span class="o">=</span> <span class="n">joint_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
                <span class="c1"># Update example inputs for the fw_compiler</span>
                <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">()</span>
                <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
                <span class="n">adjusted_flat_args</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">])</span>
                <span class="c1"># We are not clearing flat_args here because</span>
                <span class="c1"># 1) There is a check in the debug compiler at the end</span>
                <span class="c1"># 2) It does not matter as these are fake tensors</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">():</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span><span class="o">.</span><span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">inner_meta</span>

            <span class="k">with</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">report_output_strides</span><span class="p">()</span> <span class="k">as</span> <span class="n">fwd_output_strides</span><span class="p">:</span>
                <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span>
                    <span class="n">fw_module</span><span class="p">,</span> <span class="n">adjusted_flat_args</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fw_func</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
                <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fw_func</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">maybe_subclass_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Why do we need to pass in num_fw_outs_saved_for_bw?</span>
                <span class="c1"># See Note: [Partitioner handling for Subclasses, Part 2]</span>
                <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_dispatch_subclass_wrapper</span><span class="p">(</span>
                    <span class="n">compiled_fw_func</span><span class="p">,</span>
                    <span class="n">subclass_metas</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_fw_graph_out_meta</span><span class="p">,</span>
                    <span class="n">num_fw_outs_saved_for_bw</span><span class="o">=</span><span class="n">num_fw_outs_saved_for_bw</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fw_func</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
                    <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fw_func</span><span class="p">)</span>

        <span class="c1"># NB: It&#39;s important to compile backwards ahead of time, as this may</span>
        <span class="c1"># add extra guards which we need to apply to the Dynamo cache at</span>
        <span class="c1"># forwards</span>
        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">):</span>
            <span class="n">placeholder_list</span> <span class="o">=</span> <span class="n">fx_placeholder_vals</span><span class="p">(</span><span class="n">bw_module</span><span class="p">)</span>

            <span class="n">forward_saved_for_backwards_strides</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">fwd_output_strides</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">forward_saved_for_backwards_strides</span> <span class="o">=</span> <span class="n">fwd_output_strides</span><span class="p">[</span><span class="n">inner_meta</span><span class="o">.</span><span class="n">tensors_saved_for_backwards_slice</span><span class="p">]</span>

            <span class="c1"># saved activations can have different stride to eager if</span>
            <span class="c1"># the compiler does layout optimization. We should restride the</span>
            <span class="c1"># tensor passed in for compiling the backward graph using the</span>
            <span class="c1"># saved tensor&#39;s stride.</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">placeholder_list</span><span class="p">)):</span>
                <span class="n">ph_arg</span> <span class="o">=</span> <span class="n">placeholder_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ph_arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">continue</span>

                <span class="k">if</span> <span class="n">forward_saved_for_backwards_strides</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">real_stride</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="c1"># Per all_args calling convention</span>
                <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>
                <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">forward_saved_for_backwards_strides</span><span class="p">):</span>
                    <span class="n">real_stride</span> <span class="o">=</span> <span class="n">forward_saved_for_backwards_strides</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">real_stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Comparing ph_arg.stride() with real_stride directly may</span>
                <span class="c1"># cause dynamic dimensions in ph_arg being specialized to static</span>
                <span class="c1"># value. Using the hints to avoid that.</span>
                <span class="k">if</span> <span class="n">_get_hints</span><span class="p">(</span><span class="n">ph_arg</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="o">!=</span> <span class="n">real_stride</span><span class="p">:</span>
                    <span class="c1"># Note that here we use the stride of the real tensor to</span>
                    <span class="c1"># restride a FakeTensor. This does not cause trouble</span>
                    <span class="c1"># for dynamic shape since this code path only get</span>
                    <span class="c1"># executed if layout optimization is enabled. And we</span>
                    <span class="c1"># disable layout optimization for dynamic shape right</span>
                    <span class="c1"># now.</span>
                    <span class="c1">#</span>
                    <span class="c1"># A solution that decide stride order based on real</span>
                    <span class="c1"># tensor&#39;s stride and then apply that stride order to</span>
                    <span class="c1"># the FakeTensor does not work smoothly since some</span>
                    <span class="c1"># tensor&#39;s layout is not &#39;dense&#39;. E.g. mixnet_l has a</span>
                    <span class="c1"># tensor with size [8, 64, 112, 112] and strides</span>
                    <span class="c1"># (2408448, 1, 21504, 192). The solution mentioned will</span>
                    <span class="c1"># decide a stride of (802816, 1, 7168, 64) for this</span>
                    <span class="c1"># tensor which is wrong.</span>
                    <span class="n">placeholder_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ph_arg</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">ph_arg</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">real_stride</span><span class="p">)</span>

            <span class="n">compiled_bw_func</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">):</span>
                <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                <span class="k">with</span> <span class="n">context</span><span class="p">():</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">compiled_bw_func</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">bw_compiler</span><span class="p">(</span>
                            <span class="n">bw_module</span><span class="p">,</span> <span class="n">placeholder_list</span>
                        <span class="p">)</span>
                    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                        <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;failed to eagerly compile backwards for dynamic, suppressing in case backwards not needed&quot;</span><span class="p">,</span>
                            <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span>
                        <span class="p">)</span>

    <span class="n">saved_context</span> <span class="o">=</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">CompiledFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiled_fw_func</span>
        <span class="n">compiled_bw</span> <span class="o">=</span> <span class="n">compiled_bw_func</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="n">fw_metadata</span>
        <span class="n">maybe_subclass_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SubclassMeta</span><span class="p">]</span> <span class="o">=</span> <span class="n">maybe_subclass_meta</span>
        <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">_num_symints_saved_for_bw</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">_compiled_autograd_key</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">,</span> <span class="o">*</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">)</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">deduped_flat_tensor_args</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="n">deduped_flat_tensor_args</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
                <span class="c1"># Add the seed and offset to args</span>
                <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">()</span>
                <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
            <span class="c1"># There is a pretty complicated calling convention around what the compiled fw returns.</span>
            <span class="c1"># The full list of outputs and their relative order is:</span>
            <span class="c1"># (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)</span>
            <span class="c1"># - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version</span>
            <span class="c1">#   of the original view, and not the synthetic base</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_fw</span><span class="p">,</span>
                <span class="n">args</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span>
            <span class="n">num_outputs_aliased</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span>
            <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span>
            <span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
            <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span>
            <span class="p">)</span>
            <span class="n">num_forward_returns</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_forward_returns</span>
            <span class="n">num_forward</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_forward</span>

            <span class="c1"># Partitioners must put symint arguments at the end separate from tensor arguments</span>
            <span class="n">tensors_saved_for_backwards</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">tensors_saved_for_backwards_slice</span>
            <span class="p">]</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span>
            <span class="p">)</span>
            <span class="c1"># See Note [Detaching saved tensors in AOTAutograd]</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_is_view</span><span class="p">()</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span><span class="p">))</span>
            <span class="n">symint_outs</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">symints_saved_for_backwards_slice</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symint_outs</span>
            <span class="p">),</span> <span class="nb">str</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symint_outs</span><span class="p">])</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="n">symint_outs</span>

            <span class="n">raw_returns</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_forward_returns</span><span class="p">]</span>

            <span class="c1"># Wrap all autograd.Function.forward() outputs that are aliases</span>
            <span class="c1"># so that autograd.Function doesn&#39;t treat them as tensors</span>
            <span class="k">if</span> <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
                <span class="p">):</span>
                    <span class="c1"># We could make this faster by only looping over inputs with metadata-only mutations</span>
                    <span class="c1"># (instead of looping over inputs with either data or metadata mutations), but there shouldn&#39;t be many.</span>
                    <span class="n">info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                        <span class="n">raw_returns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorAlias</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
                    <span class="n">user_mutated_inputs_raw</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_mutated_inputs</span><span class="p">]</span>
                    <span class="n">mut_inp_infos</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
                    <span class="p">]</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_mutated_inputs_raw</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mut_inp_infos</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_unsafe_view_outputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">unsafe_view_out_indices</span><span class="p">:</span>
                    <span class="n">raw_return_idx</span> <span class="o">=</span> <span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">idx</span>
                    <span class="n">o</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">]</span>
                    <span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_unsafe_view</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">aliased_out_indices</span><span class="p">:</span>
                    <span class="n">raw_return_idx</span> <span class="o">=</span> <span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">idx</span>
                    <span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorAlias</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
                    <span class="n">intermediates_raw</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span><span class="p">:]</span>
                    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">intermediates_raw</span><span class="p">)</span>

            <span class="c1"># invariant: intermediate bases always require gradients, so we don&#39;t have to</span>
            <span class="c1"># consider marking them as non-differentiable.</span>
            <span class="n">raw_returns_not_including_intermediate_bases</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[:</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span><span class="p">]</span>

            <span class="n">raw_returns_meta</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">]</span>
                <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">output_info</span>
            <span class="p">)</span>

            <span class="n">fw_outs_not_requiring_grad</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_returns_not_including_intermediate_bases</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">raw_returns_meta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">]</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs_not_requiring_grad</span><span class="p">)</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">_materialize_non_diff_grads</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_forward_returns</span><span class="p">:</span><span class="n">num_forward</span><span class="p">],</span>
                <span class="n">return_new_outs</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">)</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="c1"># Calling convention: we expect a grad_out passed to the backward:</span>
            <span class="c1"># - for every output of the fw that does *not* alias an input or graph intermediate</span>
            <span class="c1"># - for every updated_input generated by the fw that does *not* alias an input (aka only data-mutations)</span>
            <span class="c1"># - for every graph intermediate that we need to use to generate an output later.</span>
            <span class="c1"># The other outputs in the autograd.Function.forward that do *not* show up in the backward include:</span>
            <span class="c1"># - outputs that alias inputs or graph intermediates</span>
            <span class="c1"># - updated inputs due to metadata-only mutations.</span>
            <span class="c1"># We need to return them in the forward, but ensure that they all do not get gradients in the backward,</span>
            <span class="c1"># and we filter them out here before passing the remaining grad_outputs into the compiled backward.</span>
            <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
            <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">expected_grad_outs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>

            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected_grad_outs</span>
            <span class="n">out_info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">output_info</span>

            <span class="n">inp_tangents</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">,</span> <span class="n">intermediate_base_tangents</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">flat_args</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_mutated_inps</span><span class="p">],</span>
                <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span><span class="p">:</span><span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">],</span>
                <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">:],</span>
            <span class="p">)</span>
            <span class="c1"># input_info contains info on *every* input,</span>
            <span class="c1"># But in the backward(), we are only given grad outputs for every mutated input</span>
            <span class="c1"># We then need to filter out the grad outputs that correspond to metadata-only mutations or don&#39;t require grad</span>
            <span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
            <span class="n">input_info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_inp_indices</span><span class="p">)</span>
            <span class="n">inp_tangents_filtered</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info_idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inp_tangents</span><span class="p">,</span> <span class="n">mutated_inp_indices</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">input_info</span><span class="p">[</span><span class="n">info_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">input_info</span><span class="p">[</span><span class="n">info_idx</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">]</span>
            <span class="c1"># We also need to filter out grad outputs that correspond to outputs aliasing inputs/intermediates</span>
            <span class="n">out_tangents_filtered</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out_tangents</span><span class="p">,</span> <span class="n">out_info</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">custom_function_view</span><span class="p">]</span>
                <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">info</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">]</span>
            <span class="c1"># intermediate bases always require gradients, and always participate in the backward graph.</span>
            <span class="n">flat_bw_args_with_grads</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">inp_tangents_filtered</span><span class="p">,</span> <span class="o">*</span><span class="n">out_tangents_filtered</span><span class="p">,</span> <span class="o">*</span><span class="n">intermediate_base_tangents</span><span class="p">]</span>
            <span class="n">num_flat_bw_args_with_grads</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_bw_args_with_grads</span><span class="p">)</span>

            <span class="c1"># sanity asserts</span>
            <span class="c1"># metadata_only_inps = [</span>
            <span class="c1">#     x for x, info_idx in zip(inp_tangents, mutated_inp_indices)</span>
            <span class="c1">#     if not input_info[info_idx].mutates_data</span>
            <span class="c1"># ]</span>
            <span class="c1"># aliased_outputs = [</span>
            <span class="c1">#     x for x, info in zip(out_tangents, out_info) if info.output_type != OutputType.non_alias]</span>
            <span class="c1"># assert all(x is None for x in metadata_only_inps)</span>
            <span class="c1"># assert all(x is None for x in aliased_outputs)</span>

            <span class="n">rng_args</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
                <span class="c1"># Add the seed and offset to args</span>
                <span class="n">rng_args</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">()</span>

            <span class="n">all_args</span> <span class="o">=</span> <span class="p">[</span>
                <span class="o">*</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">,</span>
                <span class="o">*</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">flat_bw_args_with_grads</span><span class="p">,</span>
                <span class="o">*</span><span class="n">rng_args</span>
            <span class="p">]</span>
            <span class="k">del</span> <span class="n">flat_bw_args_with_grads</span>

            <span class="n">tangents_start_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_args</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_flat_bw_args_with_grads</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">rng_args</span><span class="p">)</span>
            <span class="n">tangents_end_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_args</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">rng_args</span><span class="p">)</span>

            <span class="c1"># Note: [AOTAutograd Backward Guards]</span>
            <span class="c1"># During AOTDispatch, we eagerly create and trace out a joint fw-bw graph.</span>
            <span class="c1"># Doing so requires us to &quot;guess&quot; about some of the metadata of our grad_outputs.</span>
            <span class="c1">#</span>
            <span class="c1"># In particular: if an output to the forward is a plain tensor or a subclass,</span>
            <span class="c1"># its corresponding grad_output in the backward **may or may not** be</span>
            <span class="c1"># a plain tensor or a subclass. The main cases are:</span>
            <span class="c1"># (1) If an output is a plain tensor, its grad_out will also be a plain tensor,</span>
            <span class="c1">#     *unless* the output is used in some subclass compute later in the forward graph,</span>
            <span class="c1">#     which will cause its grad_output to become a subclass</span>
            <span class="c1"># (2) If an output is a subclass, its grad_out will also be a subclass,</span>
            <span class="c1">#     *unless* the output of the forward did not actually participate in the gradient computation,</span>
            <span class="c1">#     in which case autograd will insert a plain tensor of zeros for the grad_output.</span>
            <span class="c1">#     We could avoid this case with `torch.autograd.Function.set_materialize_grads`,</span>
            <span class="c1">#     although this is not turned on today in AOTAutgrad and would require more work.</span>
            <span class="c1">#</span>
            <span class="c1"># Today, we make a guess on subclass-ness based on the above examples,</span>
            <span class="c1"># and hard-error in the backward if we guessed wrong.</span>
            <span class="c1">#</span>
            <span class="c1"># In the future, we should add backward guards that would allow us to</span>
            <span class="c1"># properly handle this case instead of erroring: we would need to retrace the backward graph,</span>
            <span class="c1"># since we might produce an entirely different trace if our grad_outputs are subclass or not.</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">output_types</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_flat_bw_args_with_grads</span>
            <span class="n">grad_output_types</span> <span class="o">=</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">all_args</span><span class="p">[</span><span class="o">-</span><span class="n">num_flat_bw_args_with_grads</span><span class="p">:]]</span>
            <span class="c1"># In general, we can add more asserts/guards here for when we partitioned</span>
            <span class="c1"># with incorrect assumptions about the grad_outputs.</span>
            <span class="c1"># Normalize FakeTensor -&gt; torch.Tensor</span>
            <span class="c1"># - during tracing our types are FakeTensor</span>
            <span class="c1"># - at runtime in the backward our types are torch.Tensor...</span>
            <span class="c1"># - unless we&#39;re running compiled backward, in which case they are also FakeTensor</span>
            <span class="n">grad_output_types_</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">FakeTensor</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grad_output_types</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">grad_output_types_</span> <span class="o">==</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">output_types</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">We incorrectly attempted to compile the backward with incorrect subclass metadata.</span>
<span class="s2">If you run into this error, please file an issue.</span>
<span class="s2">Expected grad_output types: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">output_types</span><span class="p">)</span><span class="si">}</span>
<span class="s2">Got grad_output types: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">grad_output_types</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>

            <span class="c1"># TODO: figure out how to refactor the backward properly so I can use aot_dispatch_subclass_wrapper() here.</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">maybe_subclass_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Get the number of tangents after unwrapping</span>
                <span class="n">len_tangents</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">unwrap_tensor_subclasses</span><span class="p">(</span>
                    <span class="n">all_args</span><span class="p">[</span><span class="n">tangents_start_idx</span><span class="p">:</span> <span class="n">tangents_end_idx</span><span class="p">],</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">))</span>
                <span class="n">all_args</span> <span class="o">=</span> <span class="n">unwrap_tensor_subclasses</span><span class="p">(</span><span class="n">all_args</span><span class="p">,</span> <span class="n">is_joint_structure</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">tangents_start_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_args</span><span class="p">)</span> <span class="o">-</span> <span class="n">len_tangents</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">rng_args</span><span class="p">)</span>
                <span class="n">tangents_end_idx</span> <span class="o">=</span> <span class="n">tangents_start_idx</span> <span class="o">+</span> <span class="n">len_tangents</span>

            <span class="c1"># Make the tangents contiguous. Note that we must do this after subclass desugaring</span>
            <span class="c1"># because inputs to inductor have to be contiguous</span>
            <span class="n">all_args</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="n">tangents_start_idx</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tangents_end_idx</span> <span class="k">else</span> <span class="n">t</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_args</span><span class="p">)</span>
            <span class="p">]</span>

            <span class="k">def</span> <span class="nf">call_compiled_backward</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">_is_compiled_autograd_tracing</span><span class="p">():</span>
                    <span class="c1"># For compiled autograd, run raw FX graph so that it can be inlined into the larger graph</span>
                    <span class="n">symints</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">_get_compiled_autograd_symints</span><span class="p">()</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">symints</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">)</span>
                    <span class="n">all_args</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">symints</span><span class="p">)]</span> <span class="o">=</span> <span class="n">symints</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                    <span class="k">with</span> <span class="n">context</span><span class="p">():</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">bw_module</span><span class="p">(</span><span class="o">*</span><span class="n">all_args</span><span class="p">))</span>
                    <span class="n">out</span> <span class="o">=</span> <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
                    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">maybe_clear_saved_tensors</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                    <span class="k">with</span> <span class="n">tracing</span><span class="p">(</span><span class="n">saved_context</span><span class="p">),</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">):</span>
                        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">bw_compiler</span><span class="p">(</span>
                            <span class="n">bw_module</span><span class="p">,</span> <span class="n">placeholder_list</span>
                        <span class="p">)</span>

                <span class="n">out</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span><span class="p">,</span>
                    <span class="n">all_args</span><span class="p">,</span>
                    <span class="n">steal_args</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">out</span> <span class="o">=</span> <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">all_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
                <span class="c1"># Ensure that the graph is connected, and error if double backward is performed.</span>
                <span class="c1"># See comment for why once_differentiable is not sufficient:</span>
                <span class="c1"># https://github.com/pytorch/pytorch/pull/92348/files#r1072962107</span>
                <span class="k">class</span> <span class="nc">CompiledFunctionBackward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
                    <span class="nd">@staticmethod</span>
                    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">unused_args</span><span class="p">):</span>
                        <span class="n">outs</span> <span class="o">=</span> <span class="n">call_compiled_backward</span><span class="p">()</span>
                        <span class="c1"># TODO: figure out how to refactor the backward properly so I can use aot_dispatch_subclass_wrapper() here.</span>
                        <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">maybe_subclass_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">outs_wrapped</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses</span><span class="p">(</span>
                                <span class="n">outs</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">maybe_subclass_metadata</span><span class="o">.</span><span class="n">grad_input_metas</span><span class="p">)</span>
                            <span class="k">return</span> <span class="n">outs_wrapped</span>
                        <span class="k">return</span> <span class="n">outs</span>

                    <span class="nd">@staticmethod</span>
                    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;torch.compile with aot_autograd does not currently support double backward&quot;</span><span class="p">)</span>

                <span class="n">CompiledFunctionBackward</span><span class="o">.</span><span class="n">_compiled_autograd_key</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">_compiled_autograd_key</span>

                <span class="c1"># Pass args even though they&#39;re unused, so that the graph is built</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">CompiledFunctionBackward</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="n">all_args</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">call_compiled_backward</span><span class="p">()</span>

            <span class="c1"># TODO: figure out how to refactor the backward properly so I can use aot_dispatch_subclass_wrapper() here.</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">maybe_subclass_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">outs_wrapped</span> <span class="o">=</span> <span class="n">wrap_tensor_subclasses</span><span class="p">(</span>
                    <span class="n">out</span><span class="p">,</span> <span class="n">subclass_metas</span><span class="o">=</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">maybe_subclass_metadata</span><span class="o">.</span><span class="n">grad_input_metas</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">outs_wrapped</span>
            <span class="k">return</span> <span class="n">out</span>

    <span class="n">compiled_function</span> <span class="o">=</span> <span class="n">create_runtime_wrapper</span><span class="p">(</span>
        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">indices_of_inps_to_detach</span><span class="o">=</span><span class="n">_indices_of_inps_to_detach</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiled_function</span>

    <span class="n">flat_requires_grad</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span>
    <span class="p">]</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_function</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debug_compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># TODO: Check aliasing relationships</span>
        <span class="c1"># TODO: Check strides for metadata mutation</span>
        <span class="c1"># (NB: ideally, this logic is factored out of this function and</span>
        <span class="c1"># you move these debug checks there)</span>

        <span class="c1"># Check requires grad.  Bad case is when we compiled with</span>
        <span class="c1"># requires_grad = False, but input requires_grad = True</span>
        <span class="c1"># (vice versa is OK; we compute a gradient and then throw</span>
        <span class="c1"># it away when it hits the input.)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">can_require_grad</span> <span class="o">=</span> <span class="n">flat_requires_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">can_require_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">can_require_grad</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                    <span class="n">aot_config</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would not require grad&quot;</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">debug_compiled_function</span>


<span class="nd">@dynamo_timed</span>
<span class="k">def</span> <span class="nf">create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graphs of the attr:`flat_fn` to generate a</span>
<span class="sd">    joint graph. The joint graph is an Fx graph with Aten ops. Please refer to</span>
<span class="sd">    the tracing mechanism to understand the graph capturing details.</span>

<span class="sd">    The joint graph is then passed through attr:`partition_fn` to isolate the</span>
<span class="sd">    forward and backward portions, which are then respectively compiled via the</span>
<span class="sd">    provided attr:`fw_compiler` and attr:`bw_compiler`.</span>

<span class="sd">    The resulting compiled forward and backward graphs are then wrapped up in a</span>
<span class="sd">    ``torch.autograd.Function`` object.</span>

<span class="sd">    The calling convention here is that the first aot_config.num_params_buffers</span>
<span class="sd">    inputs in flat_args are parameters and buffers, and the rest are inputs.</span>

<span class="sd">    We use this to assume that parameters/buffer&#39;s shapes don&#39;t change.</span>

<span class="sd">    Note: this function is used both by aot_function and aot_export (controlled by aot_config.is_export)</span>
<span class="sd">        When aot_config.is_export is True, we return an FX graph + metadata</span>
<span class="sd">        When aot_config.is_export is False, we return an ordinary runtime function</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is the main entry point.</span>
    <span class="c1"># TODO: Chillee argues that dynamo itself should pass in fake tensors to</span>
    <span class="c1"># the list of arguments when compiling; at the moment we do not do this</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{}</span>


    <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">aot_autograd_decompositions</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
        <span class="c1"># Update the decompositions with functionalized random decompositions</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="o">**</span><span class="n">rng_decompositions</span><span class="p">,</span>
            <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># Check flat_args to see if they&#39;re already fake.  If so, use that fake</span>
    <span class="c1"># mode instead.</span>

    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">dynamic_shapes</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span>

    <span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">enable_python_dispatcher</span><span class="p">()</span> <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_multithreading_enabled</span><span class="p">(</span>
        <span class="kc">False</span>
    <span class="p">),</span> <span class="n">preserve_rng_state</span><span class="p">(),</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">python_dispatcher_mode</span><span class="p">,</span> <span class="n">PhiloxStateTracker</span><span class="p">():</span>

        <span class="k">def</span> <span class="nf">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">torch._dynamo.source</span> <span class="kn">import</span> <span class="n">ConstantSource</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                        <span class="n">source</span> <span class="o">=</span> <span class="n">ConstantSource</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sym_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="k">return</span> <span class="n">shape_env</span><span class="o">.</span><span class="n">create_symintnode</span><span class="p">(</span>
                            <span class="n">shape_env</span><span class="o">.</span><span class="n">create_symbol</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">),</span>
                            <span class="n">hint</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                            <span class="n">source</span><span class="o">=</span><span class="n">source</span>
                        <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">x</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span>
                    <span class="k">return</span> <span class="n">x</span>
                <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                    <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="n">FakeTensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">):</span>
                        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">)</span>
                        <span class="k">return</span> <span class="n">x</span>
                <span class="c1"># TODO: Ensure that this codepath is never exercised from</span>
                <span class="c1"># Dynamo</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
                    <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">static_weight_shapes</span>
                <span class="p">):</span>
                    <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">return</span> <span class="p">[</span><span class="n">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)]</span>

        <span class="n">fake_flat_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="n">needs_autograd</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fake_flat_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="c1"># Patch set_rng_state as set_rng_state with fake tensors is</span>
            <span class="c1"># nonsensical. This does not affect the collection of metadata.</span>
            <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="kc">None</span><span class="p">):</span>
                <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                    <span class="n">flat_fn</span><span class="p">,</span>
                    <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">needs_autograd</span><span class="p">,</span>
                    <span class="n">is_train</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">,</span>
                <span class="p">)(</span><span class="o">*</span><span class="n">fake_flat_args</span><span class="p">)</span>

                <span class="n">req_subclass_dispatch</span> <span class="o">=</span> <span class="n">requires_subclass_dispatch</span><span class="p">(</span><span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">needs_autograd</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">):</span>
                    <span class="c1"># We realized that none of the outputs require grad,</span>
                    <span class="c1"># so we actually have an inference graph.</span>
                    <span class="n">needs_autograd</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="c1"># A bit silly: right now in the subclass codepath, our ViewAndMutationMeta</span>
                    <span class="c1"># changes depending on whether we pass in is_train / keep_input_mutations,</span>
                    <span class="c1"># so we&#39;re forced to recompute the metadata.</span>
                    <span class="c1"># TODO: refactor the subclass path of run_functionalized_fw_and_collect_metadata</span>
                    <span class="c1"># so that this is unnecessary.</span>
                    <span class="k">if</span> <span class="n">req_subclass_dispatch</span><span class="p">:</span>
                        <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                            <span class="n">flat_fn</span><span class="p">,</span>
                            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">needs_autograd</span><span class="p">,</span>
                            <span class="n">is_train</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">,</span>
                        <span class="p">)(</span><span class="o">*</span><span class="n">fake_flat_args</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
                            <span class="n">input_info</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span>
                            <span class="n">output_info</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">,</span>
                            <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
                            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">needs_autograd</span><span class="p">,</span>
                            <span class="n">traced_tangents</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span>
                            <span class="n">subclass_inp_meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_inp_meta</span><span class="p">,</span>
                            <span class="n">subclass_fw_graph_out_meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_fw_graph_out_meta</span><span class="p">,</span>
                            <span class="n">subclass_tangent_meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_tangent_meta</span><span class="p">,</span>
                            <span class="n">is_train</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">,</span>
                        <span class="p">)</span>


        <span class="k">if</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">req_subclass_dispatch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">torch.compile is currently being used with tensor subclass inputs:</span>
<span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">fake_flat_args</span><span class="p">])</span><span class="si">}</span><span class="s2">. We are attempting to a compile a graph with two graph outputs</span>
<span class="s2">that alias one another, which is currently unsupported in the subclass use case. If you run into this,</span>
<span class="s2">please file a github issue&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
            <span class="c1"># aot_export: ban input metadata mutations for now to keep shared code paths simpler.</span>
            <span class="c1"># Keeping .resize_() in the graph will require some work</span>
            <span class="c1"># Allowing it but keeping the graph functional will require some calling convention changes.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found an input that received a metadata mutation, through e.g. a call to `.resize_()` or `.transpose_()`.</span>
<span class="s2">This is currently banned in the aot_export workflow. If you need this functionality, please file a github issue.</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
            <span class="c1"># In export, banning data mutations on inputs that require grad for now.</span>
            <span class="c1"># This should be rare, and is tricky to get right. When we trace the backward,</span>
            <span class="c1"># we currently trace with autograd.grad instead of .backward(), which makes it difficult</span>
            <span class="c1"># to ensure that we run autograd all the way through the input **before** it saw the mutation.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found a graph input that requires gradients, and received a mutation.</span>
<span class="s2">This is currently banned in the aot_export workflow. If you need this functionality, please file a github issue.</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">req_subclass_dispatch</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">aot_export is not currently supported with traceable tensor subclass.</span>
<span class="s2">If you need this feature, please comment on &lt;CREATE_ISSUE_LINK&gt;&quot;&quot;&quot;</span><span class="p">)</span>

            <span class="c1"># Need to decide on a strategy for functionalized RNG: toggling via global config seems bad,</span>
            <span class="c1"># and turning it on will require a non-trivial calling convention change for any export runtime.</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Functionalized RNG is not currently supported in the aot_export workflow. Please file a github issue,</span>
<span class="s2">or otherwise set torch._functorch.config.functionalize_rng_ops = False.&quot;&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># crappy version of dispatcher</span>
        <span class="c1"># TODO: Do this properly</span>
        <span class="k">if</span> <span class="n">needs_autograd</span><span class="p">:</span>
            <span class="c1"># For now, aot_dispatch_autograd knows to explicitly return a graph</span>
            <span class="c1"># when run with export, and an opaque callable otherwise.</span>
            <span class="c1"># In theory we could factor these out, but I wanted to let the dust</span>
            <span class="c1"># settle on how functionalized rng fits into export first.</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_autograd_graph</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span> <span class="k">else</span> <span class="n">aot_dispatch_autograd</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># aot_dispatch_base_graph contains only the &quot;graph bits&quot;, while aot_dispatch_base</span>
            <span class="c1"># includes some extra work around handling a runtime epilogue.</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_base_graph</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span> <span class="k">else</span> <span class="n">aot_dispatch_base</span>

        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_wrapper_synthetic_base</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="o">=</span><span class="n">compiler_fn</span><span class="p">,</span> <span class="n">needs_autograd</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">)</span>
        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_wrapper_dedupe</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="o">=</span><span class="n">compiler_fn</span><span class="p">)</span>
        <span class="c1"># You can put more passes here</span>

        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>

            <span class="n">mutated_user_inp_locs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">idx</span> <span class="o">-</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_user_inp_locs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Found following user inputs located at </span><span class="si">{</span><span class="n">mutated_user_inp_locs</span><span class="si">}</span><span class="s2"> are mutated. This is currently banned in the aot_export workflow.</span>
<span class="s2">If you need this functionality, please file a github issue.</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

            <span class="c1"># During export, we don&#39;t get back a callable - we get back the raw fx graph</span>
            <span class="c1"># (either a joint or an inference-only graph)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">,</span> <span class="n">fw_metadata</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="c1"># Inspired by autodidax (thanks!)</span>
<span class="k">class</span> <span class="nc">PytreeThunk</span><span class="p">:</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># These are some kinda dumb microoptimizations that save about 3-4 us of overhead.</span>
    <span class="n">is_simple</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>  <span class="c1"># if the output spec is a tuple/list, we won&#39;t bother unflattening it.</span>
    <span class="p">)</span>
    <span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># if the output spec is a LeafSpec</span>

    <span class="k">def</span> <span class="nf">set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">==</span> <span class="n">spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">=</span> <span class="n">spec</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">children_specs</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">create_functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_spec</span><span class="p">,</span> <span class="n">params_len</span><span class="p">):</span>
    <span class="c1"># Redundant with dynamo, but worth having in case this gets invoked elsewhere.</span>
    <span class="c1"># https://github.com/pytorch/pytorch/issues/103569</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">stateless</span><span class="o">.</span><span class="n">_reparametrize_module</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">args</span><span class="p">[:</span><span class="n">params_len</span><span class="p">],</span> <span class="n">params_spec</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">(),</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                        <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;Anomaly Detection has been enabled.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">(</span><span class="n">check_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">Interpreter</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Graph output must be a tuple(). This is so that we can avoid &quot;</span>
                <span class="s2">&quot;pytree processing of the outputs. Please change the module to &quot;</span>
                <span class="s2">&quot;have tuple outputs or use aot_module instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
    <span class="k">return</span> <span class="n">functional_call</span>

<span class="c1"># Creates a function that returns flattened inputs and outputs</span>
<span class="c1"># Also returns the output tree spec, which is needed to recover the &quot;unflattened&quot;</span>
<span class="c1"># output tree structure later.</span>
<span class="k">def</span> <span class="nf">create_tree_flattened_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">PytreeThunk</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Save the args_spec for flat_tensor_args to unflatten while tracing</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">tensor_args_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="n">out_spec</span> <span class="o">=</span> <span class="n">PytreeThunk</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="c1"># The input are flattened tensor args. Prepare the args in the</span>
        <span class="c1"># order that original function expects. Add static args as well.</span>
        <span class="c1"># They will appear as tensor constants in the traced graph.</span>
        <span class="k">nonlocal</span> <span class="n">out_spec</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">tensor_args_spec</span><span class="p">)</span>
        <span class="n">tree_out</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">flat_out</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tree_out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flat_out</span><span class="p">:</span>
            <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">KNOWN_TYPES</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
                    <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_known_type</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2"> in output, which is not a known type. &quot;</span>
                    <span class="s2">&quot;If this type holds tensors, you need to register a pytree for it. &quot;</span>
                    <span class="s2">&quot;See https://github.com/pytorch/functorch/issues/475 for a brief &quot;</span>
                    <span class="s2">&quot;explanation why. If you don&#39;t need to register a pytree, please &quot;</span>
                    <span class="s2">&quot;leave a comment explaining your use case and we&#39;ll make this more &quot;</span>
                    <span class="s2">&quot;ergonomic to deal with&quot;</span>
                <span class="p">)</span>
        <span class="n">out_spec</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">flat_out</span>
    <span class="k">return</span> <span class="n">flat_fn</span><span class="p">,</span> <span class="n">out_spec</span>

<span class="k">def</span> <span class="nf">_graph_input_names</span><span class="p">(</span><span class="n">gm</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_graph_output_names</span><span class="p">(</span><span class="n">gm</span><span class="p">):</span>
    <span class="n">output_node</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">)))</span>
    <span class="k">assert</span> <span class="n">output_node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">return_args</span> <span class="o">=</span> <span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">return_arg</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">return_arg</span> <span class="ow">in</span> <span class="n">return_args</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">create_graph_signature</span><span class="p">(</span>
    <span class="n">fx_g</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">in_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">,</span>
    <span class="n">out_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">user_args_flat</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">params_and_buffers_flat</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">param_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">buffer_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">num_user_fw_outs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">loss_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphSignature</span><span class="p">:</span>

    <span class="c1"># Retrieve graph input names</span>
    <span class="n">graph_input_names</span> <span class="o">=</span> <span class="n">_graph_input_names</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
    <span class="c1"># Retrieve graph output names</span>
    <span class="n">graph_output_names</span> <span class="o">=</span> <span class="n">_graph_output_names</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>

    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer_names</span><span class="p">)</span>
    <span class="c1"># We have enough restrictions on the graph (no de-duping, synthetic bases, etc),</span>
    <span class="c1"># Such that # graph inps = # user inps + # params + # buffers</span>
    <span class="n">num_user_args</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">graph_input_names</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_params_buffers</span>

    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">num_user_fw_outs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">num_fw_outs</span> <span class="o">=</span> <span class="n">num_user_fw_outs</span> <span class="o">+</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
        <span class="n">backward_output_names</span> <span class="o">=</span> <span class="n">graph_output_names</span><span class="p">[</span><span class="n">num_fw_outs</span><span class="p">:]</span>

        <span class="n">grad_index</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">gradients_to_parameters</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">backward_output_names</span><span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="n">grad_index</span><span class="p">)]:</span> <span class="n">param_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">}</span>

        <span class="n">gradients_to_user_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">backward_output_names</span><span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="n">grad_index</span><span class="p">)]:</span> <span class="n">graph_input_names</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">user_input</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">user_args_flat</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">}</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients_to_parameters</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients_to_user_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">backward_output_names</span>
        <span class="p">)</span>

        <span class="c1"># Check that we have fully accounted for all graph outputs</span>
        <span class="n">backward_signature</span> <span class="o">=</span> <span class="n">BackwardSignature</span><span class="p">(</span>
            <span class="n">gradients_to_parameters</span><span class="p">,</span>
            <span class="n">gradients_to_user_inputs</span><span class="p">,</span>
            <span class="n">graph_output_names</span><span class="p">[</span><span class="n">loss_index</span><span class="p">],</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">backward_signature</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">num_user_fw_outs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">graph_output_names</span><span class="p">)</span> <span class="o">-</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>

    <span class="k">return</span> <span class="n">GraphSignature</span><span class="o">.</span><span class="n">from_tracing_metadata</span><span class="p">(</span>
        <span class="n">in_spec</span><span class="o">=</span><span class="n">in_spec</span><span class="p">,</span>
        <span class="n">out_spec</span><span class="o">=</span><span class="n">out_spec</span><span class="p">,</span>
        <span class="n">graph_input_names</span><span class="o">=</span><span class="n">graph_input_names</span><span class="p">,</span>
        <span class="n">graph_output_names</span><span class="o">=</span><span class="n">graph_output_names</span><span class="p">,</span>
        <span class="n">view_mutation_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">named_parameters</span><span class="o">=</span><span class="n">param_names</span><span class="p">,</span>
        <span class="n">named_buffers</span><span class="o">=</span><span class="n">buffer_names</span><span class="p">,</span>
        <span class="n">num_user_inputs</span><span class="o">=</span><span class="n">num_user_args</span><span class="p">,</span>
        <span class="n">num_user_outputs</span><span class="o">=</span><span class="n">num_user_fw_outs</span><span class="p">,</span>
        <span class="n">loss_index</span><span class="o">=</span><span class="n">loss_index</span><span class="p">,</span>
        <span class="n">backward_signature</span><span class="o">=</span><span class="n">backward_signature</span><span class="p">,</span>
    <span class="p">)</span>

<div class="viewcode-block" id="aot_function"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_function.html#functorch.compile.aot_function">[docs]</a><span class="k">def</span> <span class="nf">aot_function</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Whether or not to trace with dynamic shapes</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enable_log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`fn` using torch dispatch</span>
<span class="sd">    mechanism, and then compiles the generated forward and backward graphs</span>
<span class="sd">    through :attr:`fw_compiler` and :attr:`bw_compiler`.</span>

<span class="sd">    :func:`aot_function` traces the forward and backward graph ahead of time,</span>
<span class="sd">    and generates a joint forward and backward graph.  :attr:`partition_fn` is</span>
<span class="sd">    then used to separate out forward and backward graphs. The partitioner</span>
<span class="sd">    function can be used to perform optimizations such as recomputation. One can</span>
<span class="sd">    set `decompositions` dictionary to decompose the operators into a sequence</span>
<span class="sd">    of core or simpler operators supported by the backend compilers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): A Python function that takes one ore more arguments. Must</span>
<span class="sd">            return one or more Tensors.</span>
<span class="sd">        fw_compiler (Callable): A Python function that accepts an Fx graph with</span>
<span class="sd">            Aten ops and input args, and returns a Callable that semantically is</span>
<span class="sd">            equivalent to the input Fx graph.</span>
<span class="sd">        bw_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph.  Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">        partition_fn (Callable): A Python function that takes a joint forward</span>
<span class="sd">            and backward graph, and partitions it into separate forward and</span>
<span class="sd">            backward graphs.</span>
<span class="sd">        decompositions (Dict): A dictionary to define the decomposition of</span>
<span class="sd">            larger Aten ops into simpler or core Aten ops.</span>
<span class="sd">        inference_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph. inference_compiler is invoked</span>
<span class="sd">            if no autograd is needed. Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`fn`, but with forward and backward graph compiled via</span>
<span class="sd">        :attr:`fw_compile` and :attr:`bw_compile`.</span>

<span class="sd">    A simple example usage of :func:`aot_function` is as follows. This example</span>
<span class="sd">    will print the forward and backward graphs of the function ``fn``</span>

<span class="sd">        &gt;&gt;&gt; fn = lambda x : x.sin().cos()</span>
<span class="sd">        &gt;&gt;&gt; def print_compile_fn(fx_module, args):</span>
<span class="sd">        &gt;&gt;&gt;     print(fx_module)</span>
<span class="sd">        &gt;&gt;&gt;     return fx_module</span>
<span class="sd">        &gt;&gt;&gt; aot_fn = aot_function(fn, print_compile_fn)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(4, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; aot_fn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="k">if</span> <span class="n">inference_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inference_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="n">inference_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_export</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">no_tangents</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">enable_log</span><span class="o">=</span><span class="n">enable_log</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">cached_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">returned_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">cached_res</span>
        <span class="c1"># Now flatten the tensor args</span>
        <span class="n">flat_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Compile the function and save it in the cache</span>
        <span class="k">if</span> <span class="n">cached_res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">flat_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">create_tree_flattened_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="n">flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cached_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">out_spec</span><span class="p">)</span>

        <span class="n">cached_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">cached_res</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cached_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returned_function</span></div>


<div class="viewcode-block" id="aot_module"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_module.html#functorch.compile.aot_module">[docs]</a><span class="k">def</span> <span class="nf">aot_module</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`mod` using torch dispatch</span>
<span class="sd">    tracing mechanism. It is wrapper function, that underneath uses</span>
<span class="sd">    :func:`aot_function` to perform tracing and compilation.</span>

<span class="sd">    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs</span>
<span class="sd">    to a new callable which is then compiled through :func:`aot_function`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (Callable): A ``nn.Module`` module.</span>
<span class="sd">        args : args to be passed to :func:`aot_function`</span>
<span class="sd">        kwargs : kwargs to be passed to :func:`aot_function`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``nn.Module`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`mod`, but with forward and backward graph compiled.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="n">named_params</span><span class="p">,</span> <span class="n">named_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">named_params</span><span class="p">,</span> <span class="o">**</span><span class="n">named_buffers</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_and_buffers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">named_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_params</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>
    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span> <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="k">class</span> <span class="nc">AOTModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orig_module</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="n">named_params</span><span class="p">,</span>
                <span class="n">named_buffers</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">AOTModule</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">aot_module_simplified</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the simplified or low overhead version of aot_module. For frontends</span>
<span class="sd">    like TorchDynamo, the input functions/modules to AOT are static and have</span>
<span class="sd">    unpacked inputs/outputs. This gives us an opportunity to remove the</span>
<span class="sd">        (1) pytree overhead to parse inputs/outputs,</span>
<span class="sd">        (2) AOT Autograd cache,</span>
<span class="sd">        (3) Reading of params/buffers in every forward call</span>

<span class="sd">    :func:`aot_module_simplified` removes these overheads.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="n">params_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params_flat</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="n">functional_call</span> <span class="o">=</span> <span class="n">create_functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_spec</span><span class="p">,</span> <span class="n">params_len</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="k">if</span> <span class="n">inference_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inference_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>

    <span class="n">seen_sources</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># First, the params</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span><span class="o">.</span><span class="n">params_flat</span> <span class="o">=</span> <span class="n">params_flat</span>

    <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Then, the params 1:1 mapped sources, if relevant.</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;_param_name_to_source&quot;</span><span class="p">):</span>
        <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># We now know this came from dynamo, and (1) we care about guards,</span>
        <span class="c1"># so setting up aot_autograd_arg_pos_to_source for downstream dedup guards</span>
        <span class="c1"># can now be done safely. (2) Dynamo logic protects the 1:1 sizing below.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_param_name_to_source</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> not found.&quot;</span>
            <span class="n">source</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">_param_name_to_source</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">source</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_sources</span><span class="p">,</span> <span class="n">source</span>
            <span class="n">seen_sources</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
            <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

    <span class="c1"># Next, the input args</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">):</span>
        <span class="c1"># Non dynamo entrypoints can get to here...</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="s2">&quot;_dynamo_source&quot;</span><span class="p">):</span>
                    <span class="c1"># ... but not here!</span>
                    <span class="k">if</span> <span class="n">aot_autograd_arg_pos_to_source</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">source</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_dynamo_source</span>
                    <span class="k">assert</span> <span class="n">source</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_sources</span><span class="p">,</span> <span class="n">source</span>
                    <span class="n">seen_sources</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
                    <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">aot_autograd_arg_pos_to_source</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">)</span>

    <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">full_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
            <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">break</span>

    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="n">inference_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">,</span>
        <span class="n">is_export</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">no_tangents</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">compiled_autograd</span><span class="o">.</span><span class="n">disable</span><span class="p">():</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
            <span class="n">functional_call</span><span class="p">,</span>
            <span class="n">full_args</span><span class="p">,</span>
            <span class="n">aot_config</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># TODO: There is something deeply wrong here; compiled_fn running with</span>
    <span class="c1"># the boxed calling convention, but aot_module_simplified somehow</span>
    <span class="c1"># historically returned a function that was not the boxed calling</span>
    <span class="c1"># convention.  This should get fixed...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">runtime_args</span><span class="p">):</span>
        <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span>

    <span class="c1"># Just for convenience</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_buffers</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span>

    <span class="k">return</span> <span class="n">forward</span>

<span class="k">def</span> <span class="nf">aot_export_module</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># If true, we&#39;ll return a joint forward-backward graph,</span>
    <span class="c1"># As well as metadata on the loss + gradients in the backward.</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="c1"># If trace_joint is True, we expect your module to return a scalar loss.</span>
    <span class="c1"># Your module can return multiple outputs, so you must specify which output the loss is.</span>
    <span class="n">output_loss_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">GraphSignature</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function takes in a module, and returns:</span>
<span class="sd">    (1) an FX graph that can be exported</span>
<span class="sd">    (2) some metadata about the graph</span>

<span class="sd">    If `trace_joint=True` we will return a joint graph of the forward + backward.</span>

<span class="sd">    The traced FX graph will have the following properties compared to the original module:</span>
<span class="sd">    (1) Inputs and outputs to the module will be pytree-flattened</span>
<span class="sd">    (2) Parameters and buffers on the module will be lifted into graph inputs,</span>
<span class="sd">        graph_inputs = (*parameters, *buffers, *user_inputs)</span>
<span class="sd">    (3) The graph will be fully functionalized</span>
<span class="sd">    (4) Any input mutations will be converted into additional outputs in the graph,</span>
<span class="sd">        meaning whoever calls this graph is responsible for applying the mutations</span>
<span class="sd">        back to the original inputs.</span>
<span class="sd">    (5) If is_joint is provided the graph will return parameter gradients in addition to user outputs.</span>
<span class="sd">        The graph output will look like:</span>
<span class="sd">        graph_outputs = (*updated_inputs, *user_outputs, *param_gradients)</span>

<span class="sd">    There are also several restrictions on what modules can use this API. In particular:</span>
<span class="sd">    (1) If trace_joint is specified, we expect the loss function to be **fused**</span>
<span class="sd">        into the module forward. One of the outputs to the forward must be a scalar loss,</span>
<span class="sd">        which is specified with `output_loss_index`.</span>
<span class="sd">        All other outputs to the forward are presumed to not require gradients.</span>
<span class="sd">    (2) This API cannot capture optimizers (although in theory we could build an API for this).</span>
<span class="sd">    (3) Metadata mutations on params/buffers/inputs are banned.</span>
<span class="sd">    (4) Data mutations on anything that requires gradients are banned (parameters)</span>
<span class="sd">    (5) If an input is mutated, it is not allowed to alias any other inputs.</span>
<span class="sd">    (6) Parameters must not be duplicated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">named_parameters</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">params_and_buffers_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params_and_buffers</span><span class="p">)</span>
    <span class="n">params_and_buffers_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>

    <span class="n">functional_call</span> <span class="o">=</span> <span class="n">create_functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_spec</span><span class="p">,</span> <span class="n">params_len</span><span class="p">)</span>

    <span class="n">num_fw_outs</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="c1"># This helper effectively just adds some extra asserts about what the backward will look like:</span>
        <span class="c1"># Outputs must include a scalar loss, that we compute gradients w.r.t.</span>
        <span class="c1"># We don&#39;t compute gradients w.r.t. anything else: so just in case we detach()</span>
        <span class="c1"># and other output tensors.</span>
        <span class="k">def</span> <span class="nf">fn_to_trace</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">nonlocal</span> <span class="n">num_fw_outs</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">output_loss_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">If trace_joint=Trueit is required that one of your forward outputs must be a scalar loss.</span>
<span class="s2">You must specify the which (index) output is the loss with output_loss_index.&quot;&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
                <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">out</span><span class="p">,)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected forward output to be either a tensor or a list/tuple of tensors. found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">out</span><span class="p">):</span>
                <span class="c1"># We only want to create a backward graph w.r.t. the loss that the user passed in.</span>
                <span class="c1"># This implies that every other output should not require gradients.</span>
                <span class="c1"># Instead of making this an error (and forcing the user to detach all other outputs</span>
                <span class="c1"># of their forward),</span>
                <span class="c1"># we&#39;ll automatically detach them here.</span>
                <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">output_loss_index</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found an output of the forward that requires gradients, that was not the scalar loss.</span>
<span class="s2">We require all outputs to the forward that are not the scalar loss to not require gradient,</span>
<span class="s2">because we will only compute a backward graph against the scalar loss.</span>
<span class="s2">You can fix this by calling .detach() on each of your forward outputs that is not the loss.</span>
<span class="s2">You specified that output index </span><span class="si">{</span><span class="n">output_loss_index</span><span class="si">}</span><span class="s2"> is the loss, but we found that</span>
<span class="s2">the output at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> requires gradients.&quot;&quot;&quot;</span><span class="p">)</span>
            <span class="n">out_loss</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">output_loss_index</span><span class="p">]</span>
            <span class="n">num_fw_outs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">out_loss</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The output at index </span><span class="si">{</span><span class="n">output_loss_index</span><span class="si">}</span><span class="s2"> was marked as the loss, but it does not require gradients&quot;&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">out_loss</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">We require the output marked as the loss (at index </span><span class="si">{</span><span class="n">output_loss_index</span><span class="si">}</span><span class="s2">) to be a scalar, but it has shape </span><span class="si">{</span><span class="n">out_loss</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run under no_grad, so our tracing machinery only traces an inference graph.</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span>
        <span class="n">fn_to_trace</span> <span class="o">=</span> <span class="n">functional_call</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># First, the params</span>
    <span class="c1"># NB: It is REQUIRED that parameters come first, Inductor infers &quot;fixed&quot;</span>
    <span class="c1"># parameters by looking at the difference in parameter count outside</span>
    <span class="c1"># and inside AOTAutograd, and assumes the prefix of arguments are fixed</span>
    <span class="c1"># arguments</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>
    <span class="c1"># Next, the input args</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ctx</span><span class="p">():</span>
        <span class="n">fx_g</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">_aot_export_function</span><span class="p">(</span>
            <span class="n">fn_to_trace</span><span class="p">,</span>
            <span class="n">full_args</span><span class="p">,</span>
            <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
            <span class="n">num_params_buffers</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">),</span>
            <span class="n">no_tangents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">flattened_joint</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="c1"># The idea here is that the joint graph that AOTAutograd creates has some strict properties:</span>
            <span class="c1"># (1) It accepts two arguments (primals, tangents), and pytree_flattens them</span>
            <span class="c1"># (2) It returns a tuple of (fw_outs, gradients)</span>
            <span class="c1"># This is a very useful convention for anyone who wants to partition the joint graph</span>
            <span class="c1"># into a separate forward and backward graph.</span>
            <span class="c1"># However,</span>
            <span class="c1"># (1) for people exporting a single joint graph, it would be preferable not to have</span>
            <span class="c1">#     any pytrees in the graph.</span>
            <span class="c1"># (2) We are guaranteed in the aot_export_module case that the forward outputs a loss,</span>
            <span class="c1">#     and there are therefore no tangents that are needed to run the joint graph.</span>
            <span class="c1"># (3) AOTAutograd creates a grad_input for every input in the forward,</span>
            <span class="c1">#     including None&#39;s for inputs that are not grad-requiring tensors.</span>
            <span class="c1">#     we don&#39;t want these in our export graph.</span>
            <span class="c1">#     and there are therefore no tangents that are needed to run the joint graph.</span>
            <span class="c1"># This function &quot;fixes&quot; both of the above by removing any tangent inputs,</span>
            <span class="c1"># and removing pytrees from the original FX graph.</span>
            <span class="n">fake_tangents</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span><span class="p">)]</span>
            <span class="n">fw_outs</span><span class="p">,</span> <span class="n">gradients</span> <span class="o">=</span> <span class="n">fx_g</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">fake_tangents</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">output_gradients</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">gradients</span><span class="p">)):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found a parameter that did not receive a gradient.</span>
<span class="s2">&quot;This is most likely a bug, but if this needs to be supported please comment on this Github issue:</span>
<span class="s2">https://github.com/pytorch/pytorch/issues/101192</span>
<span class="s2">&quot;&quot;&quot;</span>
                    <span class="n">output_gradients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="o">*</span><span class="n">fw_outs</span><span class="p">,</span> <span class="o">*</span><span class="n">output_gradients</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">flattened_joint</span><span class="p">)(</span><span class="o">*</span><span class="n">full_args</span><span class="p">)</span>

    <span class="n">user_args_flat</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">create_graph_signature</span><span class="p">(</span>
        <span class="n">fx_g</span><span class="p">,</span>
        <span class="n">metadata</span><span class="p">,</span>
        <span class="n">in_spec</span><span class="p">,</span>
        <span class="n">out_spec</span><span class="p">,</span>
        <span class="n">user_args_flat</span><span class="o">=</span><span class="n">user_args_flat</span><span class="p">,</span>
        <span class="n">params_and_buffers_flat</span><span class="o">=</span><span class="n">params_and_buffers_flat</span><span class="p">,</span>
        <span class="n">param_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">named_parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
        <span class="n">buffer_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">named_buffers</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="n">trace_joint</span><span class="p">,</span>
        <span class="n">num_user_fw_outs</span><span class="o">=</span><span class="n">num_fw_outs</span><span class="p">,</span>
        <span class="n">loss_index</span><span class="o">=</span><span class="n">output_loss_index</span><span class="p">,</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">aot_export_joint_simple</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="c1"># It looks like the main consequence of this API is that for dynamic shapes,</span>
    <span class="c1"># it will assume that parms/buffers are static.</span>
    <span class="c1"># With the new inferred dynamic shapes API, maybe this doesn&#39;t matter?</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simplified version of export. Used by higher order operators.</span>

<span class="sd">    This function makes a high-level &quot;no calling convention changes&quot; guarantee:</span>
<span class="sd">    - If no inputs require grad (so we export an inference graph),</span>
<span class="sd">      there are *no* calling convention change between the exported graph, and &quot;func&quot;.</span>
<span class="sd">    - If at least one input requires grad (so we trace out and export a joint fw-bw graph),</span>
<span class="sd">      Then if you were partition the graph into a separate forward and backward graph,</span>
<span class="sd">      The forward graph will have no calling convention changes compared to &quot;func&quot;.</span>

<span class="sd">    The above also relies on some strong restrictions around which functions this API accepts:</span>
<span class="sd">    (1) `args` cannot contain any pytrees (they must have been pytree_flattened already)</span>
<span class="sd">    (2) `func` cannot mutate any inputs</span>
<span class="sd">    (3) The outputs of `func` cannot alias any inputs.</span>

<span class="sd">    Note: this function is only lightly tested today. It will probably be tested more heavily by higher order ops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run under no_grad, so our tracing machinery only traces an inference graph.</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span>

    <span class="k">with</span> <span class="n">ctx</span><span class="p">():</span>
        <span class="n">fx_g</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">_aot_export_function</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="n">args</span><span class="p">,</span>
            <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># At this point, we can just directly return the (joint or inference graph) that we traced.</span>
    <span class="c1"># First though: a bunch of assertions to make sure that our graph doesn&#39;t require</span>
    <span class="c1"># any calling convention changes compared to the original function.</span>
    <span class="c1"># These restrictions are *in addition to* the general restrictions on export.</span>

    <span class="c1"># No input mutations</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple does not support input mutations. </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># No output aliasing</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">output_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple does not support outputs that alias inputs. </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># No pytrees</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">in_spec</span><span class="p">)</span> <span class="o">==</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires inputs to be a single list/tuple. in_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">in_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">in_spec</span><span class="o">.</span><span class="n">children_specs</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires individual inputs not to be pytrees. in_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">in_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">out_spec</span><span class="p">)</span> <span class="o">==</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires outputs to be a single list/tuple. out_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">out_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">children_specs</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires individual outputs not to be pytrees. out_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">out_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># TODO: we might have to temporarily patch config.functionalize_rng</span>
    <span class="c1"># so that it doesn&#39;t run when we&#39;re exporting a higher order op.</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="c1"># Smoke test that after partitioning, we can run the forward without any calling convention changes.</span>
        <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">default_partition</span><span class="p">(</span>
            <span class="n">fx_g</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_infos</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Attempt to run the fw_module with the original user inputs</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">fake_mode</span><span class="p">:</span>
            <span class="n">fw_module</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span>

<span class="c1"># Private for now because we aren&#39;t providing a contract on what to return</span>
<span class="c1"># for joint graphs (we could when there&#39;s a clearer use case)</span>
<span class="c1"># In the future, we may need to add more export API&#39;s that provide their own strong guarantees.</span>
<span class="c1"># This is meant as a general helper function for handling various export-y use cases.</span>
<span class="k">def</span> <span class="nf">_aot_export_function</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># If we&#39;re exporting a joint graph and we don&#39;t want any tangent inputs in the graph</span>
    <span class="c1"># (because we are backpropping through a scalar 1 loss),</span>
    <span class="c1"># we need to explicitly specify not to include tangents in the graph.</span>
    <span class="c1"># It&#39;s not enough just to check that our tangent is a scalar, since we also</span>
    <span class="c1"># need to know if it is a 1 (no need to make it a graph input), or something else</span>
    <span class="c1"># (requiring it to be a graph input).</span>
    <span class="c1"># We don&#39;t know this info at trace time though, so we need to make it an explicit config.</span>
    <span class="n">no_tangents</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">]:</span>
    <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
            <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">break</span>

    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">create_tree_flattened_fn</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">flat_args</span><span class="p">,</span> <span class="n">in_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="c1"># The export use case doesn&#39;t care about several bits of AOTConfig</span>
    <span class="c1"># (1) compilers (we just export the graph)</span>
    <span class="c1"># (2) partitioners (export is only full graph, user can partition themselves)</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="c1"># For now there&#39;s no use case involving keeping input mutations in the graph</span>
        <span class="c1"># (which we can only do in the inference case anyway).</span>
        <span class="c1"># We can add this later if we need to.</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_export</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_tangents</span><span class="o">=</span><span class="n">no_tangents</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">fx_g</span><span class="p">,</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">flat_args</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">spec</span>


<span class="n">compiled_function</span> <span class="o">=</span> <span class="n">aot_function</span>
<span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aot_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>